{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenNMT-tf 2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://opennmt.net/OpenNMT-tf/package/opennmt.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenNMT是一个开源生态系统，用于神经机器翻译和神经序列学习。支持pytorch与tensorflow，介绍基于tensorflow2.0的openNMT 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: OpenNMT-tf in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (2.11.0)\n",
      "Requirement already satisfied: pyonmttok<2,>=1.18.1; platform_system == \"Linux\" in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from OpenNMT-tf) (1.18.4)\n",
      "Requirement already satisfied: pyyaml<5.4,>=5.3 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from OpenNMT-tf) (5.3.1)\n",
      "Requirement already satisfied: sacrebleu<2,>=1.4.9 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from OpenNMT-tf) (1.4.10)\n",
      "Requirement already satisfied: tensorflow<2.3,>=2.2 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from OpenNMT-tf) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-addons<0.11,>=0.10 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from OpenNMT-tf) (0.10.0)\n",
      "Requirement already satisfied: pyter3==0.3 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from OpenNMT-tf) (0.3)\n",
      "Requirement already satisfied: ctranslate2<2,>=1.7; platform_system == \"Linux\" in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from OpenNMT-tf) (1.10.2)\n",
      "Requirement already satisfied: rouge<2,>=1.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from OpenNMT-tf) (1.0.0)\n",
      "Requirement already satisfied: portalocker in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from sacrebleu<2,>=1.4.9->OpenNMT-tf) (1.7.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (3.11.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (3.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (1.14.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (2.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (2.2.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (0.34.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (1.27.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (1.12.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (1.18.2)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (1.4.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (1.1.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (0.3.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (0.9.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow<2.3,>=2.2->OpenNMT-tf) (1.1.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorflow-addons<0.11,>=0.10->OpenNMT-tf) (2.9.1)\n",
      "Requirement already satisfied: setuptools in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (46.1.1.post20200323)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (2.21.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (1.6.0.post3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (3.2.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (1.11.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (1.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (1.3.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/dutir_2t/wangchenguang/anaconda3/envs/sunyi/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2->OpenNMT-tf) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install OpenNMT-tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opennmt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['which group are you in ?', 'where you are ?']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('src.txt', 'r') as f:\n",
    "    src_data = f.read().split('\\n')[:-1]\n",
    "src_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the search group .', 'i am in ir .']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tgt.txt', 'r') as f:\n",
    "    tgt_data = f.read().split('\\n')[:-1]\n",
    "tgt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# package opennmt.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 字典构建 opennmt.data.Vocab(special_tokens=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<s>', '</s>', '<unk>', '<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<unk>', '<pad>']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = opennmt.data.Vocab(special_tokens)\n",
    "vocab.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '</s>',\n",
       " '<unk>',\n",
       " '<pad>',\n",
       " 'which',\n",
       " 'group',\n",
       " 'are',\n",
       " 'you',\n",
       " 'in',\n",
       " '?',\n",
       " 'where']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.add_from_text('src.txt')\n",
    "vocab.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '</s>',\n",
       " '<unk>',\n",
       " '<pad>',\n",
       " 'which',\n",
       " 'group',\n",
       " 'are',\n",
       " 'you',\n",
       " 'in',\n",
       " '?',\n",
       " 'where',\n",
       " 'the',\n",
       " 'search',\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'ir']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.add_from_text('tgt.txt')\n",
    "vocab.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup('group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'group'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<unk>', '<pad>', 'group', 'are', 'you', 'in', '?', '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.prune(max_size=0, min_frequency=2).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.serialize('vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "</s>\n",
      "<unk>\n",
      "<pad>\n",
      "which\n",
      "group\n",
      "are\n",
      "you\n",
      "in\n",
      "?\n",
      "where\n",
      "the\n",
      "search\n",
      ".\n",
      "i\n",
      "am\n",
      "ir\n"
     ]
    }
   ],
   "source": [
    "with open('vocab.txt') as f:\n",
    "    for w in f.readlines():\n",
    "        print(w[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据噪声 opennmt.data.WordOmission(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'which', b'group', b'are', b'you', b'in'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = opennmt.data.WordOmission(1)\n",
    "noise(tf.constant(src_data[0].split())).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'which', b'are', b'you', b'?'], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = opennmt.data.WordOmission(2)\n",
    "noise(tf.constant(src_data[0].split())).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据噪声 opennmt.data.WordDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'group', b'you'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = opennmt.data.WordDropout(0.5)\n",
    "noise(tf.constant(src_data[0].split())).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'?'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = opennmt.data.WordDropout(0.9)\n",
    "noise(tf.constant(src_data[0].split())).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[b'group', b'are', b'you', b'in', b'?'], [b'where', b'?']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = opennmt.data.WordDropout(0.2)\n",
    "src_data = [noise(tf.constant(line.split())).numpy().tolist() for line in src_data]\n",
    "src_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[b'the', b'search', b'group'], [b'i', b'am', b'.']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_data = [noise(tf.constant(line.split())).numpy().tolist() for line in tgt_data]\n",
    "tgt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 6, 7, 8, 9], [10, 9]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_id = [[vocab.lookup(word) if word in vocab else \n",
    "           vocab.lookup('<unk>') for word in line] for line in src_data]\n",
    "src_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 12, 5], [14, 15, 13]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_id = [[vocab.lookup(word) if word in vocab else \n",
    "           vocab.lookup('<unk>') for word in line] for line in tgt_data]\n",
    "tgt_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package opennmt.encoders\n",
    "##### class opennmt.encoders.LSTMEncoder(num_layers, num_units, bidirectional=False, residual_connections=False, dropout=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  3,  3,  3,  3,  5,  6,  7,  8,  9],\n",
       "       [ 3,  3,  3,  3,  3,  3,  3,  3, 10,  9]], dtype=int32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 10\n",
    "pad_src_id = tf.keras.preprocessing.sequence.pad_sequences(src_id, value = vocab.lookup('<pad>'), \n",
    "                                                       maxlen = max_len)\n",
    "src_seq_len = [len(x) for x in src_data]\n",
    "pad_src_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  3,  3,  3,  3,  3,  3, 11, 12,  5],\n",
       "       [ 3,  3,  3,  3,  3,  3,  3, 14, 15, 13]], dtype=int32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 10\n",
    "pad_tgt_id = tf.keras.preprocessing.sequence.pad_sequences(tgt_id, value = vocab.lookup('<pad>'), \n",
    "                                                       maxlen = max_len)\n",
    "tgt_seq_len = [len(x) for x in tgt_data]\n",
    "pad_tgt_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 10, 20])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 20\n",
    "pad_src_emb = tf.keras.layers.Embedding(vocab.size, emb_dim, input_length=10)(pad_src_id)\n",
    "pad_src_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 10, 15])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = opennmt.encoders.LSTMEncoder(1, 15)\n",
    "outputs, states, sequence_length = enc(pad_src_emb, src_seq_len)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(15,), dtype=float32, numpy=\n",
       "array([ 0.04737591,  0.00166916,  0.00071735, -0.01284687,  0.01742313,\n",
       "       -0.04498446,  0.02106411,  0.00564089, -0.03366548,  0.01916166,\n",
       "       -0.01094256, -0.00107302, -0.00540767, -0.00640422,  0.00955995],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<tf.Tensor: shape=(2, 15), dtype=float32, numpy=\n",
       "  array([[ 0.04737591,  0.00166916,  0.00071735, -0.01284687,  0.01742313,\n",
       "          -0.04498446,  0.02106411,  0.00564089, -0.03366548,  0.01916166,\n",
       "          -0.01094256, -0.00107302, -0.00540767, -0.00640422,  0.00955995],\n",
       "         [ 0.02719584,  0.00054847, -0.00033527, -0.01090283,  0.007499  ,\n",
       "          -0.0263254 ,  0.01515887,  0.00537936, -0.01710399,  0.01374206,\n",
       "          -0.00555701, -0.0001357 , -0.00215719, -0.00632402,  0.00690264]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Tensor: shape=(2, 15), dtype=float32, numpy=\n",
       "  array([[ 0.09311996,  0.00331793,  0.00139865, -0.02529183,  0.0345932 ,\n",
       "          -0.09121793,  0.04238915,  0.01127814, -0.06712874,  0.03761647,\n",
       "          -0.02202717, -0.00213244, -0.01092287, -0.01310171,  0.01894908],\n",
       "         [ 0.05332884,  0.00108977, -0.00065884, -0.02153684,  0.01488502,\n",
       "          -0.05316443,  0.03050686,  0.01073244, -0.03401763,  0.02713889,\n",
       "          -0.01113   , -0.00027121, -0.00435064, -0.0129389 ,  0.01367906]],\n",
       "        dtype=float32)>),)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 15), dtype=float32, numpy=\n",
       "array([[ 0.04737591,  0.00166916,  0.00071735, -0.01284687,  0.01742313,\n",
       "        -0.04498446,  0.02106411,  0.00564089, -0.03366548,  0.01916166,\n",
       "        -0.01094256, -0.00107302, -0.00540767, -0.00640422,  0.00955995],\n",
       "       [ 0.02719584,  0.00054847, -0.00033527, -0.01090283,  0.007499  ,\n",
       "        -0.0263254 ,  0.01515887,  0.00537936, -0.01710399,  0.01374206,\n",
       "        -0.00555701, -0.0001357 , -0.00215719, -0.00632402,  0.00690264]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ConvEncoder\n",
    "* GNMTEncoder\n",
    "* LSTMEncoder \n",
    "* MeanEncoder \n",
    "* SelfAttentionEncoder \n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class opennmt.encoders.ParallelEncoder(encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc1 = opennmt.encoders.LSTMEncoder(1, 20)\n",
    "enc2 = opennmt.encoders.LSTMEncoder(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40,), dtype=float32, numpy=\n",
       "array([-1.6207565e-02, -5.9631411e-03, -1.5756657e-02,  2.4876524e-02,\n",
       "       -2.8930521e-03,  4.3384757e-04, -1.7809410e-02,  1.0466436e-02,\n",
       "       -6.8887551e-03,  2.1672165e-02, -6.8886746e-03, -2.3159022e-03,\n",
       "       -3.4424413e-03, -7.1377545e-03,  6.8965665e-04,  2.8718684e-02,\n",
       "        2.2850968e-02,  3.8266361e-02,  3.2604844e-03, -1.0929327e-02,\n",
       "        2.1015802e-02,  4.0262691e-03,  1.0469296e-02,  8.2764233e-04,\n",
       "        2.9064114e-03,  1.3466297e-04,  9.1320379e-03, -1.1087879e-02,\n",
       "       -1.0378932e-02,  1.6162093e-03, -6.6105556e-04,  9.6056797e-03,\n",
       "        1.3419278e-02,  1.8408947e-02,  9.9306898e-03, -9.6405456e-03,\n",
       "       -1.8454060e-02, -9.0356507e-06, -1.3730046e-02, -9.9743390e-03],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_enc = opennmt.encoders.ParallelEncoder([\n",
    "    enc1,\n",
    "    enc2,\n",
    "], outputs_reducer = opennmt.layers.ConcatReducer())\n",
    "outputs, states, sequence_length = p_enc(pad_src_emb, src_seq_len)\n",
    "outputs[0,src_seq_len[0]-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40,), dtype=float32, numpy=\n",
       "array([-1.6207565e-02, -5.9631411e-03, -1.5756657e-02,  2.4876524e-02,\n",
       "       -2.8930521e-03,  4.3384757e-04, -1.7809410e-02,  1.0466436e-02,\n",
       "       -6.8887551e-03,  2.1672165e-02, -6.8886746e-03, -2.3159022e-03,\n",
       "       -3.4424413e-03, -7.1377545e-03,  6.8965665e-04,  2.8718684e-02,\n",
       "        2.2850968e-02,  3.8266361e-02,  3.2604844e-03, -1.0929327e-02,\n",
       "        2.1015802e-02,  4.0262691e-03,  1.0469296e-02,  8.2764233e-04,\n",
       "        2.9064114e-03,  1.3466297e-04,  9.1320379e-03, -1.1087879e-02,\n",
       "       -1.0378932e-02,  1.6162093e-03, -6.6105556e-04,  9.6056797e-03,\n",
       "        1.3419278e-02,  1.8408947e-02,  9.9306898e-03, -9.6405456e-03,\n",
       "       -1.8454060e-02, -9.0356507e-06, -1.3730046e-02, -9.9743390e-03],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs1, _, _ = enc1(pad_src_emb, src_seq_len)\n",
    "outputs2, _, _ = enc2(pad_src_emb, src_seq_len)\n",
    "tf.concat([outputs1, outputs2], axis = -1)[0,src_seq_len[0]-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([-0.01656474,  0.01210644,  0.00080729,  0.00348837,  0.00234598,\n",
       "       -0.03267   , -0.00923768,  0.02688324, -0.00740472,  0.03477048],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_enc = opennmt.encoders.ParallelEncoder([\n",
    "    enc1,\n",
    "    enc2,\n",
    "], outputs_reducer = opennmt.layers.DenseReducer(10))\n",
    "outputs, states, sequence_length = p_enc(pad_src_emb, src_seq_len)\n",
    "outputs[0,src_seq_len[0]-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([ 4.8082378e-03, -1.9368720e-03, -5.2873613e-03,  2.5704166e-02,\n",
       "        1.3359357e-05,  5.6851056e-04, -8.6773718e-03, -6.2144268e-04,\n",
       "       -1.7267687e-02,  2.3288375e-02, -7.5497301e-03,  7.2897775e-03,\n",
       "        9.9768369e-03,  1.1271192e-02,  1.0620346e-02,  1.9078139e-02,\n",
       "        4.3969080e-03,  3.8257327e-02, -1.0469561e-02, -2.0903666e-02],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_enc = opennmt.encoders.ParallelEncoder([\n",
    "    enc1,\n",
    "    enc2,\n",
    "], outputs_reducer = opennmt.layers.SumReducer())\n",
    "outputs, states, sequence_length = p_enc(pad_src_emb, src_seq_len)\n",
    "outputs[0,src_seq_len[0]-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1.06895175e-02, 4.13929336e-02],\n",
       "       [1.24075450e-06, 2.27661896e-02]], dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs, state, _ = enc(pad_src_emb, src_seq_len)\n",
    "logits = opennmt.layers.Dense(2)(state[0][0])\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package opennmt.models\n",
    "\n",
    "* opennmt.models.GPT2Small\n",
    "* opennmt.models.LanguageModel\n",
    "* opennmt.models.LstmCnnCrfTagger\n",
    "* opennmt.models.SequenceClassifier\n",
    "* opennmt.models.SequenceGenerator\n",
    "* opennmt.models.SequenceTagger\n",
    "* opennmt.models.SequenceToSequence\n",
    "* opennmt.models.Transformer\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package opennmt.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* opennmt.utils.BLEUScorer\n",
    "* opennmt.utils.PRFScorer\n",
    "* opennmt.utils.ROUGEScorer\n",
    "* opennmt.utils.BeamSearch\n",
    "* opennmt.utils.GreedySearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package opennmt.decoders\n",
    "\n",
    "* opennmt.decoders.AttentionalRNNDecoder\n",
    "* opennmt.decoders.RNNDecoder\n",
    "* opennmt.decoders.SelfAttentionDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
