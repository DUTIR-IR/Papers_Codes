Vocab size 45554 loaded from file
Loading data from ./dataset/tacred with batch size 50...
1363 batches created for ./dataset/tacred/train.json
453 batches created for ./dataset/tacred/dev.json
Config saved to file ./saved_models/01/config.json
Overwriting old vocab file at ./saved_models/01/vocab.pkl

Running with the following configs:
	data_dir : ./dataset/tacred
	vocab_dir : ./dataset/vocab
	emb_dim : 300
	ner_dim : 30
	pos_dim : 30
	hidden_dim : 300
	num_layers : 2
	input_dropout : 0.5
	gcn_dropout : 0.5
	word_dropout : 0.04
	topn : 10000000000.0
	lower : False
	heads : 3
	sublayer_first : 2
	sublayer_second : 4
	pooling : max
	pooling_l2 : 0.002
	mlp_layers : 1
	no_adj : False
	rnn : True
	rnn_hidden : 300
	rnn_layers : 1
	rnn_dropout : 0.5
	lr : 0.7
	lr_decay : 0.9
	decay_epoch : 5
	optim : sgd
	num_epoch : 100
	batch_size : 50
	max_grad_norm : 5.0
	log_step : 20
	log : logs.txt
	save_epoch : 100
	save_dir : ./saved_models
	id : 1
	info : 
	seed : 0
	cuda : True
	cpu : False
	load : False
	model_file : None
	num_class : 42
	vocab_size : 45554
	model_save_dir : ./saved_models/01


Finetune all embeddings.
2019-12-27 10:35:54.690337: step 20/136300 (epoch 1/100), loss = 1.258255 (0.220 sec/batch), lr: 0.700000
2019-12-27 10:35:59.472062: step 40/136300 (epoch 1/100), loss = 0.755291 (0.239 sec/batch), lr: 0.700000
2019-12-27 10:36:04.346546: step 60/136300 (epoch 1/100), loss = 0.825182 (0.224 sec/batch), lr: 0.700000
2019-12-27 10:36:09.266648: step 80/136300 (epoch 1/100), loss = 0.554227 (0.227 sec/batch), lr: 0.700000
2019-12-27 10:36:14.132135: step 100/136300 (epoch 1/100), loss = 0.748647 (0.236 sec/batch), lr: 0.700000
2019-12-27 10:36:18.961972: step 120/136300 (epoch 1/100), loss = 0.981552 (0.248 sec/batch), lr: 0.700000
2019-12-27 10:36:23.803694: step 140/136300 (epoch 1/100), loss = 1.168799 (0.245 sec/batch), lr: 0.700000
2019-12-27 10:36:28.664239: step 160/136300 (epoch 1/100), loss = 0.850708 (0.243 sec/batch), lr: 0.700000
2019-12-27 10:36:33.610515: step 180/136300 (epoch 1/100), loss = 1.045660 (0.237 sec/batch), lr: 0.700000
2019-12-27 10:36:38.521554: step 200/136300 (epoch 1/100), loss = 0.992067 (0.241 sec/batch), lr: 0.700000
2019-12-27 10:36:45.033894: step 220/136300 (epoch 1/100), loss = 0.816302 (0.208 sec/batch), lr: 0.700000
2019-12-27 10:36:49.826003: step 240/136300 (epoch 1/100), loss = 1.116969 (0.204 sec/batch), lr: 0.700000
2019-12-27 10:36:54.690838: step 260/136300 (epoch 1/100), loss = 0.856713 (0.214 sec/batch), lr: 0.700000
2019-12-27 10:36:59.567843: step 280/136300 (epoch 1/100), loss = 0.749369 (0.181 sec/batch), lr: 0.700000
2019-12-27 10:37:04.444278: step 300/136300 (epoch 1/100), loss = 0.767499 (0.240 sec/batch), lr: 0.700000
2019-12-27 10:37:09.252173: step 320/136300 (epoch 1/100), loss = 0.713161 (0.228 sec/batch), lr: 0.700000
2019-12-27 10:37:14.084411: step 340/136300 (epoch 1/100), loss = 1.130685 (0.209 sec/batch), lr: 0.700000
2019-12-27 10:37:19.000232: step 360/136300 (epoch 1/100), loss = 1.216991 (0.218 sec/batch), lr: 0.700000
2019-12-27 10:37:23.811745: step 380/136300 (epoch 1/100), loss = 1.350383 (0.234 sec/batch), lr: 0.700000
2019-12-27 10:37:28.629869: step 400/136300 (epoch 1/100), loss = 0.761884 (0.247 sec/batch), lr: 0.700000
2019-12-27 10:37:34.928974: step 420/136300 (epoch 1/100), loss = 0.581456 (0.237 sec/batch), lr: 0.700000
2019-12-27 10:37:39.810688: step 440/136300 (epoch 1/100), loss = 1.042338 (0.231 sec/batch), lr: 0.700000
2019-12-27 10:37:44.734922: step 460/136300 (epoch 1/100), loss = 0.885051 (0.248 sec/batch), lr: 0.700000
2019-12-27 10:37:49.630015: step 480/136300 (epoch 1/100), loss = 0.820093 (0.246 sec/batch), lr: 0.700000
2019-12-27 10:37:54.469383: step 500/136300 (epoch 1/100), loss = 0.401990 (0.234 sec/batch), lr: 0.700000
2019-12-27 10:37:59.422225: step 520/136300 (epoch 1/100), loss = 0.872716 (0.243 sec/batch), lr: 0.700000
2019-12-27 10:38:04.391543: step 540/136300 (epoch 1/100), loss = 0.452777 (0.243 sec/batch), lr: 0.700000
2019-12-27 10:38:09.233118: step 560/136300 (epoch 1/100), loss = 0.751587 (0.216 sec/batch), lr: 0.700000
2019-12-27 10:38:14.100232: step 580/136300 (epoch 1/100), loss = 0.627069 (0.213 sec/batch), lr: 0.700000
2019-12-27 10:38:18.862289: step 600/136300 (epoch 1/100), loss = 0.743927 (0.205 sec/batch), lr: 0.700000
2019-12-27 10:38:25.355860: step 620/136300 (epoch 1/100), loss = 0.527668 (0.228 sec/batch), lr: 0.700000
2019-12-27 10:38:30.194528: step 640/136300 (epoch 1/100), loss = 0.683769 (0.181 sec/batch), lr: 0.700000
2019-12-27 10:38:35.008078: step 660/136300 (epoch 1/100), loss = 0.555790 (0.209 sec/batch), lr: 0.700000
2019-12-27 10:38:39.929361: step 680/136300 (epoch 1/100), loss = 0.662991 (0.229 sec/batch), lr: 0.700000
2019-12-27 10:38:44.906177: step 700/136300 (epoch 1/100), loss = 0.764389 (0.242 sec/batch), lr: 0.700000
2019-12-27 10:38:49.836403: step 720/136300 (epoch 1/100), loss = 0.699471 (0.229 sec/batch), lr: 0.700000
2019-12-27 10:38:54.752398: step 740/136300 (epoch 1/100), loss = 0.548661 (0.176 sec/batch), lr: 0.700000
2019-12-27 10:38:59.601114: step 760/136300 (epoch 1/100), loss = 0.525919 (0.245 sec/batch), lr: 0.700000
2019-12-27 10:39:04.436960: step 780/136300 (epoch 1/100), loss = 0.693968 (0.249 sec/batch), lr: 0.700000
2019-12-27 10:39:09.442836: step 800/136300 (epoch 1/100), loss = 1.291672 (0.209 sec/batch), lr: 0.700000
2019-12-27 10:39:15.766678: step 820/136300 (epoch 1/100), loss = 0.641977 (0.248 sec/batch), lr: 0.700000
2019-12-27 10:39:20.680831: step 840/136300 (epoch 1/100), loss = 0.407806 (0.237 sec/batch), lr: 0.700000
2019-12-27 10:39:25.619716: step 860/136300 (epoch 1/100), loss = 0.488642 (0.226 sec/batch), lr: 0.700000
2019-12-27 10:39:30.410786: step 880/136300 (epoch 1/100), loss = 0.610916 (0.246 sec/batch), lr: 0.700000
2019-12-27 10:39:35.285155: step 900/136300 (epoch 1/100), loss = 0.857171 (0.209 sec/batch), lr: 0.700000
2019-12-27 10:39:40.256758: step 920/136300 (epoch 1/100), loss = 0.492334 (0.225 sec/batch), lr: 0.700000
2019-12-27 10:39:45.144419: step 940/136300 (epoch 1/100), loss = 0.423647 (0.238 sec/batch), lr: 0.700000
2019-12-27 10:39:50.124196: step 960/136300 (epoch 1/100), loss = 0.780206 (0.219 sec/batch), lr: 0.700000
2019-12-27 10:39:55.058466: step 980/136300 (epoch 1/100), loss = 0.779320 (0.221 sec/batch), lr: 0.700000
2019-12-27 10:39:59.918366: step 1000/136300 (epoch 1/100), loss = 0.653254 (0.244 sec/batch), lr: 0.700000
2019-12-27 10:40:06.296970: step 1020/136300 (epoch 1/100), loss = 0.491809 (0.236 sec/batch), lr: 0.700000
2019-12-27 10:40:11.047258: step 1040/136300 (epoch 1/100), loss = 0.773761 (0.185 sec/batch), lr: 0.700000
2019-12-27 10:40:15.785952: step 1060/136300 (epoch 1/100), loss = 0.717679 (0.171 sec/batch), lr: 0.700000
2019-12-27 10:40:20.684590: step 1080/136300 (epoch 1/100), loss = 0.794931 (0.232 sec/batch), lr: 0.700000
2019-12-27 10:40:25.504838: step 1100/136300 (epoch 1/100), loss = 0.743156 (0.238 sec/batch), lr: 0.700000
2019-12-27 10:40:30.450662: step 1120/136300 (epoch 1/100), loss = 0.478851 (0.217 sec/batch), lr: 0.700000
2019-12-27 10:40:35.337342: step 1140/136300 (epoch 1/100), loss = 0.765819 (0.225 sec/batch), lr: 0.700000
2019-12-27 10:40:40.136760: step 1160/136300 (epoch 1/100), loss = 0.452834 (0.217 sec/batch), lr: 0.700000
2019-12-27 10:40:44.924884: step 1180/136300 (epoch 1/100), loss = 0.656067 (0.237 sec/batch), lr: 0.700000
2019-12-27 10:40:51.262677: step 1200/136300 (epoch 1/100), loss = 0.286165 (0.230 sec/batch), lr: 0.700000
2019-12-27 10:40:56.191918: step 1220/136300 (epoch 1/100), loss = 0.707488 (0.222 sec/batch), lr: 0.700000
2019-12-27 10:41:01.158402: step 1240/136300 (epoch 1/100), loss = 0.650238 (0.232 sec/batch), lr: 0.700000
2019-12-27 10:41:06.067241: step 1260/136300 (epoch 1/100), loss = 0.812772 (0.250 sec/batch), lr: 0.700000
2019-12-27 10:41:10.794775: step 1280/136300 (epoch 1/100), loss = 0.258838 (0.236 sec/batch), lr: 0.700000
2019-12-27 10:41:15.639457: step 1300/136300 (epoch 1/100), loss = 0.524507 (0.182 sec/batch), lr: 0.700000
2019-12-27 10:41:20.469096: step 1320/136300 (epoch 1/100), loss = 0.377132 (0.200 sec/batch), lr: 0.700000
2019-12-27 10:41:25.419312: step 1340/136300 (epoch 1/100), loss = 0.329833 (0.220 sec/batch), lr: 0.700000
2019-12-27 10:41:30.437130: step 1360/136300 (epoch 1/100), loss = 0.522828 (0.232 sec/batch), lr: 0.700000
Evaluating on dev set...
Precision (micro): 73.436%
   Recall (micro): 16.630%
       F1 (micro): 27.119%
epoch 1: train_loss = 0.769170, dev_loss = 0.858442, dev_f1 = 0.2712
model saved to ./saved_models/01/checkpoint_epoch_1.pt
new best model saved.

2019-12-27 10:42:08.834640: step 1380/136300 (epoch 2/100), loss = 0.443891 (0.245 sec/batch), lr: 0.700000
2019-12-27 10:42:14.923715: step 1400/136300 (epoch 2/100), loss = 0.223064 (0.251 sec/batch), lr: 0.700000
2019-12-27 10:42:19.787079: step 1420/136300 (epoch 2/100), loss = 0.718552 (0.220 sec/batch), lr: 0.700000
2019-12-27 10:42:24.665854: step 1440/136300 (epoch 2/100), loss = 0.640792 (0.245 sec/batch), lr: 0.700000
2019-12-27 10:42:29.533949: step 1460/136300 (epoch 2/100), loss = 0.626787 (0.222 sec/batch), lr: 0.700000
2019-12-27 10:42:34.334794: step 1480/136300 (epoch 2/100), loss = 0.427313 (0.230 sec/batch), lr: 0.700000
2019-12-27 10:42:39.186956: step 1500/136300 (epoch 2/100), loss = 0.603207 (0.221 sec/batch), lr: 0.700000
2019-12-27 10:42:44.051505: step 1520/136300 (epoch 2/100), loss = 0.738589 (0.229 sec/batch), lr: 0.700000
2019-12-27 10:42:48.969150: step 1540/136300 (epoch 2/100), loss = 0.560883 (0.235 sec/batch), lr: 0.700000
2019-12-27 10:42:53.889398: step 1560/136300 (epoch 2/100), loss = 0.393482 (0.238 sec/batch), lr: 0.700000
2019-12-27 10:42:58.741657: step 1580/136300 (epoch 2/100), loss = 0.726404 (0.246 sec/batch), lr: 0.700000
2019-12-27 10:43:04.984670: step 1600/136300 (epoch 2/100), loss = 0.619618 (0.241 sec/batch), lr: 0.700000
2019-12-27 10:43:09.817215: step 1620/136300 (epoch 2/100), loss = 0.449002 (0.237 sec/batch), lr: 0.700000
2019-12-27 10:43:14.725145: step 1640/136300 (epoch 2/100), loss = 0.642689 (0.227 sec/batch), lr: 0.700000
2019-12-27 10:43:19.575538: step 1660/136300 (epoch 2/100), loss = 0.632522 (0.178 sec/batch), lr: 0.700000
2019-12-27 10:43:24.371172: step 1680/136300 (epoch 2/100), loss = 0.340965 (0.220 sec/batch), lr: 0.700000
2019-12-27 10:43:29.264600: step 1700/136300 (epoch 2/100), loss = 0.266594 (0.236 sec/batch), lr: 0.700000
2019-12-27 10:43:34.153976: step 1720/136300 (epoch 2/100), loss = 0.452761 (0.232 sec/batch), lr: 0.700000
2019-12-27 10:43:38.923006: step 1740/136300 (epoch 2/100), loss = 0.522790 (0.229 sec/batch), lr: 0.700000
2019-12-27 10:43:43.710983: step 1760/136300 (epoch 2/100), loss = 0.627110 (0.225 sec/batch), lr: 0.700000
2019-12-27 10:43:48.526212: step 1780/136300 (epoch 2/100), loss = 0.394415 (0.244 sec/batch), lr: 0.700000
2019-12-27 10:43:54.871324: step 1800/136300 (epoch 2/100), loss = 0.775183 (0.240 sec/batch), lr: 0.700000
2019-12-27 10:43:59.770159: step 1820/136300 (epoch 2/100), loss = 0.576705 (0.238 sec/batch), lr: 0.700000
2019-12-27 10:44:04.622908: step 1840/136300 (epoch 2/100), loss = 0.956952 (0.237 sec/batch), lr: 0.700000
2019-12-27 10:44:09.472420: step 1860/136300 (epoch 2/100), loss = 0.609974 (0.215 sec/batch), lr: 0.700000
2019-12-27 10:44:14.424308: step 1880/136300 (epoch 2/100), loss = 0.895976 (0.235 sec/batch), lr: 0.700000
2019-12-27 10:44:19.395586: step 1900/136300 (epoch 2/100), loss = 0.686026 (0.241 sec/batch), lr: 0.700000
2019-12-27 10:44:24.246235: step 1920/136300 (epoch 2/100), loss = 0.672950 (0.237 sec/batch), lr: 0.700000
2019-12-27 10:44:29.119848: step 1940/136300 (epoch 2/100), loss = 0.547212 (0.239 sec/batch), lr: 0.700000
2019-12-27 10:44:33.885497: step 1960/136300 (epoch 2/100), loss = 0.587734 (0.226 sec/batch), lr: 0.700000
2019-12-27 10:44:38.670114: step 1980/136300 (epoch 2/100), loss = 0.458025 (0.200 sec/batch), lr: 0.700000
2019-12-27 10:44:45.033871: step 2000/136300 (epoch 2/100), loss = 0.440149 (0.234 sec/batch), lr: 0.700000
2019-12-27 10:44:49.798076: step 2020/136300 (epoch 2/100), loss = 0.292029 (0.229 sec/batch), lr: 0.700000
2019-12-27 10:44:54.659298: step 2040/136300 (epoch 2/100), loss = 0.448374 (0.234 sec/batch), lr: 0.700000
2019-12-27 10:44:59.681056: step 2060/136300 (epoch 2/100), loss = 0.638902 (0.233 sec/batch), lr: 0.700000
2019-12-27 10:45:04.591933: step 2080/136300 (epoch 2/100), loss = 0.292232 (0.233 sec/batch), lr: 0.700000
2019-12-27 10:45:09.536673: step 2100/136300 (epoch 2/100), loss = 0.576239 (0.238 sec/batch), lr: 0.700000
2019-12-27 10:45:14.362562: step 2120/136300 (epoch 2/100), loss = 0.488498 (0.230 sec/batch), lr: 0.700000
2019-12-27 10:45:19.227565: step 2140/136300 (epoch 2/100), loss = 0.423413 (0.214 sec/batch), lr: 0.700000
2019-12-27 10:45:24.190902: step 2160/136300 (epoch 2/100), loss = 0.580793 (0.238 sec/batch), lr: 0.700000
2019-12-27 10:45:29.068983: step 2180/136300 (epoch 2/100), loss = 0.504650 (0.212 sec/batch), lr: 0.700000
2019-12-27 10:45:35.392724: step 2200/136300 (epoch 2/100), loss = 0.375844 (0.234 sec/batch), lr: 0.700000
2019-12-27 10:45:40.364651: step 2220/136300 (epoch 2/100), loss = 0.545567 (0.231 sec/batch), lr: 0.700000
2019-12-27 10:45:45.066741: step 2240/136300 (epoch 2/100), loss = 0.615709 (0.205 sec/batch), lr: 0.700000
2019-12-27 10:45:49.987891: step 2260/136300 (epoch 2/100), loss = 0.500782 (0.232 sec/batch), lr: 0.700000
2019-12-27 10:45:54.920676: step 2280/136300 (epoch 2/100), loss = 0.386828 (0.217 sec/batch), lr: 0.700000
2019-12-27 10:45:59.753884: step 2300/136300 (epoch 2/100), loss = 0.397504 (0.239 sec/batch), lr: 0.700000
2019-12-27 10:46:04.744082: step 2320/136300 (epoch 2/100), loss = 0.440423 (0.244 sec/batch), lr: 0.700000
2019-12-27 10:46:09.665182: step 2340/136300 (epoch 2/100), loss = 0.816108 (0.236 sec/batch), lr: 0.700000
2019-12-27 10:46:14.519127: step 2360/136300 (epoch 2/100), loss = 0.361757 (0.244 sec/batch), lr: 0.700000
2019-12-27 10:46:20.864343: step 2380/136300 (epoch 2/100), loss = 0.381486 (0.216 sec/batch), lr: 0.700000
2019-12-27 10:46:25.713531: step 2400/136300 (epoch 2/100), loss = 0.837422 (0.213 sec/batch), lr: 0.700000
2019-12-27 10:46:30.400260: step 2420/136300 (epoch 2/100), loss = 0.392767 (0.213 sec/batch), lr: 0.700000
2019-12-27 10:46:35.228131: step 2440/136300 (epoch 2/100), loss = 0.533356 (0.234 sec/batch), lr: 0.700000
2019-12-27 10:46:40.056250: step 2460/136300 (epoch 2/100), loss = 0.379355 (0.202 sec/batch), lr: 0.700000
2019-12-27 10:46:44.949443: step 2480/136300 (epoch 2/100), loss = 0.423822 (0.241 sec/batch), lr: 0.700000
2019-12-27 10:46:49.842849: step 2500/136300 (epoch 2/100), loss = 0.705536 (0.199 sec/batch), lr: 0.700000
2019-12-27 10:46:54.652792: step 2520/136300 (epoch 2/100), loss = 0.788079 (0.212 sec/batch), lr: 0.700000
2019-12-27 10:46:59.390571: step 2540/136300 (epoch 2/100), loss = 0.253796 (0.244 sec/batch), lr: 0.700000
2019-12-27 10:47:04.266249: step 2560/136300 (epoch 2/100), loss = 0.448519 (0.212 sec/batch), lr: 0.700000
2019-12-27 10:47:10.627591: step 2580/136300 (epoch 2/100), loss = 0.389316 (0.210 sec/batch), lr: 0.700000
2019-12-27 10:47:15.575144: step 2600/136300 (epoch 2/100), loss = 0.438616 (0.216 sec/batch), lr: 0.700000
2019-12-27 10:47:20.431622: step 2620/136300 (epoch 2/100), loss = 0.580075 (0.206 sec/batch), lr: 0.700000
2019-12-27 10:47:25.182746: step 2640/136300 (epoch 2/100), loss = 0.465233 (0.245 sec/batch), lr: 0.700000
2019-12-27 10:47:30.063835: step 2660/136300 (epoch 2/100), loss = 0.344979 (0.245 sec/batch), lr: 0.700000
2019-12-27 10:47:34.842230: step 2680/136300 (epoch 2/100), loss = 0.856591 (0.215 sec/batch), lr: 0.700000
2019-12-27 10:47:39.794273: step 2700/136300 (epoch 2/100), loss = 0.919422 (0.200 sec/batch), lr: 0.700000
2019-12-27 10:47:44.741043: step 2720/136300 (epoch 2/100), loss = 0.584880 (0.242 sec/batch), lr: 0.700000
Evaluating on dev set...
Precision (micro): 67.284%
   Recall (micro): 28.716%
       F1 (micro): 40.253%
epoch 2: train_loss = 0.547320, dev_loss = 0.680989, dev_f1 = 0.4025
model saved to ./saved_models/01/checkpoint_epoch_2.pt
new best model saved.

2019-12-27 10:48:23.051212: step 2740/136300 (epoch 3/100), loss = 0.460994 (0.230 sec/batch), lr: 0.700000
2019-12-27 10:48:27.892976: step 2760/136300 (epoch 3/100), loss = 0.414411 (0.185 sec/batch), lr: 0.700000
2019-12-27 10:48:34.255500: step 2780/136300 (epoch 3/100), loss = 0.515517 (0.170 sec/batch), lr: 0.700000
2019-12-27 10:48:39.055617: step 2800/136300 (epoch 3/100), loss = 0.520262 (0.225 sec/batch), lr: 0.700000
2019-12-27 10:48:44.029186: step 2820/136300 (epoch 3/100), loss = 0.589264 (0.231 sec/batch), lr: 0.700000
2019-12-27 10:48:48.827147: step 2840/136300 (epoch 3/100), loss = 0.487192 (0.250 sec/batch), lr: 0.700000
2019-12-27 10:48:53.652402: step 2860/136300 (epoch 3/100), loss = 0.294452 (0.220 sec/batch), lr: 0.700000
2019-12-27 10:48:58.488063: step 2880/136300 (epoch 3/100), loss = 0.520821 (0.243 sec/batch), lr: 0.700000
2019-12-27 10:49:03.408451: step 2900/136300 (epoch 3/100), loss = 0.473689 (0.242 sec/batch), lr: 0.700000
2019-12-27 10:49:08.310859: step 2920/136300 (epoch 3/100), loss = 0.446120 (0.217 sec/batch), lr: 0.700000
2019-12-27 10:49:13.227361: step 2940/136300 (epoch 3/100), loss = 0.366574 (0.244 sec/batch), lr: 0.700000
2019-12-27 10:49:17.963442: step 2960/136300 (epoch 3/100), loss = 0.579113 (0.227 sec/batch), lr: 0.700000
2019-12-27 10:49:24.465628: step 2980/136300 (epoch 3/100), loss = 0.575818 (0.241 sec/batch), lr: 0.700000
2019-12-27 10:49:29.379319: step 3000/136300 (epoch 3/100), loss = 0.438081 (0.225 sec/batch), lr: 0.700000
2019-12-27 10:49:34.256170: step 3020/136300 (epoch 3/100), loss = 0.367793 (0.241 sec/batch), lr: 0.700000
2019-12-27 10:49:39.009375: step 3040/136300 (epoch 3/100), loss = 0.367436 (0.245 sec/batch), lr: 0.700000
2019-12-27 10:49:43.862445: step 3060/136300 (epoch 3/100), loss = 0.707615 (0.235 sec/batch), lr: 0.700000
2019-12-27 10:49:48.710694: step 3080/136300 (epoch 3/100), loss = 0.244765 (0.208 sec/batch), lr: 0.700000
2019-12-27 10:49:53.518796: step 3100/136300 (epoch 3/100), loss = 0.312752 (0.232 sec/batch), lr: 0.700000
2019-12-27 10:49:58.334964: step 3120/136300 (epoch 3/100), loss = 0.613544 (0.249 sec/batch), lr: 0.700000
2019-12-27 10:50:03.130245: step 3140/136300 (epoch 3/100), loss = 0.441513 (0.246 sec/batch), lr: 0.700000
2019-12-27 10:50:07.993956: step 3160/136300 (epoch 3/100), loss = 0.495632 (0.233 sec/batch), lr: 0.700000
2019-12-27 10:50:12.939758: step 3180/136300 (epoch 3/100), loss = 0.724059 (0.249 sec/batch), lr: 0.700000
2019-12-27 10:50:19.233563: step 3200/136300 (epoch 3/100), loss = 0.916472 (0.233 sec/batch), lr: 0.700000
2019-12-27 10:50:24.105437: step 3220/136300 (epoch 3/100), loss = 0.560720 (0.237 sec/batch), lr: 0.700000
2019-12-27 10:50:29.034492: step 3240/136300 (epoch 3/100), loss = 0.341547 (0.234 sec/batch), lr: 0.700000
2019-12-27 10:50:33.942447: step 3260/136300 (epoch 3/100), loss = 0.363855 (0.246 sec/batch), lr: 0.700000
2019-12-27 10:50:38.786543: step 3280/136300 (epoch 3/100), loss = 0.542577 (0.201 sec/batch), lr: 0.700000
2019-12-27 10:50:43.692193: step 3300/136300 (epoch 3/100), loss = 0.467178 (0.242 sec/batch), lr: 0.700000
2019-12-27 10:50:48.518127: step 3320/136300 (epoch 3/100), loss = 0.487626 (0.237 sec/batch), lr: 0.700000
2019-12-27 10:50:53.241625: step 3340/136300 (epoch 3/100), loss = 0.569165 (0.253 sec/batch), lr: 0.700000
2019-12-27 10:50:58.091672: step 3360/136300 (epoch 3/100), loss = 0.259541 (0.251 sec/batch), lr: 0.700000
2019-12-27 10:51:04.355738: step 3380/136300 (epoch 3/100), loss = 0.342062 (0.244 sec/batch), lr: 0.700000
2019-12-27 10:51:09.218854: step 3400/136300 (epoch 3/100), loss = 0.492467 (0.235 sec/batch), lr: 0.700000
2019-12-27 10:51:14.173485: step 3420/136300 (epoch 3/100), loss = 0.375330 (0.245 sec/batch), lr: 0.700000
2019-12-27 10:51:19.077554: step 3440/136300 (epoch 3/100), loss = 0.509213 (0.231 sec/batch), lr: 0.700000
2019-12-27 10:51:24.062217: step 3460/136300 (epoch 3/100), loss = 0.651525 (0.237 sec/batch), lr: 0.700000
2019-12-27 10:51:28.950344: step 3480/136300 (epoch 3/100), loss = 0.612181 (0.247 sec/batch), lr: 0.700000
2019-12-27 10:51:33.739224: step 3500/136300 (epoch 3/100), loss = 0.506160 (0.241 sec/batch), lr: 0.700000
2019-12-27 10:51:38.704130: step 3520/136300 (epoch 3/100), loss = 0.516778 (0.223 sec/batch), lr: 0.700000
2019-12-27 10:51:43.622013: step 3540/136300 (epoch 3/100), loss = 0.791023 (0.224 sec/batch), lr: 0.700000
2019-12-27 10:51:49.946798: step 3560/136300 (epoch 3/100), loss = 0.521465 (0.228 sec/batch), lr: 0.700000
2019-12-27 10:51:54.892988: step 3580/136300 (epoch 3/100), loss = 0.547288 (0.231 sec/batch), lr: 0.700000
2019-12-27 10:51:59.620340: step 3600/136300 (epoch 3/100), loss = 0.549159 (0.245 sec/batch), lr: 0.700000
2019-12-27 10:52:04.513739: step 3620/136300 (epoch 3/100), loss = 0.376190 (0.232 sec/batch), lr: 0.700000
2019-12-27 10:52:09.465458: step 3640/136300 (epoch 3/100), loss = 0.408215 (0.222 sec/batch), lr: 0.700000
2019-12-27 10:52:14.269541: step 3660/136300 (epoch 3/100), loss = 0.563750 (0.212 sec/batch), lr: 0.700000
2019-12-27 10:52:19.239136: step 3680/136300 (epoch 3/100), loss = 0.489090 (0.217 sec/batch), lr: 0.700000
2019-12-27 10:52:24.140474: step 3700/136300 (epoch 3/100), loss = 0.413318 (0.215 sec/batch), lr: 0.700000
2019-12-27 10:52:29.010160: step 3720/136300 (epoch 3/100), loss = 0.543464 (0.221 sec/batch), lr: 0.700000
2019-12-27 10:52:33.882987: step 3740/136300 (epoch 3/100), loss = 0.544020 (0.201 sec/batch), lr: 0.700000
2019-12-27 10:52:40.222764: step 3760/136300 (epoch 3/100), loss = 0.285234 (0.240 sec/batch), lr: 0.700000
2019-12-27 10:52:44.908218: step 3780/136300 (epoch 3/100), loss = 0.207565 (0.213 sec/batch), lr: 0.700000
2019-12-27 10:52:49.688764: step 3800/136300 (epoch 3/100), loss = 0.503953 (0.243 sec/batch), lr: 0.700000
2019-12-27 10:52:54.528177: step 3820/136300 (epoch 3/100), loss = 0.671753 (0.205 sec/batch), lr: 0.700000
2019-12-27 10:52:59.380436: step 3840/136300 (epoch 3/100), loss = 0.469953 (0.242 sec/batch), lr: 0.700000
2019-12-27 10:53:04.289952: step 3860/136300 (epoch 3/100), loss = 0.463648 (0.236 sec/batch), lr: 0.700000
2019-12-27 10:53:09.065355: step 3880/136300 (epoch 3/100), loss = 0.403068 (0.240 sec/batch), lr: 0.700000
2019-12-27 10:53:13.769541: step 3900/136300 (epoch 3/100), loss = 0.456778 (0.208 sec/batch), lr: 0.700000
2019-12-27 10:53:18.597801: step 3920/136300 (epoch 3/100), loss = 0.535698 (0.215 sec/batch), lr: 0.700000
2019-12-27 10:53:23.515595: step 3940/136300 (epoch 3/100), loss = 0.523358 (0.198 sec/batch), lr: 0.700000
2019-12-27 10:53:29.964815: step 3960/136300 (epoch 3/100), loss = 0.450191 (1.714 sec/batch), lr: 0.700000
2019-12-27 10:53:34.864938: step 3980/136300 (epoch 3/100), loss = 0.510405 (0.218 sec/batch), lr: 0.700000
2019-12-27 10:53:39.539986: step 4000/136300 (epoch 3/100), loss = 0.494776 (0.247 sec/batch), lr: 0.700000
2019-12-27 10:53:44.350240: step 4020/136300 (epoch 3/100), loss = 0.475192 (0.230 sec/batch), lr: 0.700000
2019-12-27 10:53:49.176085: step 4040/136300 (epoch 3/100), loss = 0.615848 (0.228 sec/batch), lr: 0.700000
2019-12-27 10:53:54.103362: step 4060/136300 (epoch 3/100), loss = 0.148383 (0.230 sec/batch), lr: 0.700000
2019-12-27 10:53:58.984919: step 4080/136300 (epoch 3/100), loss = 0.521980 (0.235 sec/batch), lr: 0.700000
Evaluating on dev set...
Precision (micro): 72.311%
   Recall (micro): 31.659%
       F1 (micro): 44.038%
epoch 3: train_loss = 0.502150, dev_loss = 0.641256, dev_f1 = 0.4404
model saved to ./saved_models/01/checkpoint_epoch_3.pt
new best model saved.

2019-12-27 10:54:37.969187: step 4100/136300 (epoch 4/100), loss = 0.369517 (0.246 sec/batch), lr: 0.700000
2019-12-27 10:54:42.833881: step 4120/136300 (epoch 4/100), loss = 0.473340 (0.210 sec/batch), lr: 0.700000
2019-12-27 10:54:47.682149: step 4140/136300 (epoch 4/100), loss = 0.538140 (0.244 sec/batch), lr: 0.700000
2019-12-27 10:54:53.896021: step 4160/136300 (epoch 4/100), loss = 0.331838 (0.231 sec/batch), lr: 0.700000
2019-12-27 10:54:58.846862: step 4180/136300 (epoch 4/100), loss = 0.365485 (0.234 sec/batch), lr: 0.700000
2019-12-27 10:55:03.575222: step 4200/136300 (epoch 4/100), loss = 0.554001 (0.180 sec/batch), lr: 0.700000
2019-12-27 10:55:08.359891: step 4220/136300 (epoch 4/100), loss = 0.636522 (0.212 sec/batch), lr: 0.700000
2019-12-27 10:55:13.203317: step 4240/136300 (epoch 4/100), loss = 0.421537 (0.249 sec/batch), lr: 0.700000
2019-12-27 10:55:18.028491: step 4260/136300 (epoch 4/100), loss = 0.937625 (0.214 sec/batch), lr: 0.700000
2019-12-27 10:55:22.926751: step 4280/136300 (epoch 4/100), loss = 0.345091 (0.231 sec/batch), lr: 0.700000
2019-12-27 10:55:27.734967: step 4300/136300 (epoch 4/100), loss = 0.482829 (0.239 sec/batch), lr: 0.700000
2019-12-27 10:55:32.481704: step 4320/136300 (epoch 4/100), loss = 0.628828 (0.245 sec/batch), lr: 0.700000
2019-12-27 10:55:38.625646: step 4340/136300 (epoch 4/100), loss = 0.559775 (0.239 sec/batch), lr: 0.700000
2019-12-27 10:55:43.518340: step 4360/136300 (epoch 4/100), loss = 0.560396 (0.235 sec/batch), lr: 0.700000
2019-12-27 10:55:48.354326: step 4380/136300 (epoch 4/100), loss = 0.427198 (0.243 sec/batch), lr: 0.700000
2019-12-27 10:55:53.097861: step 4400/136300 (epoch 4/100), loss = 0.165333 (0.236 sec/batch), lr: 0.700000
2019-12-27 10:55:57.877160: step 4420/136300 (epoch 4/100), loss = 0.501144 (0.221 sec/batch), lr: 0.700000
2019-12-27 10:56:02.726161: step 4440/136300 (epoch 4/100), loss = 0.470349 (0.231 sec/batch), lr: 0.700000
2019-12-27 10:56:07.573736: step 4460/136300 (epoch 4/100), loss = 0.416150 (0.231 sec/batch), lr: 0.700000
2019-12-27 10:56:12.285506: step 4480/136300 (epoch 4/100), loss = 0.224750 (0.216 sec/batch), lr: 0.700000
2019-12-27 10:56:17.005870: step 4500/136300 (epoch 4/100), loss = 0.724177 (0.216 sec/batch), lr: 0.700000
2019-12-27 10:56:21.814679: step 4520/136300 (epoch 4/100), loss = 0.710186 (0.241 sec/batch), lr: 0.700000
2019-12-27 10:56:28.170246: step 4540/136300 (epoch 4/100), loss = 0.525027 (0.230 sec/batch), lr: 0.700000
2019-12-27 10:56:33.005620: step 4560/136300 (epoch 4/100), loss = 0.801361 (0.194 sec/batch), lr: 0.700000
2019-12-27 10:56:37.798178: step 4580/136300 (epoch 4/100), loss = 0.416247 (0.247 sec/batch), lr: 0.700000
2019-12-27 10:56:42.646990: step 4600/136300 (epoch 4/100), loss = 0.638085 (0.220 sec/batch), lr: 0.700000
2019-12-27 10:56:47.530550: step 4620/136300 (epoch 4/100), loss = 0.958893 (0.239 sec/batch), lr: 0.700000
2019-12-27 10:56:52.385791: step 4640/136300 (epoch 4/100), loss = 0.418736 (0.234 sec/batch), lr: 0.700000
2019-12-27 10:56:57.188326: step 4660/136300 (epoch 4/100), loss = 0.235015 (0.230 sec/batch), lr: 0.700000
2019-12-27 10:57:02.043543: step 4680/136300 (epoch 4/100), loss = 0.467594 (0.249 sec/batch), lr: 0.700000
2019-12-27 10:57:06.703748: step 4700/136300 (epoch 4/100), loss = 0.630474 (0.244 sec/batch), lr: 0.700000
2019-12-27 10:57:11.499265: step 4720/136300 (epoch 4/100), loss = 0.253096 (0.214 sec/batch), lr: 0.700000
2019-12-27 10:57:17.701772: step 4740/136300 (epoch 4/100), loss = 0.363155 (0.234 sec/batch), lr: 0.700000
2019-12-27 10:57:22.518028: step 4760/136300 (epoch 4/100), loss = 0.545439 (0.206 sec/batch), lr: 0.700000
2019-12-27 10:57:27.450043: step 4780/136300 (epoch 4/100), loss = 0.333431 (0.229 sec/batch), lr: 0.700000
2019-12-27 10:57:32.347064: step 4800/136300 (epoch 4/100), loss = 0.388352 (0.235 sec/batch), lr: 0.700000
2019-12-27 10:57:37.256643: step 4820/136300 (epoch 4/100), loss = 0.422804 (0.236 sec/batch), lr: 0.700000
2019-12-27 10:57:42.118706: step 4840/136300 (epoch 4/100), loss = 0.484064 (0.205 sec/batch), lr: 0.700000
2019-12-27 10:57:46.892899: step 4860/136300 (epoch 4/100), loss = 0.389522 (0.253 sec/batch), lr: 0.700000
2019-12-27 10:57:51.790453: step 4880/136300 (epoch 4/100), loss = 0.388904 (0.249 sec/batch), lr: 0.700000
2019-12-27 10:57:56.706266: step 4900/136300 (epoch 4/100), loss = 0.684984 (0.238 sec/batch), lr: 0.700000
2019-12-27 10:58:01.474748: step 4920/136300 (epoch 4/100), loss = 0.541813 (0.181 sec/batch), lr: 0.700000
2019-12-27 10:58:07.947919: step 4940/136300 (epoch 4/100), loss = 0.156357 (0.225 sec/batch), lr: 0.700000
2019-12-27 10:58:12.560111: step 4960/136300 (epoch 4/100), loss = 0.247515 (0.170 sec/batch), lr: 0.700000
2019-12-27 10:58:17.445623: step 4980/136300 (epoch 4/100), loss = 0.425088 (0.203 sec/batch), lr: 0.700000
2019-12-27 10:58:22.383624: step 5000/136300 (epoch 4/100), loss = 0.302036 (0.235 sec/batch), lr: 0.700000
2019-12-27 10:58:27.166446: step 5020/136300 (epoch 4/100), loss = 0.433289 (0.205 sec/batch), lr: 0.700000
2019-12-27 10:58:32.109472: step 5040/136300 (epoch 4/100), loss = 0.536665 (0.236 sec/batch), lr: 0.700000
2019-12-27 10:58:37.034105: step 5060/136300 (epoch 4/100), loss = 0.496049 (0.229 sec/batch), lr: 0.700000
2019-12-27 10:58:41.866457: step 5080/136300 (epoch 4/100), loss = 0.718969 (0.246 sec/batch), lr: 0.700000
2019-12-27 10:58:46.730355: step 5100/136300 (epoch 4/100), loss = 0.555184 (0.225 sec/batch), lr: 0.700000
2019-12-27 10:58:53.224670: step 5120/136300 (epoch 4/100), loss = 0.797059 (0.246 sec/batch), lr: 0.700000
2019-12-27 10:58:57.858434: step 5140/136300 (epoch 4/100), loss = 0.596033 (0.240 sec/batch), lr: 0.700000
2019-12-27 10:59:02.659017: step 5160/136300 (epoch 4/100), loss = 0.418115 (0.247 sec/batch), lr: 0.700000
2019-12-27 10:59:07.496586: step 5180/136300 (epoch 4/100), loss = 0.238878 (0.203 sec/batch), lr: 0.700000
2019-12-27 10:59:12.367219: step 5200/136300 (epoch 4/100), loss = 0.519038 (0.245 sec/batch), lr: 0.700000
2019-12-27 10:59:17.276005: step 5220/136300 (epoch 4/100), loss = 0.671853 (0.223 sec/batch), lr: 0.700000
2019-12-27 10:59:22.042213: step 5240/136300 (epoch 4/100), loss = 0.386349 (0.199 sec/batch), lr: 0.700000
2019-12-27 10:59:26.808224: step 5260/136300 (epoch 4/100), loss = 0.712621 (0.243 sec/batch), lr: 0.700000
2019-12-27 10:59:31.611428: step 5280/136300 (epoch 4/100), loss = 0.616525 (0.245 sec/batch), lr: 0.700000
2019-12-27 10:59:36.540778: step 5300/136300 (epoch 4/100), loss = 0.460843 (0.216 sec/batch), lr: 0.700000
2019-12-27 10:59:43.058390: step 5320/136300 (epoch 4/100), loss = 0.500195 (0.201 sec/batch), lr: 0.700000
2019-12-27 10:59:48.007445: step 5340/136300 (epoch 4/100), loss = 0.500056 (0.235 sec/batch), lr: 0.700000
2019-12-27 10:59:52.634677: step 5360/136300 (epoch 4/100), loss = 0.419859 (0.195 sec/batch), lr: 0.700000
2019-12-27 10:59:57.489855: step 5380/136300 (epoch 4/100), loss = 0.650193 (0.239 sec/batch), lr: 0.700000
2019-12-27 11:00:02.288038: step 5400/136300 (epoch 4/100), loss = 0.508509 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:00:07.257868: step 5420/136300 (epoch 4/100), loss = 0.291165 (0.247 sec/batch), lr: 0.700000
2019-12-27 11:00:12.110580: step 5440/136300 (epoch 4/100), loss = 0.384961 (0.242 sec/batch), lr: 0.700000
Evaluating on dev set...
Precision (micro): 70.802%
   Recall (micro): 36.221%
       F1 (micro): 47.925%
epoch 4: train_loss = 0.475402, dev_loss = 0.573958, dev_f1 = 0.4793
model saved to ./saved_models/01/checkpoint_epoch_4.pt
new best model saved.

2019-12-27 11:00:51.074031: step 5460/136300 (epoch 5/100), loss = 0.418506 (0.206 sec/batch), lr: 0.700000
2019-12-27 11:00:55.871933: step 5480/136300 (epoch 5/100), loss = 0.339212 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:01:00.754697: step 5500/136300 (epoch 5/100), loss = 0.529209 (0.246 sec/batch), lr: 0.700000
2019-12-27 11:01:07.124724: step 5520/136300 (epoch 5/100), loss = 0.578836 (0.205 sec/batch), lr: 0.700000
2019-12-27 11:01:12.093908: step 5540/136300 (epoch 5/100), loss = 0.451834 (0.240 sec/batch), lr: 0.700000
2019-12-27 11:01:16.951942: step 5560/136300 (epoch 5/100), loss = 0.351349 (0.236 sec/batch), lr: 0.700000
2019-12-27 11:01:21.726431: step 5580/136300 (epoch 5/100), loss = 0.570301 (0.232 sec/batch), lr: 0.700000
2019-12-27 11:01:26.634750: step 5600/136300 (epoch 5/100), loss = 0.358591 (0.230 sec/batch), lr: 0.700000
2019-12-27 11:01:31.447152: step 5620/136300 (epoch 5/100), loss = 0.307213 (0.212 sec/batch), lr: 0.700000
2019-12-27 11:01:36.361515: step 5640/136300 (epoch 5/100), loss = 0.419361 (0.206 sec/batch), lr: 0.700000
2019-12-27 11:01:41.237583: step 5660/136300 (epoch 5/100), loss = 0.527340 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:01:45.984497: step 5680/136300 (epoch 5/100), loss = 0.683779 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:01:52.360667: step 5700/136300 (epoch 5/100), loss = 0.603376 (1.733 sec/batch), lr: 0.700000
2019-12-27 11:01:57.190871: step 5720/136300 (epoch 5/100), loss = 0.316810 (0.246 sec/batch), lr: 0.700000
2019-12-27 11:02:01.999778: step 5740/136300 (epoch 5/100), loss = 0.494493 (0.243 sec/batch), lr: 0.700000
2019-12-27 11:02:06.831876: step 5760/136300 (epoch 5/100), loss = 0.472892 (0.230 sec/batch), lr: 0.700000
2019-12-27 11:02:11.617694: step 5780/136300 (epoch 5/100), loss = 0.316506 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:02:16.446590: step 5800/136300 (epoch 5/100), loss = 0.306859 (0.207 sec/batch), lr: 0.700000
2019-12-27 11:02:21.374898: step 5820/136300 (epoch 5/100), loss = 0.539671 (0.249 sec/batch), lr: 0.700000
2019-12-27 11:02:26.114008: step 5840/136300 (epoch 5/100), loss = 0.422919 (0.177 sec/batch), lr: 0.700000
2019-12-27 11:02:30.829210: step 5860/136300 (epoch 5/100), loss = 0.463722 (0.250 sec/batch), lr: 0.700000
2019-12-27 11:02:35.626413: step 5880/136300 (epoch 5/100), loss = 0.329790 (0.215 sec/batch), lr: 0.700000
2019-12-27 11:02:42.072020: step 5900/136300 (epoch 5/100), loss = 0.512014 (0.219 sec/batch), lr: 0.700000
2019-12-27 11:02:46.934482: step 5920/136300 (epoch 5/100), loss = 0.343294 (0.175 sec/batch), lr: 0.700000
2019-12-27 11:02:51.762251: step 5940/136300 (epoch 5/100), loss = 0.555872 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:02:56.645694: step 5960/136300 (epoch 5/100), loss = 0.421184 (0.239 sec/batch), lr: 0.700000
2019-12-27 11:03:01.504712: step 5980/136300 (epoch 5/100), loss = 0.575567 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:03:06.452507: step 6000/136300 (epoch 5/100), loss = 0.278260 (0.182 sec/batch), lr: 0.700000
2019-12-27 11:03:11.252117: step 6020/136300 (epoch 5/100), loss = 0.763482 (0.170 sec/batch), lr: 0.700000
2019-12-27 11:03:16.075894: step 6040/136300 (epoch 5/100), loss = 0.397110 (0.229 sec/batch), lr: 0.700000
2019-12-27 11:03:20.797825: step 6060/136300 (epoch 5/100), loss = 0.490951 (0.178 sec/batch), lr: 0.700000
2019-12-27 11:03:25.598510: step 6080/136300 (epoch 5/100), loss = 0.595645 (0.219 sec/batch), lr: 0.700000
2019-12-27 11:03:31.772924: step 6100/136300 (epoch 5/100), loss = 0.495910 (0.205 sec/batch), lr: 0.700000
2019-12-27 11:03:36.603147: step 6120/136300 (epoch 5/100), loss = 0.418126 (0.216 sec/batch), lr: 0.700000
2019-12-27 11:03:41.533308: step 6140/136300 (epoch 5/100), loss = 0.499102 (0.252 sec/batch), lr: 0.700000
2019-12-27 11:03:46.451252: step 6160/136300 (epoch 5/100), loss = 0.408538 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:03:51.375072: step 6180/136300 (epoch 5/100), loss = 0.367504 (0.233 sec/batch), lr: 0.700000
2019-12-27 11:03:56.279044: step 6200/136300 (epoch 5/100), loss = 0.640397 (0.219 sec/batch), lr: 0.700000
2019-12-27 11:04:00.996763: step 6220/136300 (epoch 5/100), loss = 0.466284 (0.173 sec/batch), lr: 0.700000
2019-12-27 11:04:05.942157: step 6240/136300 (epoch 5/100), loss = 0.571226 (0.242 sec/batch), lr: 0.700000
2019-12-27 11:04:10.906110: step 6260/136300 (epoch 5/100), loss = 0.424948 (0.220 sec/batch), lr: 0.700000
2019-12-27 11:04:17.142837: step 6280/136300 (epoch 5/100), loss = 0.595296 (1.703 sec/batch), lr: 0.700000
2019-12-27 11:04:22.106586: step 6300/136300 (epoch 5/100), loss = 0.638142 (0.226 sec/batch), lr: 0.700000
2019-12-27 11:04:26.841534: step 6320/136300 (epoch 5/100), loss = 0.386235 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:04:31.636705: step 6340/136300 (epoch 5/100), loss = 0.379710 (0.240 sec/batch), lr: 0.700000
2019-12-27 11:04:36.538424: step 6360/136300 (epoch 5/100), loss = 0.442675 (0.242 sec/batch), lr: 0.700000
2019-12-27 11:04:41.359184: step 6380/136300 (epoch 5/100), loss = 0.757746 (0.235 sec/batch), lr: 0.700000
2019-12-27 11:04:46.247438: step 6400/136300 (epoch 5/100), loss = 0.810186 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:04:51.218614: step 6420/136300 (epoch 5/100), loss = 0.537212 (0.246 sec/batch), lr: 0.700000
2019-12-27 11:04:56.020860: step 6440/136300 (epoch 5/100), loss = 0.433835 (0.177 sec/batch), lr: 0.700000
2019-12-27 11:05:00.882280: step 6460/136300 (epoch 5/100), loss = 0.549236 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:05:07.076301: step 6480/136300 (epoch 5/100), loss = 0.444641 (0.240 sec/batch), lr: 0.700000
2019-12-27 11:05:11.689000: step 6500/136300 (epoch 5/100), loss = 0.392912 (0.236 sec/batch), lr: 0.700000
2019-12-27 11:05:16.426418: step 6520/136300 (epoch 5/100), loss = 0.643774 (0.181 sec/batch), lr: 0.700000
2019-12-27 11:05:21.320986: step 6540/136300 (epoch 5/100), loss = 0.891244 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:05:26.114493: step 6560/136300 (epoch 5/100), loss = 0.459395 (0.204 sec/batch), lr: 0.700000
2019-12-27 11:05:31.042481: step 6580/136300 (epoch 5/100), loss = 0.844358 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:05:35.825506: step 6600/136300 (epoch 5/100), loss = 0.491235 (0.229 sec/batch), lr: 0.700000
2019-12-27 11:05:40.585765: step 6620/136300 (epoch 5/100), loss = 0.351561 (0.212 sec/batch), lr: 0.700000
2019-12-27 11:05:45.385303: step 6640/136300 (epoch 5/100), loss = 0.626147 (0.236 sec/batch), lr: 0.700000
2019-12-27 11:05:50.311520: step 6660/136300 (epoch 5/100), loss = 0.363094 (0.204 sec/batch), lr: 0.700000
2019-12-27 11:05:56.772167: step 6680/136300 (epoch 5/100), loss = 0.337208 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:06:01.663099: step 6700/136300 (epoch 5/100), loss = 0.411530 (0.243 sec/batch), lr: 0.700000
2019-12-27 11:06:06.400066: step 6720/136300 (epoch 5/100), loss = 0.432718 (0.220 sec/batch), lr: 0.700000
2019-12-27 11:06:11.219672: step 6740/136300 (epoch 5/100), loss = 0.726382 (0.249 sec/batch), lr: 0.700000
2019-12-27 11:06:15.970976: step 6760/136300 (epoch 5/100), loss = 0.432417 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:06:20.948328: step 6780/136300 (epoch 5/100), loss = 0.386786 (0.245 sec/batch), lr: 0.700000
2019-12-27 11:06:25.811889: step 6800/136300 (epoch 5/100), loss = 0.262123 (0.198 sec/batch), lr: 0.700000
Evaluating on dev set...
Precision (micro): 75.836%
   Recall (micro): 31.696%
       F1 (micro): 44.707%
epoch 5: train_loss = 0.460429, dev_loss = 0.647275, dev_f1 = 0.4471
model saved to ./saved_models/01/checkpoint_epoch_5.pt

2019-12-27 11:07:04.564838: step 6820/136300 (epoch 6/100), loss = 0.325911 (0.246 sec/batch), lr: 0.700000
2019-12-27 11:07:09.385431: step 6840/136300 (epoch 6/100), loss = 0.404608 (0.225 sec/batch), lr: 0.700000
2019-12-27 11:07:15.692225: step 6860/136300 (epoch 6/100), loss = 0.391428 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:07:20.449129: step 6880/136300 (epoch 6/100), loss = 0.428417 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:07:25.364688: step 6900/136300 (epoch 6/100), loss = 0.451229 (0.220 sec/batch), lr: 0.700000
2019-12-27 11:07:30.169587: step 6920/136300 (epoch 6/100), loss = 0.453369 (0.240 sec/batch), lr: 0.700000
2019-12-27 11:07:34.951037: step 6940/136300 (epoch 6/100), loss = 0.498002 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:07:39.823517: step 6960/136300 (epoch 6/100), loss = 0.604097 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:07:44.635766: step 6980/136300 (epoch 6/100), loss = 0.314759 (0.211 sec/batch), lr: 0.700000
2019-12-27 11:07:49.537337: step 7000/136300 (epoch 6/100), loss = 0.695552 (0.233 sec/batch), lr: 0.700000
2019-12-27 11:07:54.365000: step 7020/136300 (epoch 6/100), loss = 0.298672 (0.248 sec/batch), lr: 0.700000
2019-12-27 11:07:59.125349: step 7040/136300 (epoch 6/100), loss = 0.384824 (0.240 sec/batch), lr: 0.700000
2019-12-27 11:08:03.965768: step 7060/136300 (epoch 6/100), loss = 0.588052 (0.219 sec/batch), lr: 0.700000
2019-12-27 11:08:10.201354: step 7080/136300 (epoch 6/100), loss = 0.457129 (0.228 sec/batch), lr: 0.700000
2019-12-27 11:08:15.013155: step 7100/136300 (epoch 6/100), loss = 0.621527 (0.221 sec/batch), lr: 0.700000
2019-12-27 11:08:19.868181: step 7120/136300 (epoch 6/100), loss = 0.623779 (0.195 sec/batch), lr: 0.700000
2019-12-27 11:08:24.609126: step 7140/136300 (epoch 6/100), loss = 0.504378 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:08:29.427457: step 7160/136300 (epoch 6/100), loss = 0.448616 (0.216 sec/batch), lr: 0.700000
2019-12-27 11:08:34.356773: step 7180/136300 (epoch 6/100), loss = 0.523120 (0.221 sec/batch), lr: 0.700000
2019-12-27 11:08:39.119290: step 7200/136300 (epoch 6/100), loss = 0.203249 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:08:43.749354: step 7220/136300 (epoch 6/100), loss = 0.543873 (0.213 sec/batch), lr: 0.700000
2019-12-27 11:08:48.550424: step 7240/136300 (epoch 6/100), loss = 0.245032 (0.205 sec/batch), lr: 0.700000
2019-12-27 11:08:53.527245: step 7260/136300 (epoch 6/100), loss = 0.658501 (0.233 sec/batch), lr: 0.700000
2019-12-27 11:08:58.439607: step 7280/136300 (epoch 6/100), loss = 0.358904 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:09:04.528484: step 7300/136300 (epoch 6/100), loss = 0.369393 (0.243 sec/batch), lr: 0.700000
2019-12-27 11:09:09.367232: step 7320/136300 (epoch 6/100), loss = 0.475790 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:09:14.192698: step 7340/136300 (epoch 6/100), loss = 0.618796 (0.198 sec/batch), lr: 0.700000
2019-12-27 11:09:19.161025: step 7360/136300 (epoch 6/100), loss = 0.359428 (0.245 sec/batch), lr: 0.700000
2019-12-27 11:09:23.942987: step 7380/136300 (epoch 6/100), loss = 0.254278 (0.247 sec/batch), lr: 0.700000
2019-12-27 11:09:28.741083: step 7400/136300 (epoch 6/100), loss = 0.458278 (0.211 sec/batch), lr: 0.700000
2019-12-27 11:09:33.537710: step 7420/136300 (epoch 6/100), loss = 0.592825 (0.244 sec/batch), lr: 0.700000
2019-12-27 11:09:38.306411: step 7440/136300 (epoch 6/100), loss = 0.295957 (0.233 sec/batch), lr: 0.700000
2019-12-27 11:09:43.071527: step 7460/136300 (epoch 6/100), loss = 0.396601 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:09:47.886672: step 7480/136300 (epoch 6/100), loss = 0.501503 (0.227 sec/batch), lr: 0.700000
2019-12-27 11:09:54.241735: step 7500/136300 (epoch 6/100), loss = 0.669146 (0.219 sec/batch), lr: 0.700000
2019-12-27 11:09:59.181328: step 7520/136300 (epoch 6/100), loss = 0.439402 (0.245 sec/batch), lr: 0.700000
2019-12-27 11:10:04.079709: step 7540/136300 (epoch 6/100), loss = 0.439777 (0.223 sec/batch), lr: 0.700000
2019-12-27 11:10:09.009311: step 7560/136300 (epoch 6/100), loss = 0.397001 (0.248 sec/batch), lr: 0.700000
2019-12-27 11:10:13.762499: step 7580/136300 (epoch 6/100), loss = 0.451024 (0.225 sec/batch), lr: 0.700000
2019-12-27 11:10:18.553199: step 7600/136300 (epoch 6/100), loss = 0.449498 (0.182 sec/batch), lr: 0.700000
2019-12-27 11:10:23.544812: step 7620/136300 (epoch 6/100), loss = 0.291579 (0.222 sec/batch), lr: 0.700000
2019-12-27 11:10:28.331773: step 7640/136300 (epoch 6/100), loss = 0.226318 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:10:33.280028: step 7660/136300 (epoch 6/100), loss = 0.409438 (0.249 sec/batch), lr: 0.700000
2019-12-27 11:10:38.041484: step 7680/136300 (epoch 6/100), loss = 0.743072 (0.208 sec/batch), lr: 0.700000
2019-12-27 11:10:44.237224: step 7700/136300 (epoch 6/100), loss = 0.572966 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:10:49.130408: step 7720/136300 (epoch 6/100), loss = 0.423577 (0.242 sec/batch), lr: 0.700000
2019-12-27 11:10:54.063772: step 7740/136300 (epoch 6/100), loss = 0.201139 (0.219 sec/batch), lr: 0.700000
2019-12-27 11:10:58.865908: step 7760/136300 (epoch 6/100), loss = 0.605728 (0.235 sec/batch), lr: 0.700000
2019-12-27 11:11:03.840915: step 7780/136300 (epoch 6/100), loss = 0.496606 (0.250 sec/batch), lr: 0.700000
2019-12-27 11:11:08.728053: step 7800/136300 (epoch 6/100), loss = 0.536904 (0.220 sec/batch), lr: 0.700000
2019-12-27 11:11:13.534124: step 7820/136300 (epoch 6/100), loss = 0.467161 (0.242 sec/batch), lr: 0.700000
2019-12-27 11:11:18.439370: step 7840/136300 (epoch 6/100), loss = 0.388396 (0.236 sec/batch), lr: 0.700000
2019-12-27 11:11:23.098941: step 7860/136300 (epoch 6/100), loss = 0.604471 (0.232 sec/batch), lr: 0.700000
2019-12-27 11:11:27.884719: step 7880/136300 (epoch 6/100), loss = 0.629705 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:11:32.755628: step 7900/136300 (epoch 6/100), loss = 0.382962 (0.206 sec/batch), lr: 0.700000
2019-12-27 11:11:39.018464: step 7920/136300 (epoch 6/100), loss = 0.387711 (0.235 sec/batch), lr: 0.700000
2019-12-27 11:11:43.920182: step 7940/136300 (epoch 6/100), loss = 0.525409 (0.236 sec/batch), lr: 0.700000
2019-12-27 11:11:48.674459: step 7960/136300 (epoch 6/100), loss = 0.347254 (0.223 sec/batch), lr: 0.700000
2019-12-27 11:11:53.413109: step 7980/136300 (epoch 6/100), loss = 0.534413 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:11:58.192276: step 8000/136300 (epoch 6/100), loss = 0.736925 (0.235 sec/batch), lr: 0.700000
2019-12-27 11:12:03.049684: step 8020/136300 (epoch 6/100), loss = 0.262949 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:12:07.981598: step 8040/136300 (epoch 6/100), loss = 0.589109 (0.249 sec/batch), lr: 0.700000
2019-12-27 11:12:12.877400: step 8060/136300 (epoch 6/100), loss = 0.528510 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:12:17.698641: step 8080/136300 (epoch 6/100), loss = 0.650059 (0.244 sec/batch), lr: 0.700000
2019-12-27 11:12:22.411209: step 8100/136300 (epoch 6/100), loss = 0.396566 (0.232 sec/batch), lr: 0.700000
2019-12-27 11:12:28.478410: step 8120/136300 (epoch 6/100), loss = 0.279921 (0.236 sec/batch), lr: 0.700000
2019-12-27 11:12:33.368738: step 8140/136300 (epoch 6/100), loss = 0.375957 (0.236 sec/batch), lr: 0.700000
2019-12-27 11:12:38.277507: step 8160/136300 (epoch 6/100), loss = 0.533278 (0.232 sec/batch), lr: 0.700000
Evaluating on dev set...
Precision (micro): 73.506%
   Recall (micro): 37.564%
       F1 (micro): 49.720%
epoch 6: train_loss = 0.450546, dev_loss = 0.623427, dev_f1 = 0.4972
model saved to ./saved_models/01/checkpoint_epoch_6.pt
new best model saved.

2019-12-27 11:13:16.784941: step 8180/136300 (epoch 7/100), loss = 0.615580 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:13:21.566572: step 8200/136300 (epoch 7/100), loss = 0.523737 (0.214 sec/batch), lr: 0.700000
2019-12-27 11:13:26.358520: step 8220/136300 (epoch 7/100), loss = 0.499983 (0.219 sec/batch), lr: 0.700000
2019-12-27 11:13:31.201063: step 8240/136300 (epoch 7/100), loss = 0.317741 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:13:36.071826: step 8260/136300 (epoch 7/100), loss = 0.371038 (0.205 sec/batch), lr: 0.700000
2019-12-27 11:13:40.893229: step 8280/136300 (epoch 7/100), loss = 0.336595 (0.227 sec/batch), lr: 0.700000
2019-12-27 11:13:46.763565: step 8300/136300 (epoch 7/100), loss = 0.320063 (0.246 sec/batch), lr: 0.700000
2019-12-27 11:13:51.610994: step 8320/136300 (epoch 7/100), loss = 0.516421 (0.236 sec/batch), lr: 0.700000
2019-12-27 11:13:56.429495: step 8340/136300 (epoch 7/100), loss = 0.234043 (0.225 sec/batch), lr: 0.700000
2019-12-27 11:14:01.334262: step 8360/136300 (epoch 7/100), loss = 0.371312 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:14:06.170883: step 8380/136300 (epoch 7/100), loss = 0.402527 (0.229 sec/batch), lr: 0.700000
2019-12-27 11:14:10.951292: step 8400/136300 (epoch 7/100), loss = 0.526297 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:14:15.731560: step 8420/136300 (epoch 7/100), loss = 0.635328 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:14:20.570541: step 8440/136300 (epoch 7/100), loss = 0.685541 (0.216 sec/batch), lr: 0.700000
2019-12-27 11:14:25.396982: step 8460/136300 (epoch 7/100), loss = 0.532555 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:14:30.282793: step 8480/136300 (epoch 7/100), loss = 0.533721 (0.249 sec/batch), lr: 0.700000
2019-12-27 11:14:36.532722: step 8500/136300 (epoch 7/100), loss = 0.438892 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:14:41.274882: step 8520/136300 (epoch 7/100), loss = 0.434404 (0.229 sec/batch), lr: 0.700000
2019-12-27 11:14:46.168732: step 8540/136300 (epoch 7/100), loss = 0.395357 (0.240 sec/batch), lr: 0.700000
2019-12-27 11:14:50.913207: step 8560/136300 (epoch 7/100), loss = 0.330902 (0.219 sec/batch), lr: 0.700000
2019-12-27 11:14:55.650333: step 8580/136300 (epoch 7/100), loss = 0.407121 (0.228 sec/batch), lr: 0.700000
2019-12-27 11:15:00.416709: step 8600/136300 (epoch 7/100), loss = 0.358938 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:15:05.326712: step 8620/136300 (epoch 7/100), loss = 0.582226 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:15:10.239281: step 8640/136300 (epoch 7/100), loss = 0.442059 (0.233 sec/batch), lr: 0.700000
2019-12-27 11:15:15.028190: step 8660/136300 (epoch 7/100), loss = 0.230259 (0.214 sec/batch), lr: 0.700000
2019-12-27 11:15:19.828277: step 8680/136300 (epoch 7/100), loss = 0.621121 (0.224 sec/batch), lr: 0.700000
2019-12-27 11:15:25.877919: step 8700/136300 (epoch 7/100), loss = 0.324542 (0.246 sec/batch), lr: 0.700000
2019-12-27 11:15:30.841920: step 8720/136300 (epoch 7/100), loss = 0.473174 (0.236 sec/batch), lr: 0.700000
2019-12-27 11:15:35.579703: step 8740/136300 (epoch 7/100), loss = 0.358251 (0.243 sec/batch), lr: 0.700000
2019-12-27 11:15:40.370851: step 8760/136300 (epoch 7/100), loss = 0.406421 (0.219 sec/batch), lr: 0.700000
2019-12-27 11:15:45.080217: step 8780/136300 (epoch 7/100), loss = 0.329649 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:15:49.859110: step 8800/136300 (epoch 7/100), loss = 0.395950 (0.222 sec/batch), lr: 0.700000
2019-12-27 11:15:54.679110: step 8820/136300 (epoch 7/100), loss = 0.528714 (0.219 sec/batch), lr: 0.700000
2019-12-27 11:15:59.485872: step 8840/136300 (epoch 7/100), loss = 0.375281 (0.224 sec/batch), lr: 0.700000
2019-12-27 11:16:04.361885: step 8860/136300 (epoch 7/100), loss = 0.762243 (0.206 sec/batch), lr: 0.700000
2019-12-27 11:16:09.305924: step 8880/136300 (epoch 7/100), loss = 0.431066 (0.209 sec/batch), lr: 0.700000
2019-12-27 11:16:15.678705: step 8900/136300 (epoch 7/100), loss = 0.477405 (0.249 sec/batch), lr: 0.700000
2019-12-27 11:16:20.557898: step 8920/136300 (epoch 7/100), loss = 0.498740 (0.250 sec/batch), lr: 0.700000
2019-12-27 11:16:25.321385: step 8940/136300 (epoch 7/100), loss = 0.483686 (0.214 sec/batch), lr: 0.700000
2019-12-27 11:16:30.161678: step 8960/136300 (epoch 7/100), loss = 0.319063 (0.246 sec/batch), lr: 0.700000
2019-12-27 11:16:35.107453: step 8980/136300 (epoch 7/100), loss = 0.431083 (0.220 sec/batch), lr: 0.700000
2019-12-27 11:16:39.898971: step 9000/136300 (epoch 7/100), loss = 0.481794 (0.203 sec/batch), lr: 0.700000
2019-12-27 11:16:44.838656: step 9020/136300 (epoch 7/100), loss = 0.364912 (0.253 sec/batch), lr: 0.700000
2019-12-27 11:16:49.704733: step 9040/136300 (epoch 7/100), loss = 0.414955 (0.216 sec/batch), lr: 0.700000
2019-12-27 11:16:54.434387: step 9060/136300 (epoch 7/100), loss = 0.582256 (0.211 sec/batch), lr: 0.700000
2019-12-27 11:16:59.324071: step 9080/136300 (epoch 7/100), loss = 0.305831 (0.232 sec/batch), lr: 0.700000
2019-12-27 11:17:05.650139: step 9100/136300 (epoch 7/100), loss = 0.190843 (0.223 sec/batch), lr: 0.700000
2019-12-27 11:17:10.462736: step 9120/136300 (epoch 7/100), loss = 0.527538 (0.248 sec/batch), lr: 0.700000
2019-12-27 11:17:15.471686: step 9140/136300 (epoch 7/100), loss = 0.479902 (0.247 sec/batch), lr: 0.700000
2019-12-27 11:17:20.350421: step 9160/136300 (epoch 7/100), loss = 0.573302 (0.242 sec/batch), lr: 0.700000
2019-12-27 11:17:25.177444: step 9180/136300 (epoch 7/100), loss = 0.624704 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:17:30.026034: step 9200/136300 (epoch 7/100), loss = 0.209745 (0.223 sec/batch), lr: 0.700000
2019-12-27 11:17:34.716022: step 9220/136300 (epoch 7/100), loss = 0.490872 (0.200 sec/batch), lr: 0.700000
2019-12-27 11:17:39.480143: step 9240/136300 (epoch 7/100), loss = 0.590481 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:17:44.349239: step 9260/136300 (epoch 7/100), loss = 0.171623 (0.230 sec/batch), lr: 0.700000
2019-12-27 11:17:49.130498: step 9280/136300 (epoch 7/100), loss = 0.332156 (0.233 sec/batch), lr: 0.700000
2019-12-27 11:17:55.417166: step 9300/136300 (epoch 7/100), loss = 0.793217 (0.245 sec/batch), lr: 0.700000
2019-12-27 11:18:00.281138: step 9320/136300 (epoch 7/100), loss = 0.357962 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:18:05.032378: step 9340/136300 (epoch 7/100), loss = 0.289075 (0.248 sec/batch), lr: 0.700000
2019-12-27 11:18:09.788965: step 9360/136300 (epoch 7/100), loss = 0.321719 (0.244 sec/batch), lr: 0.700000
2019-12-27 11:18:14.609489: step 9380/136300 (epoch 7/100), loss = 0.428722 (0.240 sec/batch), lr: 0.700000
2019-12-27 11:18:19.517255: step 9400/136300 (epoch 7/100), loss = 0.497218 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:18:24.461436: step 9420/136300 (epoch 7/100), loss = 0.177629 (0.239 sec/batch), lr: 0.700000
2019-12-27 11:18:29.292524: step 9440/136300 (epoch 7/100), loss = 0.478432 (0.243 sec/batch), lr: 0.700000
2019-12-27 11:18:34.019239: step 9460/136300 (epoch 7/100), loss = 0.389384 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:18:38.807660: step 9480/136300 (epoch 7/100), loss = 0.238277 (0.212 sec/batch), lr: 0.700000
2019-12-27 11:18:43.651531: step 9500/136300 (epoch 7/100), loss = 0.465427 (0.244 sec/batch), lr: 0.700000
2019-12-27 11:18:50.057323: step 9520/136300 (epoch 7/100), loss = 0.331940 (0.243 sec/batch), lr: 0.700000
2019-12-27 11:18:54.960929: step 9540/136300 (epoch 7/100), loss = 0.422659 (0.203 sec/batch), lr: 0.700000
Evaluating on dev set...
Precision (micro): 72.194%
   Recall (micro): 38.687%
       F1 (micro): 50.377%
epoch 7: train_loss = 0.441974, dev_loss = 0.638379, dev_f1 = 0.5038
model saved to ./saved_models/01/checkpoint_epoch_7.pt
new best model saved.

2019-12-27 11:19:33.139049: step 9560/136300 (epoch 8/100), loss = 0.266225 (0.230 sec/batch), lr: 0.700000
2019-12-27 11:19:37.858029: step 9580/136300 (epoch 8/100), loss = 0.275247 (0.215 sec/batch), lr: 0.700000
2019-12-27 11:19:42.720649: step 9600/136300 (epoch 8/100), loss = 0.716611 (0.216 sec/batch), lr: 0.700000
2019-12-27 11:19:47.609223: step 9620/136300 (epoch 8/100), loss = 0.267908 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:19:52.434395: step 9640/136300 (epoch 8/100), loss = 0.266401 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:19:57.213941: step 9660/136300 (epoch 8/100), loss = 0.514435 (0.225 sec/batch), lr: 0.700000
2019-12-27 11:20:02.019628: step 9680/136300 (epoch 8/100), loss = 0.494391 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:20:06.859064: step 9700/136300 (epoch 8/100), loss = 0.254265 (0.232 sec/batch), lr: 0.700000
2019-12-27 11:20:13.370447: step 9720/136300 (epoch 8/100), loss = 0.257448 (0.244 sec/batch), lr: 0.700000
2019-12-27 11:20:18.219740: step 9740/136300 (epoch 8/100), loss = 0.684405 (0.221 sec/batch), lr: 0.700000
2019-12-27 11:20:23.021670: step 9760/136300 (epoch 8/100), loss = 0.704501 (0.227 sec/batch), lr: 0.700000
2019-12-27 11:20:27.779321: step 9780/136300 (epoch 8/100), loss = 0.484855 (0.244 sec/batch), lr: 0.700000
2019-12-27 11:20:32.589771: step 9800/136300 (epoch 8/100), loss = 0.469469 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:20:37.453337: step 9820/136300 (epoch 8/100), loss = 0.344995 (0.232 sec/batch), lr: 0.700000
2019-12-27 11:20:42.242291: step 9840/136300 (epoch 8/100), loss = 0.546021 (0.214 sec/batch), lr: 0.700000
2019-12-27 11:20:47.018301: step 9860/136300 (epoch 8/100), loss = 0.524532 (0.242 sec/batch), lr: 0.700000
2019-12-27 11:20:51.832419: step 9880/136300 (epoch 8/100), loss = 0.568540 (0.199 sec/batch), lr: 0.700000
2019-12-27 11:20:58.038038: step 9900/136300 (epoch 8/100), loss = 0.451252 (0.243 sec/batch), lr: 0.700000
2019-12-27 11:21:02.786128: step 9920/136300 (epoch 8/100), loss = 0.454084 (0.245 sec/batch), lr: 0.700000
2019-12-27 11:21:07.541525: step 9940/136300 (epoch 8/100), loss = 0.454571 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:21:12.321977: step 9960/136300 (epoch 8/100), loss = 0.436364 (0.242 sec/batch), lr: 0.700000
2019-12-27 11:21:17.180743: step 9980/136300 (epoch 8/100), loss = 0.477960 (0.235 sec/batch), lr: 0.700000
2019-12-27 11:21:22.063436: step 10000/136300 (epoch 8/100), loss = 0.223690 (0.207 sec/batch), lr: 0.700000
2019-12-27 11:21:26.939020: step 10020/136300 (epoch 8/100), loss = 0.548928 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:21:31.764046: step 10040/136300 (epoch 8/100), loss = 0.300935 (0.228 sec/batch), lr: 0.700000
2019-12-27 11:21:36.679165: step 10060/136300 (epoch 8/100), loss = 0.449862 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:21:41.635644: step 10080/136300 (epoch 8/100), loss = 0.494889 (0.213 sec/batch), lr: 0.700000
2019-12-27 11:21:48.123809: step 10100/136300 (epoch 8/100), loss = 0.389820 (0.242 sec/batch), lr: 0.700000
2019-12-27 11:21:52.953003: step 10120/136300 (epoch 8/100), loss = 0.507460 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:21:57.671939: step 10140/136300 (epoch 8/100), loss = 0.555462 (0.224 sec/batch), lr: 0.700000
2019-12-27 11:22:02.374244: step 10160/136300 (epoch 8/100), loss = 0.492653 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:22:07.228160: step 10180/136300 (epoch 8/100), loss = 0.671759 (0.230 sec/batch), lr: 0.700000
2019-12-27 11:22:12.010533: step 10200/136300 (epoch 8/100), loss = 0.386503 (0.225 sec/batch), lr: 0.700000
2019-12-27 11:22:16.898585: step 10220/136300 (epoch 8/100), loss = 0.556791 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:22:21.850160: step 10240/136300 (epoch 8/100), loss = 0.488270 (0.230 sec/batch), lr: 0.700000
2019-12-27 11:22:26.784089: step 10260/136300 (epoch 8/100), loss = 0.360314 (0.239 sec/batch), lr: 0.700000
2019-12-27 11:22:31.741177: step 10280/136300 (epoch 8/100), loss = 0.659382 (0.248 sec/batch), lr: 0.700000
2019-12-27 11:22:36.509032: step 10300/136300 (epoch 8/100), loss = 0.342679 (0.213 sec/batch), lr: 0.700000
2019-12-27 11:22:42.798231: step 10320/136300 (epoch 8/100), loss = 0.494070 (0.173 sec/batch), lr: 0.700000
2019-12-27 11:22:47.828267: step 10340/136300 (epoch 8/100), loss = 0.386210 (0.246 sec/batch), lr: 0.700000
2019-12-27 11:22:52.620754: step 10360/136300 (epoch 8/100), loss = 0.507636 (0.220 sec/batch), lr: 0.700000
2019-12-27 11:22:57.521977: step 10380/136300 (epoch 8/100), loss = 0.331019 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:23:02.434685: step 10400/136300 (epoch 8/100), loss = 0.578854 (0.211 sec/batch), lr: 0.700000
2019-12-27 11:23:07.178637: step 10420/136300 (epoch 8/100), loss = 0.507551 (0.235 sec/batch), lr: 0.700000
2019-12-27 11:23:12.066835: step 10440/136300 (epoch 8/100), loss = 0.423711 (0.211 sec/batch), lr: 0.700000
2019-12-27 11:23:17.008406: step 10460/136300 (epoch 8/100), loss = 0.430641 (0.219 sec/batch), lr: 0.700000
2019-12-27 11:23:21.863305: step 10480/136300 (epoch 8/100), loss = 0.567425 (0.244 sec/batch), lr: 0.700000
2019-12-27 11:23:28.386253: step 10500/136300 (epoch 8/100), loss = 0.662796 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:23:33.283233: step 10520/136300 (epoch 8/100), loss = 0.276120 (0.246 sec/batch), lr: 0.700000
2019-12-27 11:23:38.095018: step 10540/136300 (epoch 8/100), loss = 0.719968 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:23:42.968924: step 10560/136300 (epoch 8/100), loss = 0.271907 (0.225 sec/batch), lr: 0.700000
2019-12-27 11:23:47.770960: step 10580/136300 (epoch 8/100), loss = 0.696096 (0.204 sec/batch), lr: 0.700000
2019-12-27 11:23:52.509781: step 10600/136300 (epoch 8/100), loss = 0.692719 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:23:57.334283: step 10620/136300 (epoch 8/100), loss = 0.536801 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:24:02.134689: step 10640/136300 (epoch 8/100), loss = 0.465493 (0.240 sec/batch), lr: 0.700000
2019-12-27 11:24:07.080221: step 10660/136300 (epoch 8/100), loss = 0.349867 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:24:11.936476: step 10680/136300 (epoch 8/100), loss = 0.225549 (0.239 sec/batch), lr: 0.700000
2019-12-27 11:24:16.711160: step 10700/136300 (epoch 8/100), loss = 0.516293 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:24:22.919263: step 10720/136300 (epoch 8/100), loss = 0.318743 (0.235 sec/batch), lr: 0.700000
2019-12-27 11:24:27.740618: step 10740/136300 (epoch 8/100), loss = 0.561591 (0.230 sec/batch), lr: 0.700000
2019-12-27 11:24:32.665366: step 10760/136300 (epoch 8/100), loss = 0.472767 (0.239 sec/batch), lr: 0.700000
2019-12-27 11:24:37.590764: step 10780/136300 (epoch 8/100), loss = 0.447492 (0.242 sec/batch), lr: 0.700000
2019-12-27 11:24:42.457625: step 10800/136300 (epoch 8/100), loss = 0.535999 (0.221 sec/batch), lr: 0.700000
2019-12-27 11:24:47.180875: step 10820/136300 (epoch 8/100), loss = 0.283687 (0.213 sec/batch), lr: 0.700000
2019-12-27 11:24:52.052312: step 10840/136300 (epoch 8/100), loss = 0.419702 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:24:56.839715: step 10860/136300 (epoch 8/100), loss = 0.498295 (0.240 sec/batch), lr: 0.700000
2019-12-27 11:25:01.756569: step 10880/136300 (epoch 8/100), loss = 0.357203 (0.200 sec/batch), lr: 0.700000
2019-12-27 11:25:06.733905: step 10900/136300 (epoch 8/100), loss = 0.539517 (0.241 sec/batch), lr: 0.700000
Evaluating on dev set...
Precision (micro): 68.804%
   Recall (micro): 39.882%
       F1 (micro): 50.495%
epoch 8: train_loss = 0.437496, dev_loss = 0.653851, dev_f1 = 0.5049
model saved to ./saved_models/01/checkpoint_epoch_8.pt
new best model saved.

2019-12-27 11:25:47.161167: step 10920/136300 (epoch 9/100), loss = 0.342434 (0.224 sec/batch), lr: 0.700000
2019-12-27 11:25:51.935869: step 10940/136300 (epoch 9/100), loss = 0.617450 (0.207 sec/batch), lr: 0.700000
2019-12-27 11:25:56.846287: step 10960/136300 (epoch 9/100), loss = 0.200547 (0.221 sec/batch), lr: 0.700000
2019-12-27 11:26:01.686131: step 10980/136300 (epoch 9/100), loss = 0.463770 (0.245 sec/batch), lr: 0.700000
2019-12-27 11:26:06.578787: step 11000/136300 (epoch 9/100), loss = 0.391484 (0.206 sec/batch), lr: 0.700000
2019-12-27 11:26:11.360925: step 11020/136300 (epoch 9/100), loss = 0.347265 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:26:16.217461: step 11040/136300 (epoch 9/100), loss = 0.428653 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:26:21.077586: step 11060/136300 (epoch 9/100), loss = 0.394979 (0.228 sec/batch), lr: 0.700000
2019-12-27 11:26:25.981555: step 11080/136300 (epoch 9/100), loss = 0.472207 (0.239 sec/batch), lr: 0.700000
2019-12-27 11:26:32.403720: step 11100/136300 (epoch 9/100), loss = 0.286414 (1.763 sec/batch), lr: 0.700000
2019-12-27 11:26:37.214159: step 11120/136300 (epoch 9/100), loss = 0.319654 (0.227 sec/batch), lr: 0.700000
2019-12-27 11:26:41.960438: step 11140/136300 (epoch 9/100), loss = 0.455768 (0.228 sec/batch), lr: 0.700000
2019-12-27 11:26:46.779762: step 11160/136300 (epoch 9/100), loss = 0.505659 (0.202 sec/batch), lr: 0.700000
2019-12-27 11:26:51.674862: step 11180/136300 (epoch 9/100), loss = 0.511301 (0.208 sec/batch), lr: 0.700000
2019-12-27 11:26:56.555124: step 11200/136300 (epoch 9/100), loss = 0.503241 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:27:01.287056: step 11220/136300 (epoch 9/100), loss = 0.431118 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:27:06.153609: step 11240/136300 (epoch 9/100), loss = 0.436031 (0.203 sec/batch), lr: 0.700000
2019-12-27 11:27:11.039220: step 11260/136300 (epoch 9/100), loss = 0.603144 (0.232 sec/batch), lr: 0.700000
2019-12-27 11:27:15.805099: step 11280/136300 (epoch 9/100), loss = 0.182819 (0.236 sec/batch), lr: 0.700000
2019-12-27 11:27:22.234390: step 11300/136300 (epoch 9/100), loss = 0.679375 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:27:26.998636: step 11320/136300 (epoch 9/100), loss = 0.287134 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:27:31.843521: step 11340/136300 (epoch 9/100), loss = 0.415928 (0.247 sec/batch), lr: 0.700000
2019-12-27 11:27:36.733188: step 11360/136300 (epoch 9/100), loss = 0.330733 (0.207 sec/batch), lr: 0.700000
2019-12-27 11:27:41.588332: step 11380/136300 (epoch 9/100), loss = 0.244962 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:27:46.452806: step 11400/136300 (epoch 9/100), loss = 0.350435 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:27:51.373234: step 11420/136300 (epoch 9/100), loss = 0.320751 (0.213 sec/batch), lr: 0.700000
2019-12-27 11:27:56.299343: step 11440/136300 (epoch 9/100), loss = 0.428921 (0.244 sec/batch), lr: 0.700000
2019-12-27 11:28:01.120040: step 11460/136300 (epoch 9/100), loss = 0.778661 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:28:05.975931: step 11480/136300 (epoch 9/100), loss = 0.422036 (0.207 sec/batch), lr: 0.700000
2019-12-27 11:28:12.325055: step 11500/136300 (epoch 9/100), loss = 0.497205 (0.206 sec/batch), lr: 0.700000
2019-12-27 11:28:17.097381: step 11520/136300 (epoch 9/100), loss = 0.465712 (0.230 sec/batch), lr: 0.700000
2019-12-27 11:28:21.913875: step 11540/136300 (epoch 9/100), loss = 0.286612 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:28:26.665935: step 11560/136300 (epoch 9/100), loss = 0.361338 (0.212 sec/batch), lr: 0.700000
2019-12-27 11:28:31.495002: step 11580/136300 (epoch 9/100), loss = 0.420540 (0.211 sec/batch), lr: 0.700000
2019-12-27 11:28:36.482228: step 11600/136300 (epoch 9/100), loss = 0.371274 (0.239 sec/batch), lr: 0.700000
2019-12-27 11:28:41.359657: step 11620/136300 (epoch 9/100), loss = 0.466013 (0.243 sec/batch), lr: 0.700000
2019-12-27 11:28:46.272408: step 11640/136300 (epoch 9/100), loss = 0.381868 (0.207 sec/batch), lr: 0.700000
2019-12-27 11:28:51.074577: step 11660/136300 (epoch 9/100), loss = 0.488410 (0.183 sec/batch), lr: 0.700000
2019-12-27 11:28:55.921632: step 11680/136300 (epoch 9/100), loss = 0.414581 (0.228 sec/batch), lr: 0.700000
2019-12-27 11:29:02.338607: step 11700/136300 (epoch 9/100), loss = 0.363583 (0.236 sec/batch), lr: 0.700000
2019-12-27 11:29:07.256970: step 11720/136300 (epoch 9/100), loss = 0.531916 (0.244 sec/batch), lr: 0.700000
2019-12-27 11:29:12.055695: step 11740/136300 (epoch 9/100), loss = 0.417544 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:29:17.042280: step 11760/136300 (epoch 9/100), loss = 0.503787 (0.246 sec/batch), lr: 0.700000
2019-12-27 11:29:21.780165: step 11780/136300 (epoch 9/100), loss = 0.487871 (0.242 sec/batch), lr: 0.700000
2019-12-27 11:29:26.671662: step 11800/136300 (epoch 9/100), loss = 0.294217 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:29:31.613852: step 11820/136300 (epoch 9/100), loss = 0.354901 (0.204 sec/batch), lr: 0.700000
2019-12-27 11:29:36.436725: step 11840/136300 (epoch 9/100), loss = 0.342550 (0.235 sec/batch), lr: 0.700000
2019-12-27 11:29:41.426007: step 11860/136300 (epoch 9/100), loss = 0.336033 (0.225 sec/batch), lr: 0.700000
2019-12-27 11:29:46.348297: step 11880/136300 (epoch 9/100), loss = 0.509093 (0.240 sec/batch), lr: 0.700000
2019-12-27 11:29:52.712912: step 11900/136300 (epoch 9/100), loss = 0.482867 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:29:57.601217: step 11920/136300 (epoch 9/100), loss = 0.322743 (0.233 sec/batch), lr: 0.700000
2019-12-27 11:30:02.470711: step 11940/136300 (epoch 9/100), loss = 0.430628 (0.202 sec/batch), lr: 0.700000
2019-12-27 11:30:07.181000: step 11960/136300 (epoch 9/100), loss = 0.440388 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:30:11.982574: step 11980/136300 (epoch 9/100), loss = 0.498706 (0.235 sec/batch), lr: 0.700000
2019-12-27 11:30:16.843324: step 12000/136300 (epoch 9/100), loss = 0.281211 (0.233 sec/batch), lr: 0.700000
2019-12-27 11:30:21.695316: step 12020/136300 (epoch 9/100), loss = 0.349126 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:30:26.620781: step 12040/136300 (epoch 9/100), loss = 0.321470 (0.232 sec/batch), lr: 0.700000
2019-12-27 11:30:31.406529: step 12060/136300 (epoch 9/100), loss = 0.479673 (0.233 sec/batch), lr: 0.700000
2019-12-27 11:30:36.101262: step 12080/136300 (epoch 9/100), loss = 0.589672 (0.216 sec/batch), lr: 0.700000
2019-12-27 11:30:42.680793: step 12100/136300 (epoch 9/100), loss = 0.342036 (0.233 sec/batch), lr: 0.700000
2019-12-27 11:30:47.570327: step 12120/136300 (epoch 9/100), loss = 0.531627 (0.233 sec/batch), lr: 0.700000
2019-12-27 11:30:52.505392: step 12140/136300 (epoch 9/100), loss = 0.466301 (0.213 sec/batch), lr: 0.700000
2019-12-27 11:30:57.414418: step 12160/136300 (epoch 9/100), loss = 0.551504 (0.205 sec/batch), lr: 0.700000
2019-12-27 11:31:02.129782: step 12180/136300 (epoch 9/100), loss = 0.308464 (0.246 sec/batch), lr: 0.700000
2019-12-27 11:31:07.029245: step 12200/136300 (epoch 9/100), loss = 0.457885 (0.247 sec/batch), lr: 0.700000
2019-12-27 11:31:11.835927: step 12220/136300 (epoch 9/100), loss = 0.315179 (0.202 sec/batch), lr: 0.700000
2019-12-27 11:31:16.795913: step 12240/136300 (epoch 9/100), loss = 0.530092 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:31:21.694601: step 12260/136300 (epoch 9/100), loss = 0.553282 (0.242 sec/batch), lr: 0.700000
Evaluating on dev set...
Precision (micro): 76.538%
   Recall (micro): 38.227%
       F1 (micro): 50.988%
epoch 9: train_loss = 0.433487, dev_loss = 0.697525, dev_f1 = 0.5099
model saved to ./saved_models/01/checkpoint_epoch_9.pt
new best model saved.

2019-12-27 11:32:02.306451: step 12280/136300 (epoch 10/100), loss = 0.429838 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:32:07.169733: step 12300/136300 (epoch 10/100), loss = 0.475526 (0.250 sec/batch), lr: 0.700000
2019-12-27 11:32:12.013539: step 12320/136300 (epoch 10/100), loss = 0.286163 (0.244 sec/batch), lr: 0.700000
2019-12-27 11:32:16.757691: step 12340/136300 (epoch 10/100), loss = 0.299844 (0.223 sec/batch), lr: 0.700000
2019-12-27 11:32:21.707537: step 12360/136300 (epoch 10/100), loss = 0.417688 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:32:26.478796: step 12380/136300 (epoch 10/100), loss = 0.359313 (0.221 sec/batch), lr: 0.700000
2019-12-27 11:32:31.321978: step 12400/136300 (epoch 10/100), loss = 0.595120 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:32:36.118969: step 12420/136300 (epoch 10/100), loss = 0.571347 (0.184 sec/batch), lr: 0.700000
2019-12-27 11:32:41.017842: step 12440/136300 (epoch 10/100), loss = 0.283666 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:32:45.895648: step 12460/136300 (epoch 10/100), loss = 0.479024 (0.204 sec/batch), lr: 0.700000
2019-12-27 11:32:52.313479: step 12480/136300 (epoch 10/100), loss = 0.508474 (0.244 sec/batch), lr: 0.700000
2019-12-27 11:32:57.021773: step 12500/136300 (epoch 10/100), loss = 0.562936 (0.182 sec/batch), lr: 0.700000
2019-12-27 11:33:01.836175: step 12520/136300 (epoch 10/100), loss = 0.319796 (0.242 sec/batch), lr: 0.700000
2019-12-27 11:33:06.781822: step 12540/136300 (epoch 10/100), loss = 0.505810 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:33:11.637204: step 12560/136300 (epoch 10/100), loss = 0.288177 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:33:16.377193: step 12580/136300 (epoch 10/100), loss = 0.631010 (0.203 sec/batch), lr: 0.700000
2019-12-27 11:33:21.200066: step 12600/136300 (epoch 10/100), loss = 0.503986 (0.229 sec/batch), lr: 0.700000
2019-12-27 11:33:26.042909: step 12620/136300 (epoch 10/100), loss = 0.277267 (0.247 sec/batch), lr: 0.700000
2019-12-27 11:33:30.780053: step 12640/136300 (epoch 10/100), loss = 0.312542 (0.182 sec/batch), lr: 0.700000
2019-12-27 11:33:35.524612: step 12660/136300 (epoch 10/100), loss = 0.540343 (0.213 sec/batch), lr: 0.700000
2019-12-27 11:33:41.909168: step 12680/136300 (epoch 10/100), loss = 0.199266 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:33:46.772199: step 12700/136300 (epoch 10/100), loss = 0.571495 (0.242 sec/batch), lr: 0.700000
2019-12-27 11:33:51.675326: step 12720/136300 (epoch 10/100), loss = 0.438652 (0.216 sec/batch), lr: 0.700000
2019-12-27 11:33:56.523216: step 12740/136300 (epoch 10/100), loss = 0.392687 (0.205 sec/batch), lr: 0.700000
2019-12-27 11:34:01.380252: step 12760/136300 (epoch 10/100), loss = 0.271279 (0.232 sec/batch), lr: 0.700000
2019-12-27 11:34:06.293360: step 12780/136300 (epoch 10/100), loss = 0.546280 (0.250 sec/batch), lr: 0.700000
2019-12-27 11:34:11.181034: step 12800/136300 (epoch 10/100), loss = 0.494544 (0.230 sec/batch), lr: 0.700000
2019-12-27 11:34:16.058618: step 12820/136300 (epoch 10/100), loss = 0.273134 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:34:20.909452: step 12840/136300 (epoch 10/100), loss = 0.399660 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:34:25.721631: step 12860/136300 (epoch 10/100), loss = 0.868120 (0.232 sec/batch), lr: 0.700000
2019-12-27 11:34:32.053545: step 12880/136300 (epoch 10/100), loss = 0.467399 (0.219 sec/batch), lr: 0.700000
2019-12-27 11:34:36.869632: step 12900/136300 (epoch 10/100), loss = 0.280678 (0.220 sec/batch), lr: 0.700000
2019-12-27 11:34:41.648852: step 12920/136300 (epoch 10/100), loss = 0.409403 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:34:46.509033: step 12940/136300 (epoch 10/100), loss = 0.314628 (0.242 sec/batch), lr: 0.700000
2019-12-27 11:34:51.447402: step 12960/136300 (epoch 10/100), loss = 0.456071 (0.226 sec/batch), lr: 0.700000
2019-12-27 11:34:56.355514: step 12980/136300 (epoch 10/100), loss = 0.367197 (0.235 sec/batch), lr: 0.700000
2019-12-27 11:35:01.311025: step 13000/136300 (epoch 10/100), loss = 0.639243 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:35:06.178355: step 13020/136300 (epoch 10/100), loss = 0.709869 (0.230 sec/batch), lr: 0.700000
2019-12-27 11:35:10.950433: step 13040/136300 (epoch 10/100), loss = 0.489908 (0.216 sec/batch), lr: 0.700000
2019-12-27 11:35:15.908178: step 13060/136300 (epoch 10/100), loss = 0.679390 (0.248 sec/batch), lr: 0.700000
2019-12-27 11:35:22.263433: step 13080/136300 (epoch 10/100), loss = 0.319120 (0.219 sec/batch), lr: 0.700000
2019-12-27 11:35:27.067802: step 13100/136300 (epoch 10/100), loss = 0.551983 (0.221 sec/batch), lr: 0.700000
2019-12-27 11:35:32.021920: step 13120/136300 (epoch 10/100), loss = 0.598598 (0.213 sec/batch), lr: 0.700000
2019-12-27 11:35:36.733494: step 13140/136300 (epoch 10/100), loss = 0.386771 (0.232 sec/batch), lr: 0.700000
2019-12-27 11:35:41.656131: step 13160/136300 (epoch 10/100), loss = 0.272124 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:35:46.627883: step 13180/136300 (epoch 10/100), loss = 0.542055 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:35:51.448406: step 13200/136300 (epoch 10/100), loss = 0.437832 (0.226 sec/batch), lr: 0.700000
2019-12-27 11:35:56.421811: step 13220/136300 (epoch 10/100), loss = 0.592253 (0.235 sec/batch), lr: 0.700000
2019-12-27 11:36:01.318412: step 13240/136300 (epoch 10/100), loss = 0.546347 (0.207 sec/batch), lr: 0.700000
2019-12-27 11:36:06.201202: step 13260/136300 (epoch 10/100), loss = 0.404991 (0.227 sec/batch), lr: 0.700000
2019-12-27 11:36:12.573351: step 13280/136300 (epoch 10/100), loss = 0.304603 (0.239 sec/batch), lr: 0.700000
2019-12-27 11:36:17.405425: step 13300/136300 (epoch 10/100), loss = 0.532094 (0.179 sec/batch), lr: 0.700000
2019-12-27 11:36:22.115674: step 13320/136300 (epoch 10/100), loss = 0.252704 (0.239 sec/batch), lr: 0.700000
2019-12-27 11:36:26.875260: step 13340/136300 (epoch 10/100), loss = 0.505761 (0.220 sec/batch), lr: 0.700000
2019-12-27 11:36:31.770472: step 13360/136300 (epoch 10/100), loss = 0.415750 (0.243 sec/batch), lr: 0.700000
2019-12-27 11:36:36.608950: step 13380/136300 (epoch 10/100), loss = 0.574454 (0.216 sec/batch), lr: 0.700000
2019-12-27 11:36:41.530461: step 13400/136300 (epoch 10/100), loss = 0.679817 (0.207 sec/batch), lr: 0.700000
2019-12-27 11:36:46.308439: step 13420/136300 (epoch 10/100), loss = 0.257674 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:36:51.046227: step 13440/136300 (epoch 10/100), loss = 0.420370 (0.245 sec/batch), lr: 0.700000
2019-12-27 11:36:55.896337: step 13460/136300 (epoch 10/100), loss = 0.537258 (0.233 sec/batch), lr: 0.700000
2019-12-27 11:37:02.485706: step 13480/136300 (epoch 10/100), loss = 0.344191 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:37:07.427144: step 13500/136300 (epoch 10/100), loss = 0.564818 (0.246 sec/batch), lr: 0.700000
2019-12-27 11:37:12.344937: step 13520/136300 (epoch 10/100), loss = 0.254820 (0.206 sec/batch), lr: 0.700000
2019-12-27 11:37:17.014084: step 13540/136300 (epoch 10/100), loss = 0.475659 (0.249 sec/batch), lr: 0.700000
2019-12-27 11:37:21.854289: step 13560/136300 (epoch 10/100), loss = 0.448701 (0.210 sec/batch), lr: 0.700000
2019-12-27 11:37:26.700471: step 13580/136300 (epoch 10/100), loss = 0.286295 (0.243 sec/batch), lr: 0.700000
2019-12-27 11:37:31.632196: step 13600/136300 (epoch 10/100), loss = 0.408687 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:37:36.553554: step 13620/136300 (epoch 10/100), loss = 0.540025 (0.245 sec/batch), lr: 0.700000
Evaluating on dev set...
Precision (micro): 66.025%
   Recall (micro): 44.831%
       F1 (micro): 53.402%
epoch 10: train_loss = 0.431895, dev_loss = 0.549495, dev_f1 = 0.5340
model saved to ./saved_models/01/checkpoint_epoch_10.pt
new best model saved.

2019-12-27 11:38:15.689409: step 13640/136300 (epoch 11/100), loss = 0.382604 (0.177 sec/batch), lr: 0.700000
2019-12-27 11:38:22.310931: step 13660/136300 (epoch 11/100), loss = 0.520531 (0.246 sec/batch), lr: 0.700000
2019-12-27 11:38:27.131799: step 13680/136300 (epoch 11/100), loss = 0.363011 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:38:31.874732: step 13700/136300 (epoch 11/100), loss = 0.351956 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:38:36.852592: step 13720/136300 (epoch 11/100), loss = 0.224515 (0.239 sec/batch), lr: 0.700000
2019-12-27 11:38:41.676484: step 13740/136300 (epoch 11/100), loss = 0.205609 (0.197 sec/batch), lr: 0.700000
2019-12-27 11:38:46.475968: step 13760/136300 (epoch 11/100), loss = 0.473513 (0.249 sec/batch), lr: 0.700000
2019-12-27 11:38:51.321459: step 13780/136300 (epoch 11/100), loss = 0.358057 (0.167 sec/batch), lr: 0.700000
2019-12-27 11:38:56.213272: step 13800/136300 (epoch 11/100), loss = 0.472919 (0.243 sec/batch), lr: 0.700000
2019-12-27 11:39:01.126613: step 13820/136300 (epoch 11/100), loss = 0.621704 (0.223 sec/batch), lr: 0.700000
2019-12-27 11:39:05.946176: step 13840/136300 (epoch 11/100), loss = 0.584062 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:39:10.746478: step 13860/136300 (epoch 11/100), loss = 0.371704 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:39:17.336385: step 13880/136300 (epoch 11/100), loss = 0.431329 (0.177 sec/batch), lr: 0.700000
2019-12-27 11:39:22.235667: step 13900/136300 (epoch 11/100), loss = 0.482392 (0.209 sec/batch), lr: 0.700000
2019-12-27 11:39:27.081927: step 13920/136300 (epoch 11/100), loss = 0.422101 (0.245 sec/batch), lr: 0.700000
2019-12-27 11:39:31.883394: step 13940/136300 (epoch 11/100), loss = 0.309383 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:39:36.699693: step 13960/136300 (epoch 11/100), loss = 0.313203 (0.209 sec/batch), lr: 0.700000
2019-12-27 11:39:41.569142: step 13980/136300 (epoch 11/100), loss = 0.395465 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:39:46.462731: step 14000/136300 (epoch 11/100), loss = 0.459703 (0.183 sec/batch), lr: 0.700000
2019-12-27 11:39:51.164789: step 14020/136300 (epoch 11/100), loss = 0.446435 (0.206 sec/batch), lr: 0.700000
2019-12-27 11:39:55.880918: step 14040/136300 (epoch 11/100), loss = 0.345521 (0.215 sec/batch), lr: 0.700000
2019-12-27 11:40:00.653966: step 14060/136300 (epoch 11/100), loss = 0.176613 (0.232 sec/batch), lr: 0.700000
2019-12-27 11:40:06.973998: step 14080/136300 (epoch 11/100), loss = 0.275281 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:40:11.877024: step 14100/136300 (epoch 11/100), loss = 0.460473 (0.245 sec/batch), lr: 0.700000
2019-12-27 11:40:16.655163: step 14120/136300 (epoch 11/100), loss = 0.598076 (0.228 sec/batch), lr: 0.700000
2019-12-27 11:40:21.554215: step 14140/136300 (epoch 11/100), loss = 0.277185 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:40:26.448805: step 14160/136300 (epoch 11/100), loss = 0.346525 (0.244 sec/batch), lr: 0.700000
2019-12-27 11:40:31.342956: step 14180/136300 (epoch 11/100), loss = 0.576213 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:40:36.146384: step 14200/136300 (epoch 11/100), loss = 0.537799 (0.207 sec/batch), lr: 0.700000
2019-12-27 11:40:40.981497: step 14220/136300 (epoch 11/100), loss = 0.345744 (0.244 sec/batch), lr: 0.700000
2019-12-27 11:40:45.635117: step 14240/136300 (epoch 11/100), loss = 0.400010 (0.247 sec/batch), lr: 0.700000
2019-12-27 11:40:51.859831: step 14260/136300 (epoch 11/100), loss = 0.349602 (0.241 sec/batch), lr: 0.700000
2019-12-27 11:40:56.584629: step 14280/136300 (epoch 11/100), loss = 0.274375 (0.223 sec/batch), lr: 0.700000
2019-12-27 11:41:01.407987: step 14300/136300 (epoch 11/100), loss = 0.325052 (0.232 sec/batch), lr: 0.700000
2019-12-27 11:41:06.316697: step 14320/136300 (epoch 11/100), loss = 0.340998 (0.233 sec/batch), lr: 0.700000
2019-12-27 11:41:11.201858: step 14340/136300 (epoch 11/100), loss = 0.791261 (0.218 sec/batch), lr: 0.700000
2019-12-27 11:41:16.092471: step 14360/136300 (epoch 11/100), loss = 0.391288 (0.209 sec/batch), lr: 0.700000
2019-12-27 11:41:20.965907: step 14380/136300 (epoch 11/100), loss = 0.479728 (0.201 sec/batch), lr: 0.700000
2019-12-27 11:41:25.668700: step 14400/136300 (epoch 11/100), loss = 0.198151 (0.223 sec/batch), lr: 0.700000
2019-12-27 11:41:30.556802: step 14420/136300 (epoch 11/100), loss = 0.445145 (0.228 sec/batch), lr: 0.700000
2019-12-27 11:41:35.469279: step 14440/136300 (epoch 11/100), loss = 0.434592 (0.234 sec/batch), lr: 0.700000
2019-12-27 11:41:41.812169: step 14460/136300 (epoch 11/100), loss = 0.275990 (1.783 sec/batch), lr: 0.700000
2019-12-27 11:41:46.719817: step 14480/136300 (epoch 11/100), loss = 0.448646 (0.230 sec/batch), lr: 0.700000
2019-12-27 11:41:51.360273: step 14500/136300 (epoch 11/100), loss = 0.343811 (0.221 sec/batch), lr: 0.700000
2019-12-27 11:41:56.194527: step 14520/136300 (epoch 11/100), loss = 0.351519 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:42:01.077343: step 14540/136300 (epoch 11/100), loss = 0.370506 (0.225 sec/batch), lr: 0.700000
2019-12-27 11:42:05.877498: step 14560/136300 (epoch 11/100), loss = 0.434868 (0.236 sec/batch), lr: 0.700000
2019-12-27 11:42:10.769290: step 14580/136300 (epoch 11/100), loss = 0.305144 (0.239 sec/batch), lr: 0.700000
2019-12-27 11:42:15.681988: step 14600/136300 (epoch 11/100), loss = 0.591021 (0.219 sec/batch), lr: 0.700000
2019-12-27 11:42:20.477149: step 14620/136300 (epoch 11/100), loss = 0.301558 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:42:25.346331: step 14640/136300 (epoch 11/100), loss = 0.304895 (0.240 sec/batch), lr: 0.700000
2019-12-27 11:42:30.218518: step 14660/136300 (epoch 11/100), loss = 0.222293 (0.192 sec/batch), lr: 0.700000
2019-12-27 11:42:36.289958: step 14680/136300 (epoch 11/100), loss = 0.514523 (0.220 sec/batch), lr: 0.700000
2019-12-27 11:42:40.985132: step 14700/136300 (epoch 11/100), loss = 0.400837 (0.209 sec/batch), lr: 0.700000
2019-12-27 11:42:45.823528: step 14720/136300 (epoch 11/100), loss = 0.331248 (0.217 sec/batch), lr: 0.700000
2019-12-27 11:42:50.623743: step 14740/136300 (epoch 11/100), loss = 0.479347 (0.250 sec/batch), lr: 0.700000
2019-12-27 11:42:55.499818: step 14760/136300 (epoch 11/100), loss = 0.557858 (0.235 sec/batch), lr: 0.700000
2019-12-27 11:43:00.241435: step 14780/136300 (epoch 11/100), loss = 0.405364 (0.228 sec/batch), lr: 0.700000
2019-12-27 11:43:04.912884: step 14800/136300 (epoch 11/100), loss = 0.450788 (0.229 sec/batch), lr: 0.700000
2019-12-27 11:43:09.665101: step 14820/136300 (epoch 11/100), loss = 0.525683 (0.230 sec/batch), lr: 0.700000
2019-12-27 11:43:14.574132: step 14840/136300 (epoch 11/100), loss = 0.247939 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:43:19.497214: step 14860/136300 (epoch 11/100), loss = 0.475013 (0.231 sec/batch), lr: 0.700000
2019-12-27 11:43:25.726292: step 14880/136300 (epoch 11/100), loss = 0.408973 (0.238 sec/batch), lr: 0.700000
2019-12-27 11:43:30.349823: step 14900/136300 (epoch 11/100), loss = 0.308084 (0.212 sec/batch), lr: 0.700000
2019-12-27 11:43:35.115384: step 14920/136300 (epoch 11/100), loss = 0.246848 (0.181 sec/batch), lr: 0.700000
2019-12-27 11:43:39.874403: step 14940/136300 (epoch 11/100), loss = 0.448664 (0.216 sec/batch), lr: 0.700000
2019-12-27 11:43:44.797131: step 14960/136300 (epoch 11/100), loss = 0.374450 (0.237 sec/batch), lr: 0.700000
2019-12-27 11:43:49.598430: step 14980/136300 (epoch 11/100), loss = 0.248257 (0.204 sec/batch), lr: 0.700000
Evaluating on dev set...
Precision (micro): 73.349%
   Recall (micro): 40.250%
       F1 (micro): 51.978%
epoch 11: train_loss = 0.430634, dev_loss = 0.606581, dev_f1 = 0.5198
model saved to ./saved_models/01/checkpoint_epoch_11.pt

2019-12-27 11:44:28.290404: step 15000/136300 (epoch 12/100), loss = 0.767433 (0.228 sec/batch), lr: 0.630000
2019-12-27 11:44:33.037894: step 15020/136300 (epoch 12/100), loss = 0.299570 (0.231 sec/batch), lr: 0.630000
2019-12-27 11:44:37.880557: step 15040/136300 (epoch 12/100), loss = 0.483023 (0.244 sec/batch), lr: 0.630000
2019-12-27 11:44:42.600552: step 15060/136300 (epoch 12/100), loss = 0.202177 (0.239 sec/batch), lr: 0.630000
2019-12-27 11:44:48.852942: step 15080/136300 (epoch 12/100), loss = 0.309098 (0.214 sec/batch), lr: 0.630000
2019-12-27 11:44:53.663887: step 15100/136300 (epoch 12/100), loss = 0.343145 (0.229 sec/batch), lr: 0.630000
2019-12-27 11:44:58.380112: step 15120/136300 (epoch 12/100), loss = 0.424628 (0.231 sec/batch), lr: 0.630000
2019-12-27 11:45:03.250490: step 15140/136300 (epoch 12/100), loss = 0.319879 (0.247 sec/batch), lr: 0.630000
2019-12-27 11:45:08.043842: step 15160/136300 (epoch 12/100), loss = 0.468927 (0.224 sec/batch), lr: 0.630000
2019-12-27 11:45:12.930139: step 15180/136300 (epoch 12/100), loss = 0.371139 (0.231 sec/batch), lr: 0.630000
2019-12-27 11:45:17.728523: step 15200/136300 (epoch 12/100), loss = 0.306864 (0.236 sec/batch), lr: 0.630000
2019-12-27 11:45:22.446470: step 15220/136300 (epoch 12/100), loss = 0.506370 (0.215 sec/batch), lr: 0.630000
2019-12-27 11:45:27.293581: step 15240/136300 (epoch 12/100), loss = 0.207089 (0.213 sec/batch), lr: 0.630000
2019-12-27 11:45:32.108058: step 15260/136300 (epoch 12/100), loss = 0.266507 (0.241 sec/batch), lr: 0.630000
2019-12-27 11:45:38.398829: step 15280/136300 (epoch 12/100), loss = 0.680836 (0.240 sec/batch), lr: 0.630000
2019-12-27 11:45:43.201877: step 15300/136300 (epoch 12/100), loss = 0.423266 (0.218 sec/batch), lr: 0.630000
2019-12-27 11:45:47.942661: step 15320/136300 (epoch 12/100), loss = 0.409941 (0.244 sec/batch), lr: 0.630000
2019-12-27 11:45:52.734553: step 15340/136300 (epoch 12/100), loss = 0.322224 (0.242 sec/batch), lr: 0.630000
2019-12-27 11:45:57.579964: step 15360/136300 (epoch 12/100), loss = 0.546128 (0.180 sec/batch), lr: 0.630000
2019-12-27 11:46:02.361536: step 15380/136300 (epoch 12/100), loss = 0.267783 (0.244 sec/batch), lr: 0.630000
2019-12-27 11:46:06.969552: step 15400/136300 (epoch 12/100), loss = 0.275577 (0.234 sec/batch), lr: 0.630000
2019-12-27 11:46:11.761334: step 15420/136300 (epoch 12/100), loss = 0.326419 (0.243 sec/batch), lr: 0.630000
2019-12-27 11:46:16.695770: step 15440/136300 (epoch 12/100), loss = 0.295629 (0.239 sec/batch), lr: 0.630000
2019-12-27 11:46:22.952813: step 15460/136300 (epoch 12/100), loss = 0.488651 (0.232 sec/batch), lr: 0.630000
2019-12-27 11:46:27.704042: step 15480/136300 (epoch 12/100), loss = 0.283448 (0.227 sec/batch), lr: 0.630000
2019-12-27 11:46:32.545165: step 15500/136300 (epoch 12/100), loss = 0.214310 (0.231 sec/batch), lr: 0.630000
2019-12-27 11:46:37.381912: step 15520/136300 (epoch 12/100), loss = 0.254752 (0.233 sec/batch), lr: 0.630000
2019-12-27 11:46:42.324439: step 15540/136300 (epoch 12/100), loss = 0.231042 (0.241 sec/batch), lr: 0.630000
2019-12-27 11:46:47.105541: step 15560/136300 (epoch 12/100), loss = 0.535997 (0.243 sec/batch), lr: 0.630000
2019-12-27 11:46:51.832154: step 15580/136300 (epoch 12/100), loss = 0.246957 (0.236 sec/batch), lr: 0.630000
2019-12-27 11:46:56.563702: step 15600/136300 (epoch 12/100), loss = 0.522669 (0.233 sec/batch), lr: 0.630000
2019-12-27 11:47:01.296659: step 15620/136300 (epoch 12/100), loss = 0.314479 (0.220 sec/batch), lr: 0.630000
2019-12-27 11:47:06.050626: step 15640/136300 (epoch 12/100), loss = 0.300845 (0.209 sec/batch), lr: 0.630000
2019-12-27 11:47:12.350437: step 15660/136300 (epoch 12/100), loss = 0.454075 (0.240 sec/batch), lr: 0.630000
2019-12-27 11:47:17.212950: step 15680/136300 (epoch 12/100), loss = 0.344170 (0.240 sec/batch), lr: 0.630000
2019-12-27 11:47:22.108673: step 15700/136300 (epoch 12/100), loss = 0.386612 (0.236 sec/batch), lr: 0.630000
2019-12-27 11:47:26.992433: step 15720/136300 (epoch 12/100), loss = 0.561767 (0.238 sec/batch), lr: 0.630000
2019-12-27 11:47:31.863992: step 15740/136300 (epoch 12/100), loss = 0.577478 (0.199 sec/batch), lr: 0.630000
2019-12-27 11:47:36.575919: step 15760/136300 (epoch 12/100), loss = 0.348875 (0.202 sec/batch), lr: 0.630000
2019-12-27 11:47:41.407810: step 15780/136300 (epoch 12/100), loss = 0.489796 (0.246 sec/batch), lr: 0.630000
2019-12-27 11:47:46.357720: step 15800/136300 (epoch 12/100), loss = 0.186992 (0.243 sec/batch), lr: 0.630000
2019-12-27 11:47:51.068226: step 15820/136300 (epoch 12/100), loss = 0.310577 (0.239 sec/batch), lr: 0.630000
2019-12-27 11:47:56.025531: step 15840/136300 (epoch 12/100), loss = 0.360257 (0.233 sec/batch), lr: 0.630000
2019-12-27 11:48:00.723100: step 15860/136300 (epoch 12/100), loss = 0.483080 (0.234 sec/batch), lr: 0.630000
2019-12-27 11:48:06.758153: step 15880/136300 (epoch 12/100), loss = 0.355638 (0.241 sec/batch), lr: 0.630000
2019-12-27 11:48:11.647480: step 15900/136300 (epoch 12/100), loss = 0.439247 (0.242 sec/batch), lr: 0.630000
2019-12-27 11:48:16.455325: step 15920/136300 (epoch 12/100), loss = 0.537340 (0.160 sec/batch), lr: 0.630000
2019-12-27 11:48:21.306547: step 15940/136300 (epoch 12/100), loss = 0.762562 (0.237 sec/batch), lr: 0.630000
2019-12-27 11:48:26.254771: step 15960/136300 (epoch 12/100), loss = 0.375078 (0.222 sec/batch), lr: 0.630000
2019-12-27 11:48:31.099127: step 15980/136300 (epoch 12/100), loss = 0.681808 (0.230 sec/batch), lr: 0.630000
2019-12-27 11:48:35.893027: step 16000/136300 (epoch 12/100), loss = 0.491007 (0.222 sec/batch), lr: 0.630000
2019-12-27 11:48:40.789582: step 16020/136300 (epoch 12/100), loss = 0.443267 (0.237 sec/batch), lr: 0.630000
2019-12-27 11:48:45.389317: step 16040/136300 (epoch 12/100), loss = 0.452686 (0.230 sec/batch), lr: 0.630000
2019-12-27 11:48:51.475869: step 16060/136300 (epoch 12/100), loss = 0.289197 (0.236 sec/batch), lr: 0.630000
2019-12-27 11:48:56.273168: step 16080/136300 (epoch 12/100), loss = 0.309885 (0.241 sec/batch), lr: 0.630000
2019-12-27 11:49:01.063756: step 16100/136300 (epoch 12/100), loss = 0.456397 (0.244 sec/batch), lr: 0.630000
2019-12-27 11:49:05.899963: step 16120/136300 (epoch 12/100), loss = 0.470696 (0.199 sec/batch), lr: 0.630000
2019-12-27 11:49:10.646069: step 16140/136300 (epoch 12/100), loss = 0.507459 (0.234 sec/batch), lr: 0.630000
2019-12-27 11:49:15.373540: step 16160/136300 (epoch 12/100), loss = 0.487939 (0.216 sec/batch), lr: 0.630000
2019-12-27 11:49:20.087971: step 16180/136300 (epoch 12/100), loss = 0.448358 (0.162 sec/batch), lr: 0.630000
2019-12-27 11:49:24.987151: step 16200/136300 (epoch 12/100), loss = 0.419747 (0.233 sec/batch), lr: 0.630000
2019-12-27 11:49:29.887356: step 16220/136300 (epoch 12/100), loss = 0.247545 (0.232 sec/batch), lr: 0.630000
2019-12-27 11:49:34.740697: step 16240/136300 (epoch 12/100), loss = 0.660227 (0.229 sec/batch), lr: 0.630000
2019-12-27 11:49:40.722257: step 16260/136300 (epoch 12/100), loss = 0.284741 (0.172 sec/batch), lr: 0.630000
2019-12-27 11:49:45.443625: step 16280/136300 (epoch 12/100), loss = 0.329075 (0.199 sec/batch), lr: 0.630000
2019-12-27 11:49:50.173331: step 16300/136300 (epoch 12/100), loss = 0.389473 (0.198 sec/batch), lr: 0.630000
2019-12-27 11:49:55.065646: step 16320/136300 (epoch 12/100), loss = 0.467282 (0.232 sec/batch), lr: 0.630000
2019-12-27 11:49:59.919880: step 16340/136300 (epoch 12/100), loss = 0.335714 (0.224 sec/batch), lr: 0.630000
Evaluating on dev set...
Precision (micro): 73.971%
   Recall (micro): 42.973%
       F1 (micro): 54.364%
epoch 12: train_loss = 0.414375, dev_loss = 0.533364, dev_f1 = 0.5436
model saved to ./saved_models/01/checkpoint_epoch_12.pt
new best model saved.

2019-12-27 11:50:38.358016: step 16360/136300 (epoch 13/100), loss = 0.432338 (0.204 sec/batch), lr: 0.630000
2019-12-27 11:50:43.146348: step 16380/136300 (epoch 13/100), loss = 0.632210 (0.224 sec/batch), lr: 0.630000
2019-12-27 11:50:47.933755: step 16400/136300 (epoch 13/100), loss = 0.535448 (0.218 sec/batch), lr: 0.630000
2019-12-27 11:50:54.030224: step 16420/136300 (epoch 13/100), loss = 0.267059 (0.193 sec/batch), lr: 0.630000
2019-12-27 11:50:58.917365: step 16440/136300 (epoch 13/100), loss = 0.422451 (0.229 sec/batch), lr: 0.630000
2019-12-27 11:51:03.667144: step 16460/136300 (epoch 13/100), loss = 0.290010 (0.215 sec/batch), lr: 0.630000
2019-12-27 11:51:08.416693: step 16480/136300 (epoch 13/100), loss = 0.368939 (0.205 sec/batch), lr: 0.630000
2019-12-27 11:51:13.264422: step 16500/136300 (epoch 13/100), loss = 1.055012 (0.216 sec/batch), lr: 0.630000
2019-12-27 11:51:18.064845: step 16520/136300 (epoch 13/100), loss = 0.232561 (0.241 sec/batch), lr: 0.630000
2019-12-27 11:51:22.915646: step 16540/136300 (epoch 13/100), loss = 0.568941 (0.231 sec/batch), lr: 0.630000
2019-12-27 11:51:27.683643: step 16560/136300 (epoch 13/100), loss = 0.314822 (0.172 sec/batch), lr: 0.630000
2019-12-27 11:51:32.414501: step 16580/136300 (epoch 13/100), loss = 0.627801 (0.183 sec/batch), lr: 0.630000
2019-12-27 11:51:37.236987: step 16600/136300 (epoch 13/100), loss = 0.479049 (0.233 sec/batch), lr: 0.630000
2019-12-27 11:51:43.376389: step 16620/136300 (epoch 13/100), loss = 0.194317 (0.230 sec/batch), lr: 0.630000
2019-12-27 11:51:48.161743: step 16640/136300 (epoch 13/100), loss = 0.384321 (0.202 sec/batch), lr: 0.630000
2019-12-27 11:51:53.009909: step 16660/136300 (epoch 13/100), loss = 0.616034 (0.220 sec/batch), lr: 0.630000
2019-12-27 11:51:57.698348: step 16680/136300 (epoch 13/100), loss = 0.428874 (0.204 sec/batch), lr: 0.630000
2019-12-27 11:52:02.488511: step 16700/136300 (epoch 13/100), loss = 0.459595 (0.233 sec/batch), lr: 0.630000
2019-12-27 11:52:07.380625: step 16720/136300 (epoch 13/100), loss = 0.322405 (0.250 sec/batch), lr: 0.630000
2019-12-27 11:52:12.088614: step 16740/136300 (epoch 13/100), loss = 0.471615 (0.231 sec/batch), lr: 0.630000
2019-12-27 11:52:16.717217: step 16760/136300 (epoch 13/100), loss = 0.396240 (0.170 sec/batch), lr: 0.630000
2019-12-27 11:52:21.490661: step 16780/136300 (epoch 13/100), loss = 0.208249 (0.200 sec/batch), lr: 0.630000
2019-12-27 11:52:26.437574: step 16800/136300 (epoch 13/100), loss = 0.529116 (0.214 sec/batch), lr: 0.630000
2019-12-27 11:52:32.797390: step 16820/136300 (epoch 13/100), loss = 0.518662 (0.226 sec/batch), lr: 0.630000
2019-12-27 11:52:37.608049: step 16840/136300 (epoch 13/100), loss = 0.550150 (0.219 sec/batch), lr: 0.630000
2019-12-27 11:52:42.446513: step 16860/136300 (epoch 13/100), loss = 0.213055 (0.230 sec/batch), lr: 0.630000
2019-12-27 11:52:47.318176: step 16880/136300 (epoch 13/100), loss = 0.420249 (0.230 sec/batch), lr: 0.630000
2019-12-27 11:52:52.235355: step 16900/136300 (epoch 13/100), loss = 0.516292 (0.176 sec/batch), lr: 0.630000
2019-12-27 11:52:57.015546: step 16920/136300 (epoch 13/100), loss = 0.219271 (0.181 sec/batch), lr: 0.630000
2019-12-27 11:53:01.837060: step 16940/136300 (epoch 13/100), loss = 0.363105 (0.233 sec/batch), lr: 0.630000
2019-12-27 11:53:06.592938: step 16960/136300 (epoch 13/100), loss = 0.406764 (0.223 sec/batch), lr: 0.630000
2019-12-27 11:53:11.363418: step 16980/136300 (epoch 13/100), loss = 0.348845 (0.230 sec/batch), lr: 0.630000
2019-12-27 11:53:16.138849: step 17000/136300 (epoch 13/100), loss = 0.461013 (0.204 sec/batch), lr: 0.630000
2019-12-27 11:53:22.401174: step 17020/136300 (epoch 13/100), loss = 0.508430 (0.203 sec/batch), lr: 0.630000
2019-12-27 11:53:27.296755: step 17040/136300 (epoch 13/100), loss = 0.391544 (0.237 sec/batch), lr: 0.630000
2019-12-27 11:53:32.210486: step 17060/136300 (epoch 13/100), loss = 0.172407 (0.221 sec/batch), lr: 0.630000
2019-12-27 11:53:37.127180: step 17080/136300 (epoch 13/100), loss = 0.412733 (0.202 sec/batch), lr: 0.630000
2019-12-27 11:53:42.030427: step 17100/136300 (epoch 13/100), loss = 0.243406 (0.235 sec/batch), lr: 0.630000
2019-12-27 11:53:46.806400: step 17120/136300 (epoch 13/100), loss = 0.183864 (0.242 sec/batch), lr: 0.630000
2019-12-27 11:53:51.644842: step 17140/136300 (epoch 13/100), loss = 0.404720 (0.246 sec/batch), lr: 0.630000
2019-12-27 11:53:56.593348: step 17160/136300 (epoch 13/100), loss = 0.376053 (0.231 sec/batch), lr: 0.630000
2019-12-27 11:54:01.388823: step 17180/136300 (epoch 13/100), loss = 0.416315 (0.236 sec/batch), lr: 0.630000
2019-12-27 11:54:06.308263: step 17200/136300 (epoch 13/100), loss = 0.389828 (0.232 sec/batch), lr: 0.630000
2019-12-27 11:54:12.747338: step 17220/136300 (epoch 13/100), loss = 0.372232 (0.210 sec/batch), lr: 0.630000
2019-12-27 11:54:17.486498: step 17240/136300 (epoch 13/100), loss = 0.518582 (0.235 sec/batch), lr: 0.630000
2019-12-27 11:54:22.368579: step 17260/136300 (epoch 13/100), loss = 0.289483 (0.233 sec/batch), lr: 0.630000
2019-12-27 11:54:27.325022: step 17280/136300 (epoch 13/100), loss = 0.389045 (0.239 sec/batch), lr: 0.630000
2019-12-27 11:54:32.103646: step 17300/136300 (epoch 13/100), loss = 0.470892 (0.219 sec/batch), lr: 0.630000
2019-12-27 11:54:37.081350: step 17320/136300 (epoch 13/100), loss = 0.328052 (0.231 sec/batch), lr: 0.630000
2019-12-27 11:54:42.002334: step 17340/136300 (epoch 13/100), loss = 0.290314 (0.213 sec/batch), lr: 0.630000
2019-12-27 11:54:46.762754: step 17360/136300 (epoch 13/100), loss = 0.521679 (0.213 sec/batch), lr: 0.630000
2019-12-27 11:54:51.653042: step 17380/136300 (epoch 13/100), loss = 0.389037 (0.242 sec/batch), lr: 0.630000
2019-12-27 11:54:58.093616: step 17400/136300 (epoch 13/100), loss = 0.311317 (0.230 sec/batch), lr: 0.630000
2019-12-27 11:55:02.889214: step 17420/136300 (epoch 13/100), loss = 0.343663 (0.231 sec/batch), lr: 0.630000
2019-12-27 11:55:07.761379: step 17440/136300 (epoch 13/100), loss = 0.400108 (0.230 sec/batch), lr: 0.630000
2019-12-27 11:55:12.545949: step 17460/136300 (epoch 13/100), loss = 0.204272 (0.250 sec/batch), lr: 0.630000
2019-12-27 11:55:17.493400: step 17480/136300 (epoch 13/100), loss = 0.704168 (0.246 sec/batch), lr: 0.630000
2019-12-27 11:55:22.301299: step 17500/136300 (epoch 13/100), loss = 0.390025 (0.202 sec/batch), lr: 0.630000
2019-12-27 11:55:27.082770: step 17520/136300 (epoch 13/100), loss = 0.460893 (0.168 sec/batch), lr: 0.630000
2019-12-27 11:55:31.868606: step 17540/136300 (epoch 13/100), loss = 0.409966 (0.235 sec/batch), lr: 0.630000
2019-12-27 11:55:36.778691: step 17560/136300 (epoch 13/100), loss = 0.546091 (0.232 sec/batch), lr: 0.630000
2019-12-27 11:55:41.702832: step 17580/136300 (epoch 13/100), loss = 0.471294 (0.234 sec/batch), lr: 0.630000
2019-12-27 11:55:48.378806: step 17600/136300 (epoch 13/100), loss = 0.674700 (0.245 sec/batch), lr: 0.630000
2019-12-27 11:55:53.127309: step 17620/136300 (epoch 13/100), loss = 0.237449 (0.216 sec/batch), lr: 0.630000
2019-12-27 11:55:57.825900: step 17640/136300 (epoch 13/100), loss = 0.346555 (0.238 sec/batch), lr: 0.630000
2019-12-27 11:56:02.570608: step 17660/136300 (epoch 13/100), loss = 0.238588 (0.227 sec/batch), lr: 0.630000
2019-12-27 11:56:07.468168: step 17680/136300 (epoch 13/100), loss = 0.565902 (0.246 sec/batch), lr: 0.630000
2019-12-27 11:56:12.394518: step 17700/136300 (epoch 13/100), loss = 0.467112 (0.229 sec/batch), lr: 0.630000
Evaluating on dev set...
Precision (micro): 74.121%
   Recall (micro): 43.046%
       F1 (micro): 54.463%
epoch 13: train_loss = 0.408044, dev_loss = 0.526186, dev_f1 = 0.5446
model saved to ./saved_models/01/checkpoint_epoch_13.pt
new best model saved.

2019-12-27 11:56:51.279806: step 17720/136300 (epoch 14/100), loss = 0.450595 (0.176 sec/batch), lr: 0.630000
2019-12-27 11:56:56.103252: step 17740/136300 (epoch 14/100), loss = 0.423921 (0.197 sec/batch), lr: 0.630000
2019-12-27 11:57:00.906488: step 17760/136300 (epoch 14/100), loss = 0.241584 (0.242 sec/batch), lr: 0.630000
2019-12-27 11:57:05.765100: step 17780/136300 (epoch 14/100), loss = 0.399516 (0.238 sec/batch), lr: 0.630000
2019-12-27 11:57:12.445689: step 17800/136300 (epoch 14/100), loss = 0.136656 (0.238 sec/batch), lr: 0.630000
2019-12-27 11:57:17.246160: step 17820/136300 (epoch 14/100), loss = 0.380701 (0.202 sec/batch), lr: 0.630000
2019-12-27 11:57:22.016104: step 17840/136300 (epoch 14/100), loss = 0.431849 (0.179 sec/batch), lr: 0.630000
2019-12-27 11:57:26.916504: step 17860/136300 (epoch 14/100), loss = 0.372754 (0.240 sec/batch), lr: 0.630000
2019-12-27 11:57:31.766347: step 17880/136300 (epoch 14/100), loss = 0.209285 (0.246 sec/batch), lr: 0.630000
2019-12-27 11:57:36.690291: step 17900/136300 (epoch 14/100), loss = 0.436237 (0.241 sec/batch), lr: 0.630000
2019-12-27 11:57:41.497981: step 17920/136300 (epoch 14/100), loss = 0.294528 (0.204 sec/batch), lr: 0.630000
2019-12-27 11:57:46.284432: step 17940/136300 (epoch 14/100), loss = 0.486606 (0.203 sec/batch), lr: 0.630000
2019-12-27 11:57:51.075887: step 17960/136300 (epoch 14/100), loss = 0.285409 (0.223 sec/batch), lr: 0.630000
2019-12-27 11:57:55.947883: step 17980/136300 (epoch 14/100), loss = 0.574896 (0.246 sec/batch), lr: 0.630000
2019-12-27 11:58:02.293753: step 18000/136300 (epoch 14/100), loss = 0.306403 (1.731 sec/batch), lr: 0.630000
2019-12-27 11:58:07.145004: step 18020/136300 (epoch 14/100), loss = 0.379984 (0.217 sec/batch), lr: 0.630000
2019-12-27 11:58:11.941242: step 18040/136300 (epoch 14/100), loss = 0.339731 (0.240 sec/batch), lr: 0.630000
2019-12-27 11:58:16.729456: step 18060/136300 (epoch 14/100), loss = 0.450644 (0.208 sec/batch), lr: 0.630000
2019-12-27 11:58:21.646470: step 18080/136300 (epoch 14/100), loss = 0.776030 (0.222 sec/batch), lr: 0.630000
2019-12-27 11:58:26.430693: step 18100/136300 (epoch 14/100), loss = 0.309388 (0.225 sec/batch), lr: 0.630000
2019-12-27 11:58:31.173741: step 18120/136300 (epoch 14/100), loss = 0.471611 (0.180 sec/batch), lr: 0.630000
2019-12-27 11:58:35.982501: step 18140/136300 (epoch 14/100), loss = 0.584813 (0.184 sec/batch), lr: 0.630000
2019-12-27 11:58:40.885831: step 18160/136300 (epoch 14/100), loss = 0.343506 (0.210 sec/batch), lr: 0.630000
2019-12-27 11:58:45.832729: step 18180/136300 (epoch 14/100), loss = 0.491256 (0.239 sec/batch), lr: 0.630000
2019-12-27 11:58:50.680935: step 18200/136300 (epoch 14/100), loss = 0.256699 (0.208 sec/batch), lr: 0.630000
2019-12-27 11:58:57.046685: step 18220/136300 (epoch 14/100), loss = 0.301595 (0.236 sec/batch), lr: 0.630000
2019-12-27 11:59:01.895729: step 18240/136300 (epoch 14/100), loss = 0.336886 (0.175 sec/batch), lr: 0.630000
2019-12-27 11:59:06.893180: step 18260/136300 (epoch 14/100), loss = 0.442636 (0.241 sec/batch), lr: 0.630000
2019-12-27 11:59:11.661683: step 18280/136300 (epoch 14/100), loss = 0.326994 (0.204 sec/batch), lr: 0.630000
2019-12-27 11:59:16.527610: step 18300/136300 (epoch 14/100), loss = 0.563061 (0.221 sec/batch), lr: 0.630000
2019-12-27 11:59:21.254663: step 18320/136300 (epoch 14/100), loss = 0.240189 (0.224 sec/batch), lr: 0.630000
2019-12-27 11:59:26.026321: step 18340/136300 (epoch 14/100), loss = 0.625226 (0.230 sec/batch), lr: 0.630000
2019-12-27 11:59:30.861840: step 18360/136300 (epoch 14/100), loss = 0.485371 (0.236 sec/batch), lr: 0.630000
2019-12-27 11:59:35.662898: step 18380/136300 (epoch 14/100), loss = 0.373955 (0.241 sec/batch), lr: 0.630000
2019-12-27 11:59:40.565509: step 18400/136300 (epoch 14/100), loss = 0.435310 (0.243 sec/batch), lr: 0.630000
2019-12-27 11:59:45.503313: step 18420/136300 (epoch 14/100), loss = 0.347325 (0.230 sec/batch), lr: 0.630000
2019-12-27 11:59:52.075653: step 18440/136300 (epoch 14/100), loss = 0.170285 (0.229 sec/batch), lr: 0.630000
2019-12-27 11:59:56.967676: step 18460/136300 (epoch 14/100), loss = 0.338068 (0.242 sec/batch), lr: 0.630000
2019-12-27 12:00:01.766629: step 18480/136300 (epoch 14/100), loss = 0.240709 (0.231 sec/batch), lr: 0.630000
2019-12-27 12:00:06.576969: step 18500/136300 (epoch 14/100), loss = 0.530047 (0.232 sec/batch), lr: 0.630000
2019-12-27 12:00:11.574484: step 18520/136300 (epoch 14/100), loss = 0.444614 (0.239 sec/batch), lr: 0.630000
2019-12-27 12:00:16.389093: step 18540/136300 (epoch 14/100), loss = 0.445152 (0.227 sec/batch), lr: 0.630000
2019-12-27 12:00:21.285688: step 18560/136300 (epoch 14/100), loss = 0.582851 (0.240 sec/batch), lr: 0.630000
2019-12-27 12:00:26.187167: step 18580/136300 (epoch 14/100), loss = 0.553715 (0.229 sec/batch), lr: 0.630000
2019-12-27 12:00:30.929077: step 18600/136300 (epoch 14/100), loss = 0.450848 (0.207 sec/batch), lr: 0.630000
2019-12-27 12:00:35.798225: step 18620/136300 (epoch 14/100), loss = 0.514191 (0.230 sec/batch), lr: 0.630000
2019-12-27 12:00:42.315717: step 18640/136300 (epoch 14/100), loss = 0.480519 (0.244 sec/batch), lr: 0.630000
2019-12-27 12:00:47.098547: step 18660/136300 (epoch 14/100), loss = 0.197343 (0.185 sec/batch), lr: 0.630000
2019-12-27 12:00:52.101957: step 18680/136300 (epoch 14/100), loss = 0.329989 (0.246 sec/batch), lr: 0.630000
2019-12-27 12:00:56.983082: step 18700/136300 (epoch 14/100), loss = 0.304782 (0.231 sec/batch), lr: 0.630000
2019-12-27 12:01:01.817257: step 18720/136300 (epoch 14/100), loss = 0.341998 (0.236 sec/batch), lr: 0.630000
2019-12-27 12:01:06.677713: step 18740/136300 (epoch 14/100), loss = 0.423640 (0.233 sec/batch), lr: 0.630000
2019-12-27 12:01:11.395753: step 18760/136300 (epoch 14/100), loss = 0.281504 (0.210 sec/batch), lr: 0.630000
2019-12-27 12:01:16.102737: step 18780/136300 (epoch 14/100), loss = 0.529754 (0.230 sec/batch), lr: 0.630000
2019-12-27 12:01:20.939850: step 18800/136300 (epoch 14/100), loss = 0.329098 (0.235 sec/batch), lr: 0.630000
2019-12-27 12:01:25.716312: step 18820/136300 (epoch 14/100), loss = 0.452834 (0.229 sec/batch), lr: 0.630000
2019-12-27 12:01:30.645957: step 18840/136300 (epoch 14/100), loss = 0.601824 (0.224 sec/batch), lr: 0.630000
2019-12-27 12:01:37.234599: step 18860/136300 (epoch 14/100), loss = 0.194933 (0.241 sec/batch), lr: 0.630000
2019-12-27 12:01:41.963791: step 18880/136300 (epoch 14/100), loss = 0.301478 (0.234 sec/batch), lr: 0.630000
2019-12-27 12:01:46.743057: step 18900/136300 (epoch 14/100), loss = 0.488354 (0.246 sec/batch), lr: 0.630000
2019-12-27 12:01:51.567359: step 18920/136300 (epoch 14/100), loss = 0.291659 (0.238 sec/batch), lr: 0.630000
2019-12-27 12:01:56.502516: step 18940/136300 (epoch 14/100), loss = 0.465170 (0.242 sec/batch), lr: 0.630000
2019-12-27 12:02:01.429068: step 18960/136300 (epoch 14/100), loss = 0.474340 (0.237 sec/batch), lr: 0.630000
2019-12-27 12:02:06.262791: step 18980/136300 (epoch 14/100), loss = 0.866516 (0.185 sec/batch), lr: 0.630000
2019-12-27 12:02:11.011414: step 19000/136300 (epoch 14/100), loss = 0.438582 (0.219 sec/batch), lr: 0.630000
2019-12-27 12:02:15.832329: step 19020/136300 (epoch 14/100), loss = 0.346573 (0.218 sec/batch), lr: 0.630000
2019-12-27 12:02:20.683363: step 19040/136300 (epoch 14/100), loss = 0.441526 (0.239 sec/batch), lr: 0.630000
2019-12-27 12:02:27.141796: step 19060/136300 (epoch 14/100), loss = 0.244532 (0.244 sec/batch), lr: 0.630000
2019-12-27 12:02:32.113699: step 19080/136300 (epoch 14/100), loss = 0.474965 (0.222 sec/batch), lr: 0.630000
Evaluating on dev set...
Precision (micro): 73.717%
   Recall (micro): 44.113%
       F1 (micro): 55.196%
epoch 14: train_loss = 0.410086, dev_loss = 0.505680, dev_f1 = 0.5520
model saved to ./saved_models/01/checkpoint_epoch_14.pt
new best model saved.

2019-12-27 12:03:11.393425: step 19100/136300 (epoch 15/100), loss = 0.351171 (0.247 sec/batch), lr: 0.630000
2019-12-27 12:03:16.157717: step 19120/136300 (epoch 15/100), loss = 0.427045 (0.221 sec/batch), lr: 0.630000
2019-12-27 12:03:21.013447: step 19140/136300 (epoch 15/100), loss = 0.240041 (0.235 sec/batch), lr: 0.630000
2019-12-27 12:03:25.885703: step 19160/136300 (epoch 15/100), loss = 0.228708 (0.245 sec/batch), lr: 0.630000
2019-12-27 12:03:30.721976: step 19180/136300 (epoch 15/100), loss = 0.309343 (0.220 sec/batch), lr: 0.630000
2019-12-27 12:03:35.538389: step 19200/136300 (epoch 15/100), loss = 0.612736 (0.237 sec/batch), lr: 0.630000
2019-12-27 12:03:40.359556: step 19220/136300 (epoch 15/100), loss = 0.459182 (0.232 sec/batch), lr: 0.630000
2019-12-27 12:03:46.674690: step 19240/136300 (epoch 15/100), loss = 0.357026 (0.216 sec/batch), lr: 0.630000
2019-12-27 12:03:51.576301: step 19260/136300 (epoch 15/100), loss = 0.382338 (0.240 sec/batch), lr: 0.630000
2019-12-27 12:03:56.458879: step 19280/136300 (epoch 15/100), loss = 0.643109 (0.241 sec/batch), lr: 0.630000
2019-12-27 12:04:01.259077: step 19300/136300 (epoch 15/100), loss = 0.316401 (0.223 sec/batch), lr: 0.630000
2019-12-27 12:04:06.003256: step 19320/136300 (epoch 15/100), loss = 0.253188 (0.225 sec/batch), lr: 0.630000
2019-12-27 12:04:10.819164: step 19340/136300 (epoch 15/100), loss = 0.539807 (0.243 sec/batch), lr: 0.630000
2019-12-27 12:04:15.701935: step 19360/136300 (epoch 15/100), loss = 0.211450 (0.241 sec/batch), lr: 0.630000
2019-12-27 12:04:20.518761: step 19380/136300 (epoch 15/100), loss = 0.350463 (0.241 sec/batch), lr: 0.630000
2019-12-27 12:04:25.259284: step 19400/136300 (epoch 15/100), loss = 0.468654 (0.234 sec/batch), lr: 0.630000
2019-12-27 12:04:30.086903: step 19420/136300 (epoch 15/100), loss = 0.570304 (0.237 sec/batch), lr: 0.630000
2019-12-27 12:04:36.174142: step 19440/136300 (epoch 15/100), loss = 0.326002 (0.213 sec/batch), lr: 0.630000
2019-12-27 12:04:40.884880: step 19460/136300 (epoch 15/100), loss = 0.382748 (0.220 sec/batch), lr: 0.630000
2019-12-27 12:04:45.622097: step 19480/136300 (epoch 15/100), loss = 0.465572 (0.204 sec/batch), lr: 0.630000
2019-12-27 12:04:50.367043: step 19500/136300 (epoch 15/100), loss = 0.200041 (0.210 sec/batch), lr: 0.630000
2019-12-27 12:04:55.202062: step 19520/136300 (epoch 15/100), loss = 0.437474 (0.233 sec/batch), lr: 0.630000
2019-12-27 12:05:00.064379: step 19540/136300 (epoch 15/100), loss = 0.245862 (0.241 sec/batch), lr: 0.630000
2019-12-27 12:05:04.859849: step 19560/136300 (epoch 15/100), loss = 0.339150 (0.232 sec/batch), lr: 0.630000
2019-12-27 12:05:09.642629: step 19580/136300 (epoch 15/100), loss = 0.366908 (0.230 sec/batch), lr: 0.630000
2019-12-27 12:05:14.507894: step 19600/136300 (epoch 15/100), loss = 0.372613 (0.215 sec/batch), lr: 0.630000
2019-12-27 12:05:19.425818: step 19620/136300 (epoch 15/100), loss = 0.295796 (0.241 sec/batch), lr: 0.630000
2019-12-27 12:05:25.460241: step 19640/136300 (epoch 15/100), loss = 0.356548 (0.236 sec/batch), lr: 0.630000
2019-12-27 12:05:30.253530: step 19660/136300 (epoch 15/100), loss = 0.390780 (0.241 sec/batch), lr: 0.630000
2019-12-27 12:05:34.945212: step 19680/136300 (epoch 15/100), loss = 0.385246 (0.238 sec/batch), lr: 0.630000
2019-12-27 12:05:39.627627: step 19700/136300 (epoch 15/100), loss = 0.492883 (0.212 sec/batch), lr: 0.630000
2019-12-27 12:05:44.456833: step 19720/136300 (epoch 15/100), loss = 0.402022 (0.232 sec/batch), lr: 0.630000
2019-12-27 12:05:49.176503: step 19740/136300 (epoch 15/100), loss = 0.210743 (0.237 sec/batch), lr: 0.630000
2019-12-27 12:05:53.991614: step 19760/136300 (epoch 15/100), loss = 0.277179 (0.245 sec/batch), lr: 0.630000
2019-12-27 12:05:58.906462: step 19780/136300 (epoch 15/100), loss = 0.435110 (0.200 sec/batch), lr: 0.630000
2019-12-27 12:06:03.780891: step 19800/136300 (epoch 15/100), loss = 0.259760 (0.226 sec/batch), lr: 0.630000
2019-12-27 12:06:08.672909: step 19820/136300 (epoch 15/100), loss = 0.473596 (0.234 sec/batch), lr: 0.630000
2019-12-27 12:06:14.805700: step 19840/136300 (epoch 15/100), loss = 0.458619 (0.220 sec/batch), lr: 0.630000
2019-12-27 12:06:19.600631: step 19860/136300 (epoch 15/100), loss = 0.399645 (0.232 sec/batch), lr: 0.630000
2019-12-27 12:06:24.502234: step 19880/136300 (epoch 15/100), loss = 0.387592 (0.238 sec/batch), lr: 0.630000
2019-12-27 12:06:29.265190: step 19900/136300 (epoch 15/100), loss = 0.330059 (0.174 sec/batch), lr: 0.630000
2019-12-27 12:06:34.094180: step 19920/136300 (epoch 15/100), loss = 0.257894 (0.245 sec/batch), lr: 0.630000
2019-12-27 12:06:38.986087: step 19940/136300 (epoch 15/100), loss = 0.453368 (0.213 sec/batch), lr: 0.630000
2019-12-27 12:06:43.640893: step 19960/136300 (epoch 15/100), loss = 0.347210 (0.232 sec/batch), lr: 0.630000
2019-12-27 12:06:48.496766: step 19980/136300 (epoch 15/100), loss = 0.510790 (0.234 sec/batch), lr: 0.630000
2019-12-27 12:06:53.374519: step 20000/136300 (epoch 15/100), loss = 0.409113 (0.241 sec/batch), lr: 0.630000
2019-12-27 12:06:58.153912: step 20020/136300 (epoch 15/100), loss = 0.309281 (0.237 sec/batch), lr: 0.630000
2019-12-27 12:07:04.347460: step 20040/136300 (epoch 15/100), loss = 0.179738 (0.247 sec/batch), lr: 0.630000
2019-12-27 12:07:09.184158: step 20060/136300 (epoch 15/100), loss = 0.455673 (0.234 sec/batch), lr: 0.630000
2019-12-27 12:07:13.986971: step 20080/136300 (epoch 15/100), loss = 0.334692 (0.216 sec/batch), lr: 0.630000
2019-12-27 12:07:18.863163: step 20100/136300 (epoch 15/100), loss = 0.382656 (0.248 sec/batch), lr: 0.630000
2019-12-27 12:07:23.671984: step 20120/136300 (epoch 15/100), loss = 0.213507 (0.202 sec/batch), lr: 0.630000
2019-12-27 12:07:28.389511: step 20140/136300 (epoch 15/100), loss = 0.578323 (0.230 sec/batch), lr: 0.630000
2019-12-27 12:07:33.173717: step 20160/136300 (epoch 15/100), loss = 0.426581 (0.215 sec/batch), lr: 0.630000
2019-12-27 12:07:37.958327: step 20180/136300 (epoch 15/100), loss = 0.339418 (0.182 sec/batch), lr: 0.630000
2019-12-27 12:07:42.899323: step 20200/136300 (epoch 15/100), loss = 0.308940 (0.242 sec/batch), lr: 0.630000
2019-12-27 12:07:47.718253: step 20220/136300 (epoch 15/100), loss = 0.409677 (0.223 sec/batch), lr: 0.630000
2019-12-27 12:07:53.962677: step 20240/136300 (epoch 15/100), loss = 0.517665 (0.217 sec/batch), lr: 0.630000
2019-12-27 12:07:58.642423: step 20260/136300 (epoch 15/100), loss = 0.320086 (0.238 sec/batch), lr: 0.630000
2019-12-27 12:08:03.438080: step 20280/136300 (epoch 15/100), loss = 0.590288 (0.224 sec/batch), lr: 0.630000
2019-12-27 12:08:08.306712: step 20300/136300 (epoch 15/100), loss = 0.249050 (0.246 sec/batch), lr: 0.630000
2019-12-27 12:08:13.221162: step 20320/136300 (epoch 15/100), loss = 0.250902 (0.237 sec/batch), lr: 0.630000
2019-12-27 12:08:18.098948: step 20340/136300 (epoch 15/100), loss = 0.545036 (0.237 sec/batch), lr: 0.630000
2019-12-27 12:08:22.818967: step 20360/136300 (epoch 15/100), loss = 0.346562 (0.227 sec/batch), lr: 0.630000
2019-12-27 12:08:27.660367: step 20380/136300 (epoch 15/100), loss = 0.496690 (0.212 sec/batch), lr: 0.630000
2019-12-27 12:08:32.441228: step 20400/136300 (epoch 15/100), loss = 0.814947 (0.236 sec/batch), lr: 0.630000
2019-12-27 12:08:37.384450: step 20420/136300 (epoch 15/100), loss = 0.347697 (0.245 sec/batch), lr: 0.630000
2019-12-27 12:08:42.310842: step 20440/136300 (epoch 15/100), loss = 0.642211 (0.240 sec/batch), lr: 0.630000
Evaluating on dev set...
Precision (micro): 70.039%
   Recall (micro): 49.411%
       F1 (micro): 57.944%
epoch 15: train_loss = 0.404125, dev_loss = 0.505489, dev_f1 = 0.5794
model saved to ./saved_models/01/checkpoint_epoch_15.pt
new best model saved.

2019-12-27 12:09:22.294742: step 20460/136300 (epoch 16/100), loss = 0.289274 (0.231 sec/batch), lr: 0.630000
2019-12-27 12:09:27.031287: step 20480/136300 (epoch 16/100), loss = 0.435888 (0.200 sec/batch), lr: 0.630000
2019-12-27 12:09:31.851133: step 20500/136300 (epoch 16/100), loss = 0.225165 (0.235 sec/batch), lr: 0.630000
2019-12-27 12:09:36.608540: step 20520/136300 (epoch 16/100), loss = 0.340899 (0.250 sec/batch), lr: 0.630000
2019-12-27 12:09:41.470083: step 20540/136300 (epoch 16/100), loss = 0.456848 (0.213 sec/batch), lr: 0.630000
2019-12-27 12:09:46.174585: step 20560/136300 (epoch 16/100), loss = 0.217276 (0.204 sec/batch), lr: 0.630000
2019-12-27 12:09:50.941612: step 20580/136300 (epoch 16/100), loss = 0.283281 (0.212 sec/batch), lr: 0.630000
2019-12-27 12:09:55.734147: step 20600/136300 (epoch 16/100), loss = 0.316408 (0.243 sec/batch), lr: 0.630000
2019-12-27 12:10:00.562693: step 20620/136300 (epoch 16/100), loss = 0.465901 (0.226 sec/batch), lr: 0.630000
2019-12-27 12:10:06.562091: step 20640/136300 (epoch 16/100), loss = 0.519257 (0.237 sec/batch), lr: 0.630000
2019-12-27 12:10:11.346847: step 20660/136300 (epoch 16/100), loss = 0.268963 (0.181 sec/batch), lr: 0.630000
2019-12-27 12:10:16.048124: step 20680/136300 (epoch 16/100), loss = 0.491205 (0.225 sec/batch), lr: 0.630000
2019-12-27 12:10:20.845060: step 20700/136300 (epoch 16/100), loss = 0.462647 (0.238 sec/batch), lr: 0.630000
2019-12-27 12:10:25.680019: step 20720/136300 (epoch 16/100), loss = 0.285746 (0.214 sec/batch), lr: 0.630000
2019-12-27 12:10:30.476658: step 20740/136300 (epoch 16/100), loss = 0.445077 (0.200 sec/batch), lr: 0.630000
2019-12-27 12:10:35.172704: step 20760/136300 (epoch 16/100), loss = 0.404764 (0.201 sec/batch), lr: 0.630000
2019-12-27 12:10:39.998768: step 20780/136300 (epoch 16/100), loss = 0.476732 (0.239 sec/batch), lr: 0.630000
2019-12-27 12:10:44.796030: step 20800/136300 (epoch 16/100), loss = 0.269682 (0.244 sec/batch), lr: 0.630000
2019-12-27 12:10:49.506684: step 20820/136300 (epoch 16/100), loss = 0.195267 (0.207 sec/batch), lr: 0.630000
2019-12-27 12:10:55.559192: step 20840/136300 (epoch 16/100), loss = 0.272515 (0.210 sec/batch), lr: 0.630000
2019-12-27 12:11:00.281996: step 20860/136300 (epoch 16/100), loss = 0.387494 (0.221 sec/batch), lr: 0.630000
2019-12-27 12:11:05.066351: step 20880/136300 (epoch 16/100), loss = 0.357946 (0.217 sec/batch), lr: 0.630000
2019-12-27 12:11:09.952938: step 20900/136300 (epoch 16/100), loss = 0.170678 (0.234 sec/batch), lr: 0.630000
2019-12-27 12:11:14.726710: step 20920/136300 (epoch 16/100), loss = 0.561128 (0.223 sec/batch), lr: 0.630000
2019-12-27 12:11:19.540617: step 20940/136300 (epoch 16/100), loss = 0.344667 (0.230 sec/batch), lr: 0.630000
2019-12-27 12:11:24.398427: step 20960/136300 (epoch 16/100), loss = 0.236917 (0.230 sec/batch), lr: 0.630000
2019-12-27 12:11:29.244906: step 20980/136300 (epoch 16/100), loss = 0.443610 (0.239 sec/batch), lr: 0.630000
2019-12-27 12:11:34.023156: step 21000/136300 (epoch 16/100), loss = 0.369659 (0.243 sec/batch), lr: 0.630000
2019-12-27 12:11:38.833881: step 21020/136300 (epoch 16/100), loss = 0.275373 (0.227 sec/batch), lr: 0.630000
2019-12-27 12:11:44.957123: step 21040/136300 (epoch 16/100), loss = 0.443098 (0.173 sec/batch), lr: 0.630000
2019-12-27 12:11:49.656729: step 21060/136300 (epoch 16/100), loss = 0.512976 (0.225 sec/batch), lr: 0.630000
2019-12-27 12:11:54.443578: step 21080/136300 (epoch 16/100), loss = 0.317248 (0.238 sec/batch), lr: 0.630000
2019-12-27 12:11:59.154070: step 21100/136300 (epoch 16/100), loss = 0.446182 (0.230 sec/batch), lr: 0.630000
2019-12-27 12:12:03.941118: step 21120/136300 (epoch 16/100), loss = 0.401016 (0.230 sec/batch), lr: 0.630000
2019-12-27 12:12:08.849939: step 21140/136300 (epoch 16/100), loss = 0.362628 (0.250 sec/batch), lr: 0.630000
2019-12-27 12:12:13.686979: step 21160/136300 (epoch 16/100), loss = 0.306980 (0.239 sec/batch), lr: 0.630000
2019-12-27 12:12:18.596067: step 21180/136300 (epoch 16/100), loss = 0.346462 (0.240 sec/batch), lr: 0.630000
2019-12-27 12:12:23.377937: step 21200/136300 (epoch 16/100), loss = 0.288432 (0.203 sec/batch), lr: 0.630000
2019-12-27 12:12:28.134102: step 21220/136300 (epoch 16/100), loss = 0.680788 (0.246 sec/batch), lr: 0.630000
2019-12-27 12:12:34.443062: step 21240/136300 (epoch 16/100), loss = 0.366965 (0.233 sec/batch), lr: 0.630000
2019-12-27 12:12:39.276528: step 21260/136300 (epoch 16/100), loss = 0.337699 (0.230 sec/batch), lr: 0.630000
2019-12-27 12:12:44.029528: step 21280/136300 (epoch 16/100), loss = 0.306346 (0.236 sec/batch), lr: 0.630000
2019-12-27 12:12:48.922521: step 21300/136300 (epoch 16/100), loss = 0.500496 (0.232 sec/batch), lr: 0.630000
2019-12-27 12:12:53.583457: step 21320/136300 (epoch 16/100), loss = 0.501724 (0.246 sec/batch), lr: 0.630000
2019-12-27 12:12:58.411481: step 21340/136300 (epoch 16/100), loss = 0.444464 (0.240 sec/batch), lr: 0.630000
2019-12-27 12:13:03.320943: step 21360/136300 (epoch 16/100), loss = 0.374582 (0.247 sec/batch), lr: 0.630000
2019-12-27 12:13:08.041343: step 21380/136300 (epoch 16/100), loss = 0.398126 (0.215 sec/batch), lr: 0.630000
2019-12-27 12:13:12.975770: step 21400/136300 (epoch 16/100), loss = 0.382280 (0.230 sec/batch), lr: 0.630000
2019-12-27 12:13:17.813321: step 21420/136300 (epoch 16/100), loss = 0.309350 (0.233 sec/batch), lr: 0.630000
2019-12-27 12:13:24.015852: step 21440/136300 (epoch 16/100), loss = 0.325788 (1.613 sec/batch), lr: 0.630000
2019-12-27 12:13:28.839513: step 21460/136300 (epoch 16/100), loss = 0.368150 (0.235 sec/batch), lr: 0.630000
2019-12-27 12:13:33.677937: step 21480/136300 (epoch 16/100), loss = 0.236843 (0.240 sec/batch), lr: 0.630000
2019-12-27 12:13:38.288650: step 21500/136300 (epoch 16/100), loss = 0.239719 (0.228 sec/batch), lr: 0.630000
2019-12-27 12:13:43.005618: step 21520/136300 (epoch 16/100), loss = 0.450456 (0.220 sec/batch), lr: 0.630000
2019-12-27 12:13:47.808405: step 21540/136300 (epoch 16/100), loss = 0.384989 (0.235 sec/batch), lr: 0.630000
2019-12-27 12:13:52.603953: step 21560/136300 (epoch 16/100), loss = 0.292845 (0.214 sec/batch), lr: 0.630000
2019-12-27 12:13:57.474529: step 21580/136300 (epoch 16/100), loss = 0.222283 (0.222 sec/batch), lr: 0.630000
2019-12-27 12:14:02.202380: step 21600/136300 (epoch 16/100), loss = 0.425147 (0.227 sec/batch), lr: 0.630000
2019-12-27 12:14:06.856659: step 21620/136300 (epoch 16/100), loss = 0.407157 (0.225 sec/batch), lr: 0.630000
2019-12-27 12:14:13.112459: step 21640/136300 (epoch 16/100), loss = 0.472482 (0.246 sec/batch), lr: 0.630000
2019-12-27 12:14:17.950890: step 21660/136300 (epoch 16/100), loss = 0.297876 (0.225 sec/batch), lr: 0.630000
2019-12-27 12:14:22.841322: step 21680/136300 (epoch 16/100), loss = 0.495139 (0.226 sec/batch), lr: 0.630000
2019-12-27 12:14:27.692056: step 21700/136300 (epoch 16/100), loss = 0.503723 (0.213 sec/batch), lr: 0.630000
2019-12-27 12:14:32.288885: step 21720/136300 (epoch 16/100), loss = 0.279563 (0.175 sec/batch), lr: 0.630000
2019-12-27 12:14:37.106184: step 21740/136300 (epoch 16/100), loss = 0.598493 (0.234 sec/batch), lr: 0.630000
2019-12-27 12:14:41.892115: step 21760/136300 (epoch 16/100), loss = 0.385828 (0.244 sec/batch), lr: 0.630000
2019-12-27 12:14:46.766975: step 21780/136300 (epoch 16/100), loss = 0.469813 (0.240 sec/batch), lr: 0.630000
2019-12-27 12:14:51.597158: step 21800/136300 (epoch 16/100), loss = 0.367694 (0.236 sec/batch), lr: 0.630000
Evaluating on dev set...
Precision (micro): 73.681%
   Recall (micro): 45.990%
       F1 (micro): 56.632%
epoch 16: train_loss = 0.402481, dev_loss = 0.530805, dev_f1 = 0.5663
model saved to ./saved_models/01/checkpoint_epoch_16.pt

2019-12-27 12:15:30.121093: step 21820/136300 (epoch 17/100), loss = 0.453148 (0.232 sec/batch), lr: 0.567000
2019-12-27 12:15:36.021359: step 21840/136300 (epoch 17/100), loss = 0.320882 (0.215 sec/batch), lr: 0.567000
2019-12-27 12:15:40.825608: step 21860/136300 (epoch 17/100), loss = 0.507010 (0.217 sec/batch), lr: 0.567000
2019-12-27 12:15:45.533736: step 21880/136300 (epoch 17/100), loss = 0.247834 (0.244 sec/batch), lr: 0.567000
2019-12-27 12:15:50.412031: step 21900/136300 (epoch 17/100), loss = 0.408261 (0.201 sec/batch), lr: 0.567000
2019-12-27 12:15:55.133716: step 21920/136300 (epoch 17/100), loss = 0.277974 (0.219 sec/batch), lr: 0.567000
2019-12-27 12:15:59.899993: step 21940/136300 (epoch 17/100), loss = 0.325887 (0.228 sec/batch), lr: 0.567000
2019-12-27 12:16:04.696026: step 21960/136300 (epoch 17/100), loss = 0.383995 (0.205 sec/batch), lr: 0.567000
2019-12-27 12:16:09.497725: step 21980/136300 (epoch 17/100), loss = 0.539327 (0.204 sec/batch), lr: 0.567000
2019-12-27 12:16:14.377691: step 22000/136300 (epoch 17/100), loss = 0.433303 (0.219 sec/batch), lr: 0.567000
2019-12-27 12:16:20.425480: step 22020/136300 (epoch 17/100), loss = 0.332995 (0.230 sec/batch), lr: 0.567000
2019-12-27 12:16:25.174167: step 22040/136300 (epoch 17/100), loss = 0.652304 (0.240 sec/batch), lr: 0.567000
2019-12-27 12:16:29.877678: step 22060/136300 (epoch 17/100), loss = 0.473164 (0.202 sec/batch), lr: 0.567000
2019-12-27 12:16:34.762797: step 22080/136300 (epoch 17/100), loss = 0.387512 (0.235 sec/batch), lr: 0.567000
2019-12-27 12:16:39.548890: step 22100/136300 (epoch 17/100), loss = 0.285359 (0.216 sec/batch), lr: 0.567000
2019-12-27 12:16:44.278055: step 22120/136300 (epoch 17/100), loss = 0.461130 (0.229 sec/batch), lr: 0.567000
2019-12-27 12:16:49.027790: step 22140/136300 (epoch 17/100), loss = 0.337361 (0.245 sec/batch), lr: 0.567000
2019-12-27 12:16:53.840646: step 22160/136300 (epoch 17/100), loss = 0.427707 (0.233 sec/batch), lr: 0.567000
2019-12-27 12:16:58.630420: step 22180/136300 (epoch 17/100), loss = 0.473809 (0.196 sec/batch), lr: 0.567000
2019-12-27 12:17:03.330723: step 22200/136300 (epoch 17/100), loss = 0.277853 (0.220 sec/batch), lr: 0.567000
2019-12-27 12:17:09.361653: step 22220/136300 (epoch 17/100), loss = 0.355282 (0.223 sec/batch), lr: 0.567000
2019-12-27 12:17:14.159004: step 22240/136300 (epoch 17/100), loss = 0.488005 (0.245 sec/batch), lr: 0.567000
2019-12-27 12:17:19.037192: step 22260/136300 (epoch 17/100), loss = 0.246343 (0.225 sec/batch), lr: 0.567000
2019-12-27 12:17:23.849735: step 22280/136300 (epoch 17/100), loss = 0.565267 (0.226 sec/batch), lr: 0.567000
2019-12-27 12:17:28.629075: step 22300/136300 (epoch 17/100), loss = 0.317954 (0.225 sec/batch), lr: 0.567000
2019-12-27 12:17:33.479658: step 22320/136300 (epoch 17/100), loss = 0.391710 (0.239 sec/batch), lr: 0.567000
2019-12-27 12:17:38.332119: step 22340/136300 (epoch 17/100), loss = 0.501313 (0.236 sec/batch), lr: 0.567000
2019-12-27 12:17:43.169439: step 22360/136300 (epoch 17/100), loss = 0.289185 (0.238 sec/batch), lr: 0.567000
2019-12-27 12:17:47.944997: step 22380/136300 (epoch 17/100), loss = 0.274433 (0.233 sec/batch), lr: 0.567000
2019-12-27 12:17:52.725734: step 22400/136300 (epoch 17/100), loss = 0.539366 (0.178 sec/batch), lr: 0.567000
2019-12-27 12:17:58.725329: step 22420/136300 (epoch 17/100), loss = 0.362597 (0.220 sec/batch), lr: 0.567000
2019-12-27 12:18:03.496247: step 22440/136300 (epoch 17/100), loss = 0.555441 (0.237 sec/batch), lr: 0.567000
2019-12-27 12:18:08.201974: step 22460/136300 (epoch 17/100), loss = 0.354670 (0.220 sec/batch), lr: 0.567000
2019-12-27 12:18:13.006707: step 22480/136300 (epoch 17/100), loss = 0.273361 (0.230 sec/batch), lr: 0.567000
2019-12-27 12:18:17.911843: step 22500/136300 (epoch 17/100), loss = 0.403891 (0.235 sec/batch), lr: 0.567000
2019-12-27 12:18:22.759155: step 22520/136300 (epoch 17/100), loss = 0.451480 (0.218 sec/batch), lr: 0.567000
2019-12-27 12:18:27.655033: step 22540/136300 (epoch 17/100), loss = 0.452084 (0.239 sec/batch), lr: 0.567000
2019-12-27 12:18:32.477741: step 22560/136300 (epoch 17/100), loss = 0.375529 (0.231 sec/batch), lr: 0.567000
2019-12-27 12:18:37.209626: step 22580/136300 (epoch 17/100), loss = 0.510795 (0.226 sec/batch), lr: 0.567000
2019-12-27 12:18:43.495692: step 22600/136300 (epoch 17/100), loss = 0.423028 (0.232 sec/batch), lr: 0.567000
2019-12-27 12:18:48.364752: step 22620/136300 (epoch 17/100), loss = 0.384995 (0.215 sec/batch), lr: 0.567000
2019-12-27 12:18:53.113749: step 22640/136300 (epoch 17/100), loss = 0.404462 (0.243 sec/batch), lr: 0.567000
2019-12-27 12:18:58.009482: step 22660/136300 (epoch 17/100), loss = 0.371665 (0.176 sec/batch), lr: 0.567000
2019-12-27 12:19:02.627440: step 22680/136300 (epoch 17/100), loss = 0.234677 (0.226 sec/batch), lr: 0.567000
2019-12-27 12:19:07.503697: step 22700/136300 (epoch 17/100), loss = 0.349853 (0.234 sec/batch), lr: 0.567000
2019-12-27 12:19:12.398737: step 22720/136300 (epoch 17/100), loss = 0.188483 (0.216 sec/batch), lr: 0.567000
2019-12-27 12:19:17.165476: step 22740/136300 (epoch 17/100), loss = 0.361098 (0.237 sec/batch), lr: 0.567000
2019-12-27 12:19:22.076327: step 22760/136300 (epoch 17/100), loss = 0.341204 (0.240 sec/batch), lr: 0.567000
2019-12-27 12:19:26.951878: step 22780/136300 (epoch 17/100), loss = 0.439330 (0.223 sec/batch), lr: 0.567000
2019-12-27 12:19:31.749313: step 22800/136300 (epoch 17/100), loss = 0.328622 (0.225 sec/batch), lr: 0.567000
2019-12-27 12:19:38.025239: step 22820/136300 (epoch 17/100), loss = 0.266539 (0.214 sec/batch), lr: 0.567000
2019-12-27 12:19:42.871725: step 22840/136300 (epoch 17/100), loss = 0.310611 (0.199 sec/batch), lr: 0.567000
2019-12-27 12:19:47.472215: step 22860/136300 (epoch 17/100), loss = 0.272127 (0.202 sec/batch), lr: 0.567000
2019-12-27 12:19:52.190930: step 22880/136300 (epoch 17/100), loss = 0.494502 (0.229 sec/batch), lr: 0.567000
2019-12-27 12:19:56.998652: step 22900/136300 (epoch 17/100), loss = 0.500445 (0.237 sec/batch), lr: 0.567000
2019-12-27 12:20:01.801739: step 22920/136300 (epoch 17/100), loss = 0.430055 (0.200 sec/batch), lr: 0.567000
2019-12-27 12:20:06.685696: step 22940/136300 (epoch 17/100), loss = 0.514513 (0.235 sec/batch), lr: 0.567000
2019-12-27 12:20:11.400042: step 22960/136300 (epoch 17/100), loss = 0.251041 (0.235 sec/batch), lr: 0.567000
2019-12-27 12:20:16.058883: step 22980/136300 (epoch 17/100), loss = 0.269695 (0.177 sec/batch), lr: 0.567000
2019-12-27 12:20:20.866575: step 23000/136300 (epoch 17/100), loss = 0.289149 (0.237 sec/batch), lr: 0.567000
2019-12-27 12:20:27.105684: step 23020/136300 (epoch 17/100), loss = 0.378149 (0.243 sec/batch), lr: 0.567000
2019-12-27 12:20:31.975469: step 23040/136300 (epoch 17/100), loss = 0.376622 (0.231 sec/batch), lr: 0.567000
2019-12-27 12:20:36.873171: step 23060/136300 (epoch 17/100), loss = 0.149586 (0.224 sec/batch), lr: 0.567000
2019-12-27 12:20:41.445078: step 23080/136300 (epoch 17/100), loss = 0.557241 (0.220 sec/batch), lr: 0.567000
2019-12-27 12:20:46.256808: step 23100/136300 (epoch 17/100), loss = 0.255489 (0.231 sec/batch), lr: 0.567000
2019-12-27 12:20:51.002524: step 23120/136300 (epoch 17/100), loss = 0.444059 (0.233 sec/batch), lr: 0.567000
2019-12-27 12:20:55.885798: step 23140/136300 (epoch 17/100), loss = 0.396278 (0.202 sec/batch), lr: 0.567000
2019-12-27 12:21:00.714826: step 23160/136300 (epoch 17/100), loss = 0.202773 (0.231 sec/batch), lr: 0.567000
Evaluating on dev set...
Precision (micro): 69.282%
   Recall (micro): 53.606%
       F1 (micro): 60.444%
epoch 17: train_loss = 0.390616, dev_loss = 0.506464, dev_f1 = 0.6044
model saved to ./saved_models/01/checkpoint_epoch_17.pt
new best model saved.

2019-12-27 12:21:39.621954: step 23180/136300 (epoch 18/100), loss = 0.251790 (0.218 sec/batch), lr: 0.567000
2019-12-27 12:21:45.676427: step 23200/136300 (epoch 18/100), loss = 0.426439 (0.239 sec/batch), lr: 0.567000
2019-12-27 12:21:50.478454: step 23220/136300 (epoch 18/100), loss = 0.664249 (0.215 sec/batch), lr: 0.567000
2019-12-27 12:21:55.153848: step 23240/136300 (epoch 18/100), loss = 0.324055 (0.232 sec/batch), lr: 0.567000
2019-12-27 12:22:00.066845: step 23260/136300 (epoch 18/100), loss = 0.355173 (0.242 sec/batch), lr: 0.567000
2019-12-27 12:22:04.863535: step 23280/136300 (epoch 18/100), loss = 0.463204 (0.244 sec/batch), lr: 0.567000
2019-12-27 12:22:09.540511: step 23300/136300 (epoch 18/100), loss = 0.533468 (0.219 sec/batch), lr: 0.567000
2019-12-27 12:22:14.404666: step 23320/136300 (epoch 18/100), loss = 0.425371 (0.234 sec/batch), lr: 0.567000
2019-12-27 12:22:19.159358: step 23340/136300 (epoch 18/100), loss = 0.378306 (0.233 sec/batch), lr: 0.567000
2019-12-27 12:22:24.037935: step 23360/136300 (epoch 18/100), loss = 0.207018 (0.244 sec/batch), lr: 0.567000
2019-12-27 12:22:28.781702: step 23380/136300 (epoch 18/100), loss = 0.394427 (0.176 sec/batch), lr: 0.567000
2019-12-27 12:22:34.982283: step 23400/136300 (epoch 18/100), loss = 0.181829 (0.212 sec/batch), lr: 0.567000
2019-12-27 12:22:39.789336: step 23420/136300 (epoch 18/100), loss = 0.243980 (0.201 sec/batch), lr: 0.567000
2019-12-27 12:22:44.617243: step 23440/136300 (epoch 18/100), loss = 0.268632 (0.236 sec/batch), lr: 0.567000
2019-12-27 12:22:49.390989: step 23460/136300 (epoch 18/100), loss = 0.527453 (0.235 sec/batch), lr: 0.567000
2019-12-27 12:22:54.110683: step 23480/136300 (epoch 18/100), loss = 0.445770 (0.169 sec/batch), lr: 0.567000
2019-12-27 12:22:58.899943: step 23500/136300 (epoch 18/100), loss = 0.293316 (0.235 sec/batch), lr: 0.567000
2019-12-27 12:23:03.678894: step 23520/136300 (epoch 18/100), loss = 0.371453 (0.247 sec/batch), lr: 0.567000
2019-12-27 12:23:08.551121: step 23540/136300 (epoch 18/100), loss = 0.486670 (0.241 sec/batch), lr: 0.567000
2019-12-27 12:23:13.204852: step 23560/136300 (epoch 18/100), loss = 0.461420 (0.216 sec/batch), lr: 0.567000
2019-12-27 12:23:19.276846: step 23580/136300 (epoch 18/100), loss = 0.326070 (1.626 sec/batch), lr: 0.567000
2019-12-27 12:23:24.003358: step 23600/136300 (epoch 18/100), loss = 0.300255 (0.231 sec/batch), lr: 0.567000
2019-12-27 12:23:28.921965: step 23620/136300 (epoch 18/100), loss = 0.326930 (0.226 sec/batch), lr: 0.567000
2019-12-27 12:23:33.754917: step 23640/136300 (epoch 18/100), loss = 0.590299 (0.236 sec/batch), lr: 0.567000
2019-12-27 12:23:38.509869: step 23660/136300 (epoch 18/100), loss = 0.516576 (0.199 sec/batch), lr: 0.567000
2019-12-27 12:23:43.354615: step 23680/136300 (epoch 18/100), loss = 0.548835 (0.211 sec/batch), lr: 0.567000
2019-12-27 12:23:48.193122: step 23700/136300 (epoch 18/100), loss = 0.387733 (0.238 sec/batch), lr: 0.567000
2019-12-27 12:23:53.033997: step 23720/136300 (epoch 18/100), loss = 0.561508 (0.181 sec/batch), lr: 0.567000
2019-12-27 12:23:57.860098: step 23740/136300 (epoch 18/100), loss = 0.308903 (0.240 sec/batch), lr: 0.567000
2019-12-27 12:24:02.645494: step 23760/136300 (epoch 18/100), loss = 0.381599 (0.232 sec/batch), lr: 0.567000
2019-12-27 12:24:08.755588: step 23780/136300 (epoch 18/100), loss = 0.391427 (0.174 sec/batch), lr: 0.567000
2019-12-27 12:24:13.546332: step 23800/136300 (epoch 18/100), loss = 0.379822 (0.215 sec/batch), lr: 0.567000
2019-12-27 12:24:18.281865: step 23820/136300 (epoch 18/100), loss = 0.383505 (0.221 sec/batch), lr: 0.567000
2019-12-27 12:24:23.093161: step 23840/136300 (epoch 18/100), loss = 0.276057 (0.235 sec/batch), lr: 0.567000
2019-12-27 12:24:27.971835: step 23860/136300 (epoch 18/100), loss = 0.482393 (0.214 sec/batch), lr: 0.567000
2019-12-27 12:24:32.848152: step 23880/136300 (epoch 18/100), loss = 0.453674 (0.221 sec/batch), lr: 0.567000
2019-12-27 12:24:37.736805: step 23900/136300 (epoch 18/100), loss = 0.346383 (0.220 sec/batch), lr: 0.567000
2019-12-27 12:24:42.605247: step 23920/136300 (epoch 18/100), loss = 0.322430 (0.234 sec/batch), lr: 0.567000
2019-12-27 12:24:47.277435: step 23940/136300 (epoch 18/100), loss = 0.189562 (0.230 sec/batch), lr: 0.567000
2019-12-27 12:24:52.149780: step 23960/136300 (epoch 18/100), loss = 0.322358 (0.204 sec/batch), lr: 0.567000
2019-12-27 12:24:58.420683: step 23980/136300 (epoch 18/100), loss = 0.425879 (0.181 sec/batch), lr: 0.567000
2019-12-27 12:25:03.203965: step 24000/136300 (epoch 18/100), loss = 0.387393 (0.247 sec/batch), lr: 0.567000
2019-12-27 12:25:08.122602: step 24020/136300 (epoch 18/100), loss = 0.428010 (0.236 sec/batch), lr: 0.567000
2019-12-27 12:25:12.763463: step 24040/136300 (epoch 18/100), loss = 0.377849 (0.179 sec/batch), lr: 0.567000
2019-12-27 12:25:17.582184: step 24060/136300 (epoch 18/100), loss = 0.789032 (0.229 sec/batch), lr: 0.567000
2019-12-27 12:25:22.477381: step 24080/136300 (epoch 18/100), loss = 0.360266 (0.243 sec/batch), lr: 0.567000
2019-12-27 12:25:27.249767: step 24100/136300 (epoch 18/100), loss = 0.359398 (0.223 sec/batch), lr: 0.567000
2019-12-27 12:25:32.130194: step 24120/136300 (epoch 18/100), loss = 0.437798 (0.247 sec/batch), lr: 0.567000
2019-12-27 12:25:37.056245: step 24140/136300 (epoch 18/100), loss = 0.309805 (0.231 sec/batch), lr: 0.567000
2019-12-27 12:25:41.810327: step 24160/136300 (epoch 18/100), loss = 0.389051 (0.224 sec/batch), lr: 0.567000
2019-12-27 12:25:48.014558: step 24180/136300 (epoch 18/100), loss = 0.398810 (0.239 sec/batch), lr: 0.567000
2019-12-27 12:25:52.908386: step 24200/136300 (epoch 18/100), loss = 0.723104 (0.235 sec/batch), lr: 0.567000
2019-12-27 12:25:57.487025: step 24220/136300 (epoch 18/100), loss = 0.313720 (0.233 sec/batch), lr: 0.567000
2019-12-27 12:26:02.184950: step 24240/136300 (epoch 18/100), loss = 0.376074 (0.238 sec/batch), lr: 0.567000
2019-12-27 12:26:07.009309: step 24260/136300 (epoch 18/100), loss = 0.330065 (0.213 sec/batch), lr: 0.567000
2019-12-27 12:26:11.770681: step 24280/136300 (epoch 18/100), loss = 0.539791 (0.227 sec/batch), lr: 0.567000
2019-12-27 12:26:16.651354: step 24300/136300 (epoch 18/100), loss = 0.369259 (0.232 sec/batch), lr: 0.567000
2019-12-27 12:26:21.386529: step 24320/136300 (epoch 18/100), loss = 0.542081 (0.237 sec/batch), lr: 0.567000
2019-12-27 12:26:26.047108: step 24340/136300 (epoch 18/100), loss = 0.138728 (0.199 sec/batch), lr: 0.567000
2019-12-27 12:26:30.793379: step 24360/136300 (epoch 18/100), loss = 0.693931 (0.207 sec/batch), lr: 0.567000
2019-12-27 12:26:37.009707: step 24380/136300 (epoch 18/100), loss = 0.392404 (0.239 sec/batch), lr: 0.567000
2019-12-27 12:26:41.916996: step 24400/136300 (epoch 18/100), loss = 0.323619 (0.220 sec/batch), lr: 0.567000
2019-12-27 12:26:46.792836: step 24420/136300 (epoch 18/100), loss = 0.317442 (0.237 sec/batch), lr: 0.567000
2019-12-27 12:26:51.437027: step 24440/136300 (epoch 18/100), loss = 0.163149 (0.204 sec/batch), lr: 0.567000
2019-12-27 12:26:56.229052: step 24460/136300 (epoch 18/100), loss = 0.406248 (0.230 sec/batch), lr: 0.567000
2019-12-27 12:27:00.943241: step 24480/136300 (epoch 18/100), loss = 0.219548 (0.242 sec/batch), lr: 0.567000
2019-12-27 12:27:05.842053: step 24500/136300 (epoch 18/100), loss = 0.236472 (0.208 sec/batch), lr: 0.567000
2019-12-27 12:27:10.675484: step 24520/136300 (epoch 18/100), loss = 0.263952 (0.229 sec/batch), lr: 0.567000
Evaluating on dev set...
Precision (micro): 71.087%
   Recall (micro): 48.620%
       F1 (micro): 57.745%
epoch 18: train_loss = 0.387292, dev_loss = 0.478859, dev_f1 = 0.5775
model saved to ./saved_models/01/checkpoint_epoch_18.pt

2019-12-27 12:27:49.734133: step 24540/136300 (epoch 19/100), loss = 0.372251 (0.243 sec/batch), lr: 0.510300
2019-12-27 12:27:55.628530: step 24560/136300 (epoch 19/100), loss = 0.353935 (0.209 sec/batch), lr: 0.510300
2019-12-27 12:28:00.436590: step 24580/136300 (epoch 19/100), loss = 0.239397 (0.235 sec/batch), lr: 0.510300
2019-12-27 12:28:05.149489: step 24600/136300 (epoch 19/100), loss = 0.261184 (0.226 sec/batch), lr: 0.510300
2019-12-27 12:28:10.042581: step 24620/136300 (epoch 19/100), loss = 0.360255 (0.227 sec/batch), lr: 0.510300
2019-12-27 12:28:14.833458: step 24640/136300 (epoch 19/100), loss = 0.268710 (0.245 sec/batch), lr: 0.510300
2019-12-27 12:28:19.549631: step 24660/136300 (epoch 19/100), loss = 0.361923 (0.206 sec/batch), lr: 0.510300
2019-12-27 12:28:24.393653: step 24680/136300 (epoch 19/100), loss = 0.370624 (0.204 sec/batch), lr: 0.510300
2019-12-27 12:28:29.195915: step 24700/136300 (epoch 19/100), loss = 0.441052 (0.234 sec/batch), lr: 0.510300
2019-12-27 12:28:34.062849: step 24720/136300 (epoch 19/100), loss = 0.440394 (0.223 sec/batch), lr: 0.510300
2019-12-27 12:28:38.848784: step 24740/136300 (epoch 19/100), loss = 0.219497 (0.209 sec/batch), lr: 0.510300
2019-12-27 12:28:44.988515: step 24760/136300 (epoch 19/100), loss = 0.288364 (0.205 sec/batch), lr: 0.510300
2019-12-27 12:28:49.816882: step 24780/136300 (epoch 19/100), loss = 0.179938 (0.238 sec/batch), lr: 0.510300
2019-12-27 12:28:54.572852: step 24800/136300 (epoch 19/100), loss = 0.369379 (0.244 sec/batch), lr: 0.510300
2019-12-27 12:28:59.335158: step 24820/136300 (epoch 19/100), loss = 0.437338 (0.235 sec/batch), lr: 0.510300
2019-12-27 12:29:04.148948: step 24840/136300 (epoch 19/100), loss = 0.440265 (0.233 sec/batch), lr: 0.510300
2019-12-27 12:29:08.847143: step 24860/136300 (epoch 19/100), loss = 0.426092 (0.231 sec/batch), lr: 0.510300
2019-12-27 12:29:13.628488: step 24880/136300 (epoch 19/100), loss = 0.253396 (0.238 sec/batch), lr: 0.510300
2019-12-27 12:29:18.525630: step 24900/136300 (epoch 19/100), loss = 0.399405 (0.245 sec/batch), lr: 0.510300
2019-12-27 12:29:23.220860: step 24920/136300 (epoch 19/100), loss = 0.201945 (0.212 sec/batch), lr: 0.510300
2019-12-27 12:29:27.816243: step 24940/136300 (epoch 19/100), loss = 0.231591 (0.219 sec/batch), lr: 0.510300
2019-12-27 12:29:33.799907: step 24960/136300 (epoch 19/100), loss = 0.642226 (0.230 sec/batch), lr: 0.510300
2019-12-27 12:29:38.727242: step 24980/136300 (epoch 19/100), loss = 0.400198 (0.224 sec/batch), lr: 0.510300
2019-12-27 12:29:43.606218: step 25000/136300 (epoch 19/100), loss = 0.386798 (0.229 sec/batch), lr: 0.510300
2019-12-27 12:29:48.345544: step 25020/136300 (epoch 19/100), loss = 0.337939 (0.177 sec/batch), lr: 0.510300
2019-12-27 12:29:53.181084: step 25040/136300 (epoch 19/100), loss = 0.541726 (0.200 sec/batch), lr: 0.510300
2019-12-27 12:29:58.013914: step 25060/136300 (epoch 19/100), loss = 0.271815 (0.236 sec/batch), lr: 0.510300
2019-12-27 12:30:02.939182: step 25080/136300 (epoch 19/100), loss = 0.363940 (0.227 sec/batch), lr: 0.510300
2019-12-27 12:30:07.712030: step 25100/136300 (epoch 19/100), loss = 0.124130 (0.249 sec/batch), lr: 0.510300
2019-12-27 12:30:12.439922: step 25120/136300 (epoch 19/100), loss = 0.445849 (0.227 sec/batch), lr: 0.510300
2019-12-27 12:30:17.169074: step 25140/136300 (epoch 19/100), loss = 0.474404 (0.199 sec/batch), lr: 0.510300
2019-12-27 12:30:23.382683: step 25160/136300 (epoch 19/100), loss = 0.221333 (0.203 sec/batch), lr: 0.510300
2019-12-27 12:30:28.130366: step 25180/136300 (epoch 19/100), loss = 0.304702 (0.236 sec/batch), lr: 0.510300
2019-12-27 12:30:32.874704: step 25200/136300 (epoch 19/100), loss = 0.531300 (0.229 sec/batch), lr: 0.510300
2019-12-27 12:30:37.732128: step 25220/136300 (epoch 19/100), loss = 0.369698 (0.234 sec/batch), lr: 0.510300
2019-12-27 12:30:42.617120: step 25240/136300 (epoch 19/100), loss = 0.514494 (0.213 sec/batch), lr: 0.510300
2019-12-27 12:30:47.484813: step 25260/136300 (epoch 19/100), loss = 0.239129 (0.226 sec/batch), lr: 0.510300
2019-12-27 12:30:52.379549: step 25280/136300 (epoch 19/100), loss = 0.311420 (0.234 sec/batch), lr: 0.510300
2019-12-27 12:30:57.087013: step 25300/136300 (epoch 19/100), loss = 0.405826 (0.233 sec/batch), lr: 0.510300
2019-12-27 12:31:01.858265: step 25320/136300 (epoch 19/100), loss = 0.357131 (0.249 sec/batch), lr: 0.510300
2019-12-27 12:31:07.971692: step 25340/136300 (epoch 19/100), loss = 0.393436 (0.242 sec/batch), lr: 0.510300
2019-12-27 12:31:12.673479: step 25360/136300 (epoch 19/100), loss = 0.639780 (0.205 sec/batch), lr: 0.510300
2019-12-27 12:31:17.615894: step 25380/136300 (epoch 19/100), loss = 0.303515 (0.241 sec/batch), lr: 0.510300
2019-12-27 12:31:22.301895: step 25400/136300 (epoch 19/100), loss = 0.352292 (0.209 sec/batch), lr: 0.510300
2019-12-27 12:31:27.064784: step 25420/136300 (epoch 19/100), loss = 0.165422 (0.232 sec/batch), lr: 0.510300
2019-12-27 12:31:31.945007: step 25440/136300 (epoch 19/100), loss = 0.430095 (0.243 sec/batch), lr: 0.510300
2019-12-27 12:31:36.825779: step 25460/136300 (epoch 19/100), loss = 0.321024 (0.219 sec/batch), lr: 0.510300
2019-12-27 12:31:41.595835: step 25480/136300 (epoch 19/100), loss = 0.539800 (0.220 sec/batch), lr: 0.510300
2019-12-27 12:31:46.546990: step 25500/136300 (epoch 19/100), loss = 0.397993 (0.231 sec/batch), lr: 0.510300
2019-12-27 12:31:51.373134: step 25520/136300 (epoch 19/100), loss = 0.431440 (0.204 sec/batch), lr: 0.510300
2019-12-27 12:31:57.552065: step 25540/136300 (epoch 19/100), loss = 0.372651 (0.235 sec/batch), lr: 0.510300
2019-12-27 12:32:02.412172: step 25560/136300 (epoch 19/100), loss = 0.309381 (0.236 sec/batch), lr: 0.510300
2019-12-27 12:32:07.005183: step 25580/136300 (epoch 19/100), loss = 0.433446 (0.227 sec/batch), lr: 0.510300
2019-12-27 12:32:11.739760: step 25600/136300 (epoch 19/100), loss = 0.512589 (0.236 sec/batch), lr: 0.510300
2019-12-27 12:32:16.525243: step 25620/136300 (epoch 19/100), loss = 0.244496 (0.206 sec/batch), lr: 0.510300
2019-12-27 12:32:21.298840: step 25640/136300 (epoch 19/100), loss = 0.121563 (0.226 sec/batch), lr: 0.510300
2019-12-27 12:32:26.182678: step 25660/136300 (epoch 19/100), loss = 0.441228 (0.225 sec/batch), lr: 0.510300
2019-12-27 12:32:30.887329: step 25680/136300 (epoch 19/100), loss = 0.390141 (0.203 sec/batch), lr: 0.510300
2019-12-27 12:32:35.627358: step 25700/136300 (epoch 19/100), loss = 0.252943 (0.225 sec/batch), lr: 0.510300
2019-12-27 12:32:40.393998: step 25720/136300 (epoch 19/100), loss = 0.276605 (0.246 sec/batch), lr: 0.510300
2019-12-27 12:32:46.627732: step 25740/136300 (epoch 19/100), loss = 0.464551 (0.243 sec/batch), lr: 0.510300
2019-12-27 12:32:51.524055: step 25760/136300 (epoch 19/100), loss = 0.228659 (0.248 sec/batch), lr: 0.510300
2019-12-27 12:32:56.369896: step 25780/136300 (epoch 19/100), loss = 0.357585 (0.236 sec/batch), lr: 0.510300
2019-12-27 12:33:01.132262: step 25800/136300 (epoch 19/100), loss = 0.399696 (0.215 sec/batch), lr: 0.510300
2019-12-27 12:33:05.842604: step 25820/136300 (epoch 19/100), loss = 0.304068 (0.232 sec/batch), lr: 0.510300
2019-12-27 12:33:10.569400: step 25840/136300 (epoch 19/100), loss = 0.450831 (0.241 sec/batch), lr: 0.510300
2019-12-27 12:33:15.428263: step 25860/136300 (epoch 19/100), loss = 0.538378 (0.245 sec/batch), lr: 0.510300
2019-12-27 12:33:20.293345: step 25880/136300 (epoch 19/100), loss = 0.349361 (0.236 sec/batch), lr: 0.510300
Evaluating on dev set...
Precision (micro): 73.077%
   Recall (micro): 51.380%
       F1 (micro): 60.337%
epoch 19: train_loss = 0.375021, dev_loss = 0.461457, dev_f1 = 0.6034
model saved to ./saved_models/01/checkpoint_epoch_19.pt

2019-12-27 12:33:59.203540: step 25900/136300 (epoch 20/100), loss = 0.426957 (0.214 sec/batch), lr: 0.510300
2019-12-27 12:34:05.156081: step 25920/136300 (epoch 20/100), loss = 0.313539 (0.213 sec/batch), lr: 0.510300
2019-12-27 12:34:09.936668: step 25940/136300 (epoch 20/100), loss = 0.518745 (0.239 sec/batch), lr: 0.510300
2019-12-27 12:34:14.701000: step 25960/136300 (epoch 20/100), loss = 0.308864 (0.202 sec/batch), lr: 0.510300
2019-12-27 12:34:19.561603: step 25980/136300 (epoch 20/100), loss = 0.518507 (0.230 sec/batch), lr: 0.510300
2019-12-27 12:34:24.326204: step 26000/136300 (epoch 20/100), loss = 0.246542 (0.212 sec/batch), lr: 0.510300
2019-12-27 12:34:29.094130: step 26020/136300 (epoch 20/100), loss = 0.278392 (0.230 sec/batch), lr: 0.510300
2019-12-27 12:34:33.931924: step 26040/136300 (epoch 20/100), loss = 0.430763 (0.238 sec/batch), lr: 0.510300
2019-12-27 12:34:38.722602: step 26060/136300 (epoch 20/100), loss = 0.403977 (0.223 sec/batch), lr: 0.510300
2019-12-27 12:34:43.589171: step 26080/136300 (epoch 20/100), loss = 0.311876 (0.220 sec/batch), lr: 0.510300
2019-12-27 12:34:48.434852: step 26100/136300 (epoch 20/100), loss = 0.593316 (0.246 sec/batch), lr: 0.510300
2019-12-27 12:34:54.532069: step 26120/136300 (epoch 20/100), loss = 0.356983 (0.215 sec/batch), lr: 0.510300
2019-12-27 12:34:59.304270: step 26140/136300 (epoch 20/100), loss = 0.454331 (0.237 sec/batch), lr: 0.510300
2019-12-27 12:35:04.044040: step 26160/136300 (epoch 20/100), loss = 0.194596 (0.201 sec/batch), lr: 0.510300
2019-12-27 12:35:08.857345: step 26180/136300 (epoch 20/100), loss = 0.289150 (0.239 sec/batch), lr: 0.510300
2019-12-27 12:35:13.682504: step 26200/136300 (epoch 20/100), loss = 0.757775 (0.231 sec/batch), lr: 0.510300
2019-12-27 12:35:18.373756: step 26220/136300 (epoch 20/100), loss = 0.274645 (0.180 sec/batch), lr: 0.510300
2019-12-27 12:35:23.107361: step 26240/136300 (epoch 20/100), loss = 0.318886 (0.214 sec/batch), lr: 0.510300
2019-12-27 12:35:28.012857: step 26260/136300 (epoch 20/100), loss = 0.334807 (0.221 sec/batch), lr: 0.510300
2019-12-27 12:35:32.768873: step 26280/136300 (epoch 20/100), loss = 0.393894 (0.232 sec/batch), lr: 0.510300
2019-12-27 12:35:37.478350: step 26300/136300 (epoch 20/100), loss = 0.261762 (0.214 sec/batch), lr: 0.510300
2019-12-27 12:35:43.549580: step 26320/136300 (epoch 20/100), loss = 0.497966 (0.211 sec/batch), lr: 0.510300
2019-12-27 12:35:48.462591: step 26340/136300 (epoch 20/100), loss = 0.470980 (0.244 sec/batch), lr: 0.510300
2019-12-27 12:35:53.362372: step 26360/136300 (epoch 20/100), loss = 0.574250 (0.227 sec/batch), lr: 0.510300
2019-12-27 12:35:58.209100: step 26380/136300 (epoch 20/100), loss = 0.204537 (0.238 sec/batch), lr: 0.510300
2019-12-27 12:36:03.053781: step 26400/136300 (epoch 20/100), loss = 0.387926 (0.235 sec/batch), lr: 0.510300
2019-12-27 12:36:07.900089: step 26420/136300 (epoch 20/100), loss = 0.407484 (0.234 sec/batch), lr: 0.510300
2019-12-27 12:36:12.847665: step 26440/136300 (epoch 20/100), loss = 0.439419 (0.224 sec/batch), lr: 0.510300
2019-12-27 12:36:17.597860: step 26460/136300 (epoch 20/100), loss = 0.338654 (0.226 sec/batch), lr: 0.510300
2019-12-27 12:36:22.348243: step 26480/136300 (epoch 20/100), loss = 0.233338 (0.176 sec/batch), lr: 0.510300
2019-12-27 12:36:27.085281: step 26500/136300 (epoch 20/100), loss = 0.361715 (0.233 sec/batch), lr: 0.510300
2019-12-27 12:36:33.122610: step 26520/136300 (epoch 20/100), loss = 0.264663 (0.231 sec/batch), lr: 0.510300
2019-12-27 12:36:37.879884: step 26540/136300 (epoch 20/100), loss = 0.427233 (0.220 sec/batch), lr: 0.510300
2019-12-27 12:36:42.633667: step 26560/136300 (epoch 20/100), loss = 0.528306 (0.220 sec/batch), lr: 0.510300
2019-12-27 12:36:47.466530: step 26580/136300 (epoch 20/100), loss = 0.255315 (0.228 sec/batch), lr: 0.510300
2019-12-27 12:36:52.365749: step 26600/136300 (epoch 20/100), loss = 0.401142 (0.240 sec/batch), lr: 0.510300
2019-12-27 12:36:57.265794: step 26620/136300 (epoch 20/100), loss = 0.266325 (0.239 sec/batch), lr: 0.510300
2019-12-27 12:37:02.108429: step 26640/136300 (epoch 20/100), loss = 0.407622 (0.229 sec/batch), lr: 0.510300
2019-12-27 12:37:06.852662: step 26660/136300 (epoch 20/100), loss = 0.455006 (0.238 sec/batch), lr: 0.510300
2019-12-27 12:37:11.652048: step 26680/136300 (epoch 20/100), loss = 0.332497 (0.241 sec/batch), lr: 0.510300
2019-12-27 12:37:17.794815: step 26700/136300 (epoch 20/100), loss = 0.269110 (0.247 sec/batch), lr: 0.510300
2019-12-27 12:37:22.541812: step 26720/136300 (epoch 20/100), loss = 0.459511 (0.248 sec/batch), lr: 0.510300
2019-12-27 12:37:27.433899: step 26740/136300 (epoch 20/100), loss = 0.734255 (0.238 sec/batch), lr: 0.510300
2019-12-27 12:37:32.179211: step 26760/136300 (epoch 20/100), loss = 0.452219 (0.204 sec/batch), lr: 0.510300
2019-12-27 12:37:36.866088: step 26780/136300 (epoch 20/100), loss = 0.270656 (0.216 sec/batch), lr: 0.510300
2019-12-27 12:37:41.725229: step 26800/136300 (epoch 20/100), loss = 0.144101 (0.235 sec/batch), lr: 0.510300
2019-12-27 12:37:46.554897: step 26820/136300 (epoch 20/100), loss = 0.140578 (0.248 sec/batch), lr: 0.510300
2019-12-27 12:37:51.337141: step 26840/136300 (epoch 20/100), loss = 0.325410 (0.242 sec/batch), lr: 0.510300
2019-12-27 12:37:56.252734: step 26860/136300 (epoch 20/100), loss = 0.297262 (0.179 sec/batch), lr: 0.510300
2019-12-27 12:38:01.157849: step 26880/136300 (epoch 20/100), loss = 0.389115 (0.237 sec/batch), lr: 0.510300
2019-12-27 12:38:07.241854: step 26900/136300 (epoch 20/100), loss = 0.596467 (0.199 sec/batch), lr: 0.510300
2019-12-27 12:38:12.077553: step 26920/136300 (epoch 20/100), loss = 0.299995 (0.234 sec/batch), lr: 0.510300
2019-12-27 12:38:16.701779: step 26940/136300 (epoch 20/100), loss = 0.455935 (0.211 sec/batch), lr: 0.510300
2019-12-27 12:38:21.443777: step 26960/136300 (epoch 20/100), loss = 0.466716 (0.236 sec/batch), lr: 0.510300
2019-12-27 12:38:26.279140: step 26980/136300 (epoch 20/100), loss = 0.325567 (0.244 sec/batch), lr: 0.510300
2019-12-27 12:38:30.996322: step 27000/136300 (epoch 20/100), loss = 0.381859 (0.217 sec/batch), lr: 0.510300
2019-12-27 12:38:35.882641: step 27020/136300 (epoch 20/100), loss = 0.271518 (0.219 sec/batch), lr: 0.510300
2019-12-27 12:38:40.675037: step 27040/136300 (epoch 20/100), loss = 0.414736 (0.184 sec/batch), lr: 0.510300
2019-12-27 12:38:45.432695: step 27060/136300 (epoch 20/100), loss = 0.148032 (0.215 sec/batch), lr: 0.510300
2019-12-27 12:38:50.097123: step 27080/136300 (epoch 20/100), loss = 0.427557 (0.165 sec/batch), lr: 0.510300
2019-12-27 12:38:56.253748: step 27100/136300 (epoch 20/100), loss = 0.316602 (0.235 sec/batch), lr: 0.510300
2019-12-27 12:39:01.120536: step 27120/136300 (epoch 20/100), loss = 0.447612 (0.231 sec/batch), lr: 0.510300
2019-12-27 12:39:05.993193: step 27140/136300 (epoch 20/100), loss = 0.616237 (0.204 sec/batch), lr: 0.510300
2019-12-27 12:39:10.796024: step 27160/136300 (epoch 20/100), loss = 0.441690 (0.221 sec/batch), lr: 0.510300
2019-12-27 12:39:15.481287: step 27180/136300 (epoch 20/100), loss = 0.269988 (0.218 sec/batch), lr: 0.510300
2019-12-27 12:39:20.217033: step 27200/136300 (epoch 20/100), loss = 0.357352 (0.204 sec/batch), lr: 0.510300
2019-12-27 12:39:25.067081: step 27220/136300 (epoch 20/100), loss = 0.282490 (0.233 sec/batch), lr: 0.510300
2019-12-27 12:39:29.971475: step 27240/136300 (epoch 20/100), loss = 0.243976 (0.241 sec/batch), lr: 0.510300
2019-12-27 12:39:34.735234: step 27260/136300 (epoch 20/100), loss = 0.079080 (0.130 sec/batch), lr: 0.510300
Evaluating on dev set...
Precision (micro): 71.075%
   Recall (micro): 52.888%
       F1 (micro): 60.648%
epoch 20: train_loss = 0.370251, dev_loss = 0.478142, dev_f1 = 0.6065
model saved to ./saved_models/01/checkpoint_epoch_20.pt
new best model saved.

2019-12-27 12:40:15.218976: step 27280/136300 (epoch 21/100), loss = 0.485096 (0.218 sec/batch), lr: 0.510300
2019-12-27 12:40:19.923864: step 27300/136300 (epoch 21/100), loss = 0.144946 (0.235 sec/batch), lr: 0.510300
2019-12-27 12:40:24.738481: step 27320/136300 (epoch 21/100), loss = 0.228588 (0.219 sec/batch), lr: 0.510300
2019-12-27 12:40:29.591613: step 27340/136300 (epoch 21/100), loss = 0.242913 (0.223 sec/batch), lr: 0.510300
2019-12-27 12:40:34.394328: step 27360/136300 (epoch 21/100), loss = 0.232784 (0.233 sec/batch), lr: 0.510300
2019-12-27 12:40:39.159609: step 27380/136300 (epoch 21/100), loss = 0.258471 (0.245 sec/batch), lr: 0.510300
2019-12-27 12:40:43.942647: step 27400/136300 (epoch 21/100), loss = 0.392303 (0.245 sec/batch), lr: 0.510300
2019-12-27 12:40:48.738541: step 27420/136300 (epoch 21/100), loss = 0.201756 (0.242 sec/batch), lr: 0.510300
2019-12-27 12:40:53.622661: step 27440/136300 (epoch 21/100), loss = 0.160995 (0.235 sec/batch), lr: 0.510300
2019-12-27 12:40:58.456547: step 27460/136300 (epoch 21/100), loss = 0.370368 (0.237 sec/batch), lr: 0.510300
2019-12-27 12:41:04.631269: step 27480/136300 (epoch 21/100), loss = 0.224932 (0.204 sec/batch), lr: 0.510300
2019-12-27 12:41:09.348587: step 27500/136300 (epoch 21/100), loss = 0.352746 (0.200 sec/batch), lr: 0.510300
2019-12-27 12:41:14.139903: step 27520/136300 (epoch 21/100), loss = 0.300487 (0.210 sec/batch), lr: 0.510300
2019-12-27 12:41:18.949479: step 27540/136300 (epoch 21/100), loss = 0.289342 (0.183 sec/batch), lr: 0.510300
2019-12-27 12:41:23.777737: step 27560/136300 (epoch 21/100), loss = 0.286217 (0.239 sec/batch), lr: 0.510300
2019-12-27 12:41:28.511925: step 27580/136300 (epoch 21/100), loss = 0.264537 (0.226 sec/batch), lr: 0.510300
2019-12-27 12:41:33.283481: step 27600/136300 (epoch 21/100), loss = 0.467108 (0.210 sec/batch), lr: 0.510300
2019-12-27 12:41:38.137139: step 27620/136300 (epoch 21/100), loss = 0.735098 (0.218 sec/batch), lr: 0.510300
2019-12-27 12:41:42.881162: step 27640/136300 (epoch 21/100), loss = 0.501430 (0.234 sec/batch), lr: 0.510300
2019-12-27 12:41:47.630043: step 27660/136300 (epoch 21/100), loss = 0.385261 (0.245 sec/batch), lr: 0.510300
2019-12-27 12:41:53.914855: step 27680/136300 (epoch 21/100), loss = 0.367146 (0.233 sec/batch), lr: 0.510300
2019-12-27 12:41:58.746651: step 27700/136300 (epoch 21/100), loss = 0.513150 (0.229 sec/batch), lr: 0.510300
2019-12-27 12:42:03.595547: step 27720/136300 (epoch 21/100), loss = 0.480404 (0.247 sec/batch), lr: 0.510300
2019-12-27 12:42:08.427844: step 27740/136300 (epoch 21/100), loss = 0.466634 (0.246 sec/batch), lr: 0.510300
2019-12-27 12:42:13.192121: step 27760/136300 (epoch 21/100), loss = 0.379128 (0.229 sec/batch), lr: 0.510300
2019-12-27 12:42:18.077710: step 27780/136300 (epoch 21/100), loss = 0.450485 (0.242 sec/batch), lr: 0.510300
2019-12-27 12:42:22.976690: step 27800/136300 (epoch 21/100), loss = 0.346596 (0.239 sec/batch), lr: 0.510300
2019-12-27 12:42:27.751215: step 27820/136300 (epoch 21/100), loss = 0.437204 (0.216 sec/batch), lr: 0.510300
2019-12-27 12:42:32.551523: step 27840/136300 (epoch 21/100), loss = 0.325690 (0.213 sec/batch), lr: 0.510300
2019-12-27 12:42:37.232888: step 27860/136300 (epoch 21/100), loss = 0.424998 (0.206 sec/batch), lr: 0.510300
2019-12-27 12:42:43.176248: step 27880/136300 (epoch 21/100), loss = 0.423579 (0.226 sec/batch), lr: 0.510300
2019-12-27 12:42:47.942617: step 27900/136300 (epoch 21/100), loss = 0.363711 (0.179 sec/batch), lr: 0.510300
2019-12-27 12:42:52.686778: step 27920/136300 (epoch 21/100), loss = 0.216639 (0.207 sec/batch), lr: 0.510300
2019-12-27 12:42:57.541023: step 27940/136300 (epoch 21/100), loss = 0.512778 (0.227 sec/batch), lr: 0.510300
2019-12-27 12:43:02.458868: step 27960/136300 (epoch 21/100), loss = 0.563807 (0.241 sec/batch), lr: 0.510300
2019-12-27 12:43:07.330023: step 27980/136300 (epoch 21/100), loss = 0.381997 (0.227 sec/batch), lr: 0.510300
2019-12-27 12:43:12.179043: step 28000/136300 (epoch 21/100), loss = 0.227831 (0.173 sec/batch), lr: 0.510300
2019-12-27 12:43:16.964793: step 28020/136300 (epoch 21/100), loss = 0.395632 (0.244 sec/batch), lr: 0.510300
2019-12-27 12:43:21.743017: step 28040/136300 (epoch 21/100), loss = 0.253979 (0.248 sec/batch), lr: 0.510300
2019-12-27 12:43:28.162175: step 28060/136300 (epoch 21/100), loss = 0.761333 (0.207 sec/batch), lr: 0.510300
2019-12-27 12:43:32.948153: step 28080/136300 (epoch 21/100), loss = 0.481277 (0.245 sec/batch), lr: 0.510300
2019-12-27 12:43:37.789907: step 28100/136300 (epoch 21/100), loss = 0.308386 (0.230 sec/batch), lr: 0.510300
2019-12-27 12:43:42.664859: step 28120/136300 (epoch 21/100), loss = 0.344641 (0.226 sec/batch), lr: 0.510300
2019-12-27 12:43:47.379125: step 28140/136300 (epoch 21/100), loss = 0.273117 (0.246 sec/batch), lr: 0.510300
2019-12-27 12:43:52.186687: step 28160/136300 (epoch 21/100), loss = 0.374647 (0.210 sec/batch), lr: 0.510300
2019-12-27 12:43:57.092536: step 28180/136300 (epoch 21/100), loss = 0.526662 (0.221 sec/batch), lr: 0.510300
2019-12-27 12:44:01.910510: step 28200/136300 (epoch 21/100), loss = 0.216023 (0.232 sec/batch), lr: 0.510300
2019-12-27 12:44:06.833196: step 28220/136300 (epoch 21/100), loss = 0.460094 (0.216 sec/batch), lr: 0.510300
2019-12-27 12:44:11.701070: step 28240/136300 (epoch 21/100), loss = 0.407348 (0.217 sec/batch), lr: 0.510300
2019-12-27 12:44:16.494048: step 28260/136300 (epoch 21/100), loss = 0.308434 (0.240 sec/batch), lr: 0.510300
2019-12-27 12:44:22.538131: step 28280/136300 (epoch 21/100), loss = 0.377852 (0.232 sec/batch), lr: 0.510300
2019-12-27 12:44:27.244017: step 28300/136300 (epoch 21/100), loss = 0.386328 (0.184 sec/batch), lr: 0.510300
2019-12-27 12:44:31.919267: step 28320/136300 (epoch 21/100), loss = 0.466357 (0.170 sec/batch), lr: 0.510300
2019-12-27 12:44:36.758106: step 28340/136300 (epoch 21/100), loss = 0.461437 (0.230 sec/batch), lr: 0.510300
2019-12-27 12:44:41.517705: step 28360/136300 (epoch 21/100), loss = 0.469778 (0.236 sec/batch), lr: 0.510300
2019-12-27 12:44:46.404158: step 28380/136300 (epoch 21/100), loss = 0.320814 (0.214 sec/batch), lr: 0.510300
2019-12-27 12:44:51.238807: step 28400/136300 (epoch 21/100), loss = 0.302910 (0.225 sec/batch), lr: 0.510300
2019-12-27 12:44:55.963564: step 28420/136300 (epoch 21/100), loss = 0.187795 (0.213 sec/batch), lr: 0.510300
2019-12-27 12:45:00.684791: step 28440/136300 (epoch 21/100), loss = 0.397831 (0.233 sec/batch), lr: 0.510300
2019-12-27 12:45:07.094203: step 28460/136300 (epoch 21/100), loss = 0.186717 (1.836 sec/batch), lr: 0.510300
2019-12-27 12:45:11.963580: step 28480/136300 (epoch 21/100), loss = 0.475783 (0.220 sec/batch), lr: 0.510300
2019-12-27 12:45:16.863170: step 28500/136300 (epoch 21/100), loss = 0.345033 (0.231 sec/batch), lr: 0.510300
2019-12-27 12:45:21.698845: step 28520/136300 (epoch 21/100), loss = 0.531483 (0.247 sec/batch), lr: 0.510300
2019-12-27 12:45:26.358992: step 28540/136300 (epoch 21/100), loss = 0.113856 (0.235 sec/batch), lr: 0.510300
2019-12-27 12:45:31.132569: step 28560/136300 (epoch 21/100), loss = 0.369662 (0.178 sec/batch), lr: 0.510300
2019-12-27 12:45:35.893147: step 28580/136300 (epoch 21/100), loss = 0.250487 (0.197 sec/batch), lr: 0.510300
2019-12-27 12:45:40.781195: step 28600/136300 (epoch 21/100), loss = 0.222993 (0.219 sec/batch), lr: 0.510300
2019-12-27 12:45:45.721479: step 28620/136300 (epoch 21/100), loss = 0.154351 (0.229 sec/batch), lr: 0.510300
Evaluating on dev set...
Precision (micro): 72.146%
   Recall (micro): 54.176%
       F1 (micro): 61.883%
epoch 21: train_loss = 0.372909, dev_loss = 0.473646, dev_f1 = 0.6188
model saved to ./saved_models/01/checkpoint_epoch_21.pt
new best model saved.

2019-12-27 12:46:24.748220: step 28640/136300 (epoch 22/100), loss = 0.324769 (0.246 sec/batch), lr: 0.510300
2019-12-27 12:46:30.733305: step 28660/136300 (epoch 22/100), loss = 0.165260 (0.249 sec/batch), lr: 0.510300
2019-12-27 12:46:35.544164: step 28680/136300 (epoch 22/100), loss = 0.381702 (0.217 sec/batch), lr: 0.510300
2019-12-27 12:46:40.353755: step 28700/136300 (epoch 22/100), loss = 0.566793 (0.240 sec/batch), lr: 0.510300
2019-12-27 12:46:45.156564: step 28720/136300 (epoch 22/100), loss = 0.373567 (0.217 sec/batch), lr: 0.510300
2019-12-27 12:46:49.897372: step 28740/136300 (epoch 22/100), loss = 0.281085 (0.229 sec/batch), lr: 0.510300
2019-12-27 12:46:54.685889: step 28760/136300 (epoch 22/100), loss = 0.406919 (0.223 sec/batch), lr: 0.510300
2019-12-27 12:46:59.477319: step 28780/136300 (epoch 22/100), loss = 0.616460 (0.228 sec/batch), lr: 0.510300
2019-12-27 12:47:04.333981: step 28800/136300 (epoch 22/100), loss = 0.453019 (0.234 sec/batch), lr: 0.510300
2019-12-27 12:47:09.176460: step 28820/136300 (epoch 22/100), loss = 0.496114 (0.235 sec/batch), lr: 0.510300
2019-12-27 12:47:15.133277: step 28840/136300 (epoch 22/100), loss = 0.439387 (0.241 sec/batch), lr: 0.510300
2019-12-27 12:47:19.836243: step 28860/136300 (epoch 22/100), loss = 0.401410 (0.237 sec/batch), lr: 0.510300
2019-12-27 12:47:24.604663: step 28880/136300 (epoch 22/100), loss = 0.271814 (0.233 sec/batch), lr: 0.510300
2019-12-27 12:47:29.446088: step 28900/136300 (epoch 22/100), loss = 0.393440 (0.227 sec/batch), lr: 0.510300
2019-12-27 12:47:34.231452: step 28920/136300 (epoch 22/100), loss = 0.429194 (0.178 sec/batch), lr: 0.510300
2019-12-27 12:47:38.954028: step 28940/136300 (epoch 22/100), loss = 0.206141 (0.220 sec/batch), lr: 0.510300
2019-12-27 12:47:43.781795: step 28960/136300 (epoch 22/100), loss = 0.255880 (0.237 sec/batch), lr: 0.510300
2019-12-27 12:47:48.606435: step 28980/136300 (epoch 22/100), loss = 0.283253 (0.230 sec/batch), lr: 0.510300
2019-12-27 12:47:53.316151: step 29000/136300 (epoch 22/100), loss = 0.427095 (0.227 sec/batch), lr: 0.510300
2019-12-27 12:47:58.070076: step 29020/136300 (epoch 22/100), loss = 0.329359 (0.226 sec/batch), lr: 0.510300
2019-12-27 12:48:04.228525: step 29040/136300 (epoch 22/100), loss = 0.282800 (0.242 sec/batch), lr: 0.510300
2019-12-27 12:48:09.030262: step 29060/136300 (epoch 22/100), loss = 0.620607 (0.240 sec/batch), lr: 0.510300
2019-12-27 12:48:13.877857: step 29080/136300 (epoch 22/100), loss = 0.363770 (0.242 sec/batch), lr: 0.510300
2019-12-27 12:48:18.677058: step 29100/136300 (epoch 22/100), loss = 0.553691 (0.238 sec/batch), lr: 0.510300
2019-12-27 12:48:23.462213: step 29120/136300 (epoch 22/100), loss = 0.468528 (0.216 sec/batch), lr: 0.510300
2019-12-27 12:48:28.337301: step 29140/136300 (epoch 22/100), loss = 0.484411 (0.234 sec/batch), lr: 0.510300
2019-12-27 12:48:33.222511: step 29160/136300 (epoch 22/100), loss = 0.502475 (0.241 sec/batch), lr: 0.510300
2019-12-27 12:48:37.982900: step 29180/136300 (epoch 22/100), loss = 0.456302 (0.237 sec/batch), lr: 0.510300
2019-12-27 12:48:42.778274: step 29200/136300 (epoch 22/100), loss = 0.264487 (0.238 sec/batch), lr: 0.510300
2019-12-27 12:48:47.478492: step 29220/136300 (epoch 22/100), loss = 0.390628 (0.226 sec/batch), lr: 0.510300
2019-12-27 12:48:53.591013: step 29240/136300 (epoch 22/100), loss = 0.362301 (0.199 sec/batch), lr: 0.510300
2019-12-27 12:48:58.400504: step 29260/136300 (epoch 22/100), loss = 0.276990 (0.232 sec/batch), lr: 0.510300
2019-12-27 12:49:03.116948: step 29280/136300 (epoch 22/100), loss = 0.190211 (0.227 sec/batch), lr: 0.510300
2019-12-27 12:49:07.926084: step 29300/136300 (epoch 22/100), loss = 0.416510 (0.230 sec/batch), lr: 0.510300
2019-12-27 12:49:12.877087: step 29320/136300 (epoch 22/100), loss = 0.619007 (0.233 sec/batch), lr: 0.510300
2019-12-27 12:49:17.721912: step 29340/136300 (epoch 22/100), loss = 0.289568 (0.229 sec/batch), lr: 0.510300
2019-12-27 12:49:22.605744: step 29360/136300 (epoch 22/100), loss = 0.293002 (0.235 sec/batch), lr: 0.510300
2019-12-27 12:49:27.369987: step 29380/136300 (epoch 22/100), loss = 0.313928 (0.231 sec/batch), lr: 0.510300
2019-12-27 12:49:32.168692: step 29400/136300 (epoch 22/100), loss = 0.374872 (0.216 sec/batch), lr: 0.510300
2019-12-27 12:49:37.080315: step 29420/136300 (epoch 22/100), loss = 0.445459 (0.238 sec/batch), lr: 0.510300
2019-12-27 12:49:43.229229: step 29440/136300 (epoch 22/100), loss = 0.326467 (0.208 sec/batch), lr: 0.510300
2019-12-27 12:49:47.986202: step 29460/136300 (epoch 22/100), loss = 0.267935 (0.231 sec/batch), lr: 0.510300
2019-12-27 12:49:52.908759: step 29480/136300 (epoch 22/100), loss = 0.316634 (0.227 sec/batch), lr: 0.510300
2019-12-27 12:49:57.551400: step 29500/136300 (epoch 22/100), loss = 0.377705 (0.204 sec/batch), lr: 0.510300
2019-12-27 12:50:02.403334: step 29520/136300 (epoch 22/100), loss = 0.245539 (0.227 sec/batch), lr: 0.510300
2019-12-27 12:50:07.272926: step 29540/136300 (epoch 22/100), loss = 0.333588 (0.214 sec/batch), lr: 0.510300
2019-12-27 12:50:12.056923: step 29560/136300 (epoch 22/100), loss = 0.276906 (0.236 sec/batch), lr: 0.510300
2019-12-27 12:50:16.989614: step 29580/136300 (epoch 22/100), loss = 0.296231 (0.242 sec/batch), lr: 0.510300
2019-12-27 12:50:21.840947: step 29600/136300 (epoch 22/100), loss = 0.567121 (0.234 sec/batch), lr: 0.510300
2019-12-27 12:50:26.635929: step 29620/136300 (epoch 22/100), loss = 0.283746 (0.241 sec/batch), lr: 0.510300
2019-12-27 12:50:32.841049: step 29640/136300 (epoch 22/100), loss = 0.232176 (0.215 sec/batch), lr: 0.510300
2019-12-27 12:50:37.657003: step 29660/136300 (epoch 22/100), loss = 0.504417 (0.212 sec/batch), lr: 0.510300
2019-12-27 12:50:42.299409: step 29680/136300 (epoch 22/100), loss = 0.233503 (0.215 sec/batch), lr: 0.510300
2019-12-27 12:50:47.064245: step 29700/136300 (epoch 22/100), loss = 0.362048 (0.234 sec/batch), lr: 0.510300
2019-12-27 12:50:51.841967: step 29720/136300 (epoch 22/100), loss = 0.231800 (0.201 sec/batch), lr: 0.510300
2019-12-27 12:50:56.683191: step 29740/136300 (epoch 22/100), loss = 0.239929 (0.240 sec/batch), lr: 0.510300
2019-12-27 12:51:01.518826: step 29760/136300 (epoch 22/100), loss = 0.600355 (0.199 sec/batch), lr: 0.510300
2019-12-27 12:51:06.254759: step 29780/136300 (epoch 22/100), loss = 0.507549 (0.209 sec/batch), lr: 0.510300
2019-12-27 12:51:10.928483: step 29800/136300 (epoch 22/100), loss = 0.223195 (0.240 sec/batch), lr: 0.510300
2019-12-27 12:51:15.750033: step 29820/136300 (epoch 22/100), loss = 0.310247 (0.210 sec/batch), lr: 0.510300
2019-12-27 12:51:22.003921: step 29840/136300 (epoch 22/100), loss = 0.312092 (0.210 sec/batch), lr: 0.510300
2019-12-27 12:51:26.885427: step 29860/136300 (epoch 22/100), loss = 0.237934 (0.216 sec/batch), lr: 0.510300
2019-12-27 12:51:31.720652: step 29880/136300 (epoch 22/100), loss = 0.391507 (0.204 sec/batch), lr: 0.510300
2019-12-27 12:51:36.410523: step 29900/136300 (epoch 22/100), loss = 0.268929 (0.246 sec/batch), lr: 0.510300
2019-12-27 12:51:41.231576: step 29920/136300 (epoch 22/100), loss = 0.265723 (0.242 sec/batch), lr: 0.510300
2019-12-27 12:51:45.955763: step 29940/136300 (epoch 22/100), loss = 0.554619 (0.213 sec/batch), lr: 0.510300
2019-12-27 12:51:50.858406: step 29960/136300 (epoch 22/100), loss = 0.539795 (0.199 sec/batch), lr: 0.510300
2019-12-27 12:51:55.747867: step 29980/136300 (epoch 22/100), loss = 0.461802 (0.241 sec/batch), lr: 0.510300
Evaluating on dev set...
Precision (micro): 69.020%
   Recall (micro): 54.673%
       F1 (micro): 61.014%
epoch 22: train_loss = 0.370668, dev_loss = 0.478363, dev_f1 = 0.6101
model saved to ./saved_models/01/checkpoint_epoch_22.pt

2019-12-27 12:52:35.029102: step 30000/136300 (epoch 23/100), loss = 0.312837 (0.230 sec/batch), lr: 0.459270
2019-12-27 12:52:41.133430: step 30020/136300 (epoch 23/100), loss = 0.345650 (0.183 sec/batch), lr: 0.459270
2019-12-27 12:52:45.923753: step 30040/136300 (epoch 23/100), loss = 0.365590 (0.168 sec/batch), lr: 0.459270
2019-12-27 12:52:50.667101: step 30060/136300 (epoch 23/100), loss = 0.471217 (0.221 sec/batch), lr: 0.459270
2019-12-27 12:52:55.570492: step 30080/136300 (epoch 23/100), loss = 0.401658 (0.227 sec/batch), lr: 0.459270
2019-12-27 12:53:00.293656: step 30100/136300 (epoch 23/100), loss = 0.350294 (0.248 sec/batch), lr: 0.459270
2019-12-27 12:53:05.054636: step 30120/136300 (epoch 23/100), loss = 0.240386 (0.218 sec/batch), lr: 0.459270
2019-12-27 12:53:09.820213: step 30140/136300 (epoch 23/100), loss = 0.381365 (0.242 sec/batch), lr: 0.459270
2019-12-27 12:53:14.673616: step 30160/136300 (epoch 23/100), loss = 0.274157 (0.241 sec/batch), lr: 0.459270
2019-12-27 12:53:19.509096: step 30180/136300 (epoch 23/100), loss = 0.459066 (0.213 sec/batch), lr: 0.459270
2019-12-27 12:53:25.821213: step 30200/136300 (epoch 23/100), loss = 0.228596 (0.241 sec/batch), lr: 0.459270
2019-12-27 12:53:30.487124: step 30220/136300 (epoch 23/100), loss = 0.570283 (0.223 sec/batch), lr: 0.459270
2019-12-27 12:53:35.268929: step 30240/136300 (epoch 23/100), loss = 0.449126 (0.237 sec/batch), lr: 0.459270
2019-12-27 12:53:40.138817: step 30260/136300 (epoch 23/100), loss = 0.273873 (0.223 sec/batch), lr: 0.459270
2019-12-27 12:53:44.962736: step 30280/136300 (epoch 23/100), loss = 0.432399 (0.239 sec/batch), lr: 0.459270
2019-12-27 12:53:49.659553: step 30300/136300 (epoch 23/100), loss = 0.331761 (0.243 sec/batch), lr: 0.459270
2019-12-27 12:53:54.446006: step 30320/136300 (epoch 23/100), loss = 0.523675 (0.236 sec/batch), lr: 0.459270
2019-12-27 12:53:59.240795: step 30340/136300 (epoch 23/100), loss = 0.211495 (0.206 sec/batch), lr: 0.459270
2019-12-27 12:54:03.982587: step 30360/136300 (epoch 23/100), loss = 0.191248 (0.228 sec/batch), lr: 0.459270
2019-12-27 12:54:08.733969: step 30380/136300 (epoch 23/100), loss = 0.264193 (0.246 sec/batch), lr: 0.459270
2019-12-27 12:54:14.822679: step 30400/136300 (epoch 23/100), loss = 0.243466 (0.243 sec/batch), lr: 0.459270
2019-12-27 12:54:19.613791: step 30420/136300 (epoch 23/100), loss = 0.412345 (0.230 sec/batch), lr: 0.459270
2019-12-27 12:54:24.484128: step 30440/136300 (epoch 23/100), loss = 0.543192 (0.246 sec/batch), lr: 0.459270
2019-12-27 12:54:29.274417: step 30460/136300 (epoch 23/100), loss = 0.564010 (0.231 sec/batch), lr: 0.459270
2019-12-27 12:54:34.094597: step 30480/136300 (epoch 23/100), loss = 0.419789 (0.235 sec/batch), lr: 0.459270
2019-12-27 12:54:38.963609: step 30500/136300 (epoch 23/100), loss = 0.148921 (0.232 sec/batch), lr: 0.459270
2019-12-27 12:54:43.811969: step 30520/136300 (epoch 23/100), loss = 0.333136 (0.244 sec/batch), lr: 0.459270
2019-12-27 12:54:48.595322: step 30540/136300 (epoch 23/100), loss = 0.431091 (0.200 sec/batch), lr: 0.459270
2019-12-27 12:54:53.445595: step 30560/136300 (epoch 23/100), loss = 0.236085 (0.241 sec/batch), lr: 0.459270
2019-12-27 12:54:58.217283: step 30580/136300 (epoch 23/100), loss = 0.268480 (0.237 sec/batch), lr: 0.459270
2019-12-27 12:55:04.214698: step 30600/136300 (epoch 23/100), loss = 0.402489 (0.248 sec/batch), lr: 0.459270
2019-12-27 12:55:08.987208: step 30620/136300 (epoch 23/100), loss = 0.268402 (0.249 sec/batch), lr: 0.459270
2019-12-27 12:55:13.716737: step 30640/136300 (epoch 23/100), loss = 0.244788 (0.242 sec/batch), lr: 0.459270
2019-12-27 12:55:18.519400: step 30660/136300 (epoch 23/100), loss = 0.387819 (0.232 sec/batch), lr: 0.459270
2019-12-27 12:55:23.418646: step 30680/136300 (epoch 23/100), loss = 0.245258 (0.239 sec/batch), lr: 0.459270
2019-12-27 12:55:28.268178: step 30700/136300 (epoch 23/100), loss = 0.373318 (0.230 sec/batch), lr: 0.459270
2019-12-27 12:55:33.184638: step 30720/136300 (epoch 23/100), loss = 0.330622 (0.235 sec/batch), lr: 0.459270
2019-12-27 12:55:38.010665: step 30740/136300 (epoch 23/100), loss = 0.485138 (0.243 sec/batch), lr: 0.459270
2019-12-27 12:55:42.732330: step 30760/136300 (epoch 23/100), loss = 0.362398 (0.237 sec/batch), lr: 0.459270
2019-12-27 12:55:47.623477: step 30780/136300 (epoch 23/100), loss = 0.384630 (0.219 sec/batch), lr: 0.459270
2019-12-27 12:55:53.686944: step 30800/136300 (epoch 23/100), loss = 0.706899 (0.221 sec/batch), lr: 0.459270
2019-12-27 12:55:58.447855: step 30820/136300 (epoch 23/100), loss = 0.244442 (0.218 sec/batch), lr: 0.459270
2019-12-27 12:56:03.346520: step 30840/136300 (epoch 23/100), loss = 0.434803 (0.228 sec/batch), lr: 0.459270
2019-12-27 12:56:08.007870: step 30860/136300 (epoch 23/100), loss = 0.402707 (0.241 sec/batch), lr: 0.459270
2019-12-27 12:56:12.863992: step 30880/136300 (epoch 23/100), loss = 0.307071 (0.229 sec/batch), lr: 0.459270
2019-12-27 12:56:17.772470: step 30900/136300 (epoch 23/100), loss = 0.295806 (0.221 sec/batch), lr: 0.459270
2019-12-27 12:56:22.535012: step 30920/136300 (epoch 23/100), loss = 0.415239 (0.212 sec/batch), lr: 0.459270
2019-12-27 12:56:27.470271: step 30940/136300 (epoch 23/100), loss = 0.281303 (0.216 sec/batch), lr: 0.459270
2019-12-27 12:56:32.325836: step 30960/136300 (epoch 23/100), loss = 0.322935 (0.213 sec/batch), lr: 0.459270
2019-12-27 12:56:37.156068: step 30980/136300 (epoch 23/100), loss = 0.436118 (0.219 sec/batch), lr: 0.459270
2019-12-27 12:56:43.295506: step 31000/136300 (epoch 23/100), loss = 0.369742 (0.199 sec/batch), lr: 0.459270
2019-12-27 12:56:48.133785: step 31020/136300 (epoch 23/100), loss = 0.141767 (0.239 sec/batch), lr: 0.459270
2019-12-27 12:56:52.776822: step 31040/136300 (epoch 23/100), loss = 0.314503 (0.212 sec/batch), lr: 0.459270
2019-12-27 12:56:57.512326: step 31060/136300 (epoch 23/100), loss = 0.355917 (0.241 sec/batch), lr: 0.459270
2019-12-27 12:57:02.310992: step 31080/136300 (epoch 23/100), loss = 0.342425 (0.204 sec/batch), lr: 0.459270
2019-12-27 12:57:07.127228: step 31100/136300 (epoch 23/100), loss = 0.472417 (0.240 sec/batch), lr: 0.459270
2019-12-27 12:57:11.993410: step 31120/136300 (epoch 23/100), loss = 0.385515 (0.235 sec/batch), lr: 0.459270
2019-12-27 12:57:16.719830: step 31140/136300 (epoch 23/100), loss = 0.375091 (0.238 sec/batch), lr: 0.459270
2019-12-27 12:57:21.380650: step 31160/136300 (epoch 23/100), loss = 0.446940 (0.205 sec/batch), lr: 0.459270
2019-12-27 12:57:26.187086: step 31180/136300 (epoch 23/100), loss = 0.376592 (0.213 sec/batch), lr: 0.459270
2019-12-27 12:57:32.287835: step 31200/136300 (epoch 23/100), loss = 0.437044 (0.198 sec/batch), lr: 0.459270
2019-12-27 12:57:37.178883: step 31220/136300 (epoch 23/100), loss = 0.413068 (0.212 sec/batch), lr: 0.459270
2019-12-27 12:57:42.054485: step 31240/136300 (epoch 23/100), loss = 0.363454 (0.217 sec/batch), lr: 0.459270
2019-12-27 12:57:46.701510: step 31260/136300 (epoch 23/100), loss = 0.377078 (0.246 sec/batch), lr: 0.459270
2019-12-27 12:57:51.469235: step 31280/136300 (epoch 23/100), loss = 0.378071 (0.228 sec/batch), lr: 0.459270
2019-12-27 12:57:56.250150: step 31300/136300 (epoch 23/100), loss = 0.357575 (0.227 sec/batch), lr: 0.459270
2019-12-27 12:58:01.135985: step 31320/136300 (epoch 23/100), loss = 0.092830 (0.228 sec/batch), lr: 0.459270
2019-12-27 12:58:05.980693: step 31340/136300 (epoch 23/100), loss = 0.313421 (0.231 sec/batch), lr: 0.459270
Evaluating on dev set...
Precision (micro): 72.869%
   Recall (micro): 53.458%
       F1 (micro): 61.672%
epoch 23: train_loss = 0.357320, dev_loss = 0.483354, dev_f1 = 0.6167
model saved to ./saved_models/01/checkpoint_epoch_23.pt

2019-12-27 12:58:44.954719: step 31360/136300 (epoch 24/100), loss = 0.185745 (0.245 sec/batch), lr: 0.459270
2019-12-27 12:58:50.890206: step 31380/136300 (epoch 24/100), loss = 0.367456 (0.209 sec/batch), lr: 0.459270
2019-12-27 12:58:55.707224: step 31400/136300 (epoch 24/100), loss = 0.425285 (0.243 sec/batch), lr: 0.459270
2019-12-27 12:59:00.407831: step 31420/136300 (epoch 24/100), loss = 0.293484 (0.236 sec/batch), lr: 0.459270
2019-12-27 12:59:05.344669: step 31440/136300 (epoch 24/100), loss = 0.257962 (0.234 sec/batch), lr: 0.459270
2019-12-27 12:59:10.056048: step 31460/136300 (epoch 24/100), loss = 0.395576 (0.181 sec/batch), lr: 0.459270
2019-12-27 12:59:14.833295: step 31480/136300 (epoch 24/100), loss = 0.507379 (0.211 sec/batch), lr: 0.459270
2019-12-27 12:59:19.664009: step 31500/136300 (epoch 24/100), loss = 0.311831 (0.248 sec/batch), lr: 0.459270
2019-12-27 12:59:24.469487: step 31520/136300 (epoch 24/100), loss = 0.585737 (0.214 sec/batch), lr: 0.459270
2019-12-27 12:59:29.348500: step 31540/136300 (epoch 24/100), loss = 0.281148 (0.226 sec/batch), lr: 0.459270
2019-12-27 12:59:34.136889: step 31560/136300 (epoch 24/100), loss = 0.288690 (0.240 sec/batch), lr: 0.459270
2019-12-27 12:59:40.258757: step 31580/136300 (epoch 24/100), loss = 0.414614 (0.245 sec/batch), lr: 0.459270
2019-12-27 12:59:45.004470: step 31600/136300 (epoch 24/100), loss = 0.443415 (0.241 sec/batch), lr: 0.459270
2019-12-27 12:59:49.858611: step 31620/136300 (epoch 24/100), loss = 0.352674 (0.231 sec/batch), lr: 0.459270
2019-12-27 12:59:54.666207: step 31640/136300 (epoch 24/100), loss = 0.395274 (0.236 sec/batch), lr: 0.459270
2019-12-27 12:59:59.379752: step 31660/136300 (epoch 24/100), loss = 0.172255 (0.236 sec/batch), lr: 0.459270
2019-12-27 13:00:04.117260: step 31680/136300 (epoch 24/100), loss = 0.274064 (0.222 sec/batch), lr: 0.459270
2019-12-27 13:00:08.935585: step 31700/136300 (epoch 24/100), loss = 0.348466 (0.232 sec/batch), lr: 0.459270
2019-12-27 13:00:13.756557: step 31720/136300 (epoch 24/100), loss = 0.246796 (0.230 sec/batch), lr: 0.459270
2019-12-27 13:00:18.434754: step 31740/136300 (epoch 24/100), loss = 0.247973 (0.218 sec/batch), lr: 0.459270
2019-12-27 13:00:24.301168: step 31760/136300 (epoch 24/100), loss = 0.628301 (0.215 sec/batch), lr: 0.459270
2019-12-27 13:00:29.090042: step 31780/136300 (epoch 24/100), loss = 0.359597 (0.243 sec/batch), lr: 0.459270
2019-12-27 13:00:34.003348: step 31800/136300 (epoch 24/100), loss = 0.406434 (0.230 sec/batch), lr: 0.459270
2019-12-27 13:00:38.827828: step 31820/136300 (epoch 24/100), loss = 0.589918 (0.194 sec/batch), lr: 0.459270
2019-12-27 13:00:43.621523: step 31840/136300 (epoch 24/100), loss = 0.302835 (0.248 sec/batch), lr: 0.459270
2019-12-27 13:00:48.455413: step 31860/136300 (epoch 24/100), loss = 0.306990 (0.222 sec/batch), lr: 0.459270
2019-12-27 13:00:53.323445: step 31880/136300 (epoch 24/100), loss = 0.709649 (0.238 sec/batch), lr: 0.459270
2019-12-27 13:00:58.169229: step 31900/136300 (epoch 24/100), loss = 0.364651 (0.233 sec/batch), lr: 0.459270
2019-12-27 13:01:02.956821: step 31920/136300 (epoch 24/100), loss = 0.159561 (0.228 sec/batch), lr: 0.459270
2019-12-27 13:01:07.797413: step 31940/136300 (epoch 24/100), loss = 0.178468 (0.248 sec/batch), lr: 0.459270
2019-12-27 13:01:13.810024: step 31960/136300 (epoch 24/100), loss = 0.478567 (0.244 sec/batch), lr: 0.459270
2019-12-27 13:01:18.566558: step 31980/136300 (epoch 24/100), loss = 0.248642 (0.215 sec/batch), lr: 0.459270
2019-12-27 13:01:23.300617: step 32000/136300 (epoch 24/100), loss = 0.202107 (0.235 sec/batch), lr: 0.459270
2019-12-27 13:01:28.096518: step 32020/136300 (epoch 24/100), loss = 0.375709 (0.207 sec/batch), lr: 0.459270
2019-12-27 13:01:32.996405: step 32040/136300 (epoch 24/100), loss = 0.274982 (0.230 sec/batch), lr: 0.459270
2019-12-27 13:01:37.865581: step 32060/136300 (epoch 24/100), loss = 0.311397 (0.234 sec/batch), lr: 0.459270
2019-12-27 13:01:42.745565: step 32080/136300 (epoch 24/100), loss = 0.427587 (0.234 sec/batch), lr: 0.459270
2019-12-27 13:01:47.575071: step 32100/136300 (epoch 24/100), loss = 0.318726 (0.204 sec/batch), lr: 0.459270
2019-12-27 13:01:52.317642: step 32120/136300 (epoch 24/100), loss = 0.373474 (0.253 sec/batch), lr: 0.459270
2019-12-27 13:01:57.186413: step 32140/136300 (epoch 24/100), loss = 0.347838 (0.249 sec/batch), lr: 0.459270
2019-12-27 13:02:03.247796: step 32160/136300 (epoch 24/100), loss = 0.642848 (0.236 sec/batch), lr: 0.459270
2019-12-27 13:02:07.985448: step 32180/136300 (epoch 24/100), loss = 0.385024 (0.179 sec/batch), lr: 0.459270
2019-12-27 13:02:12.974151: step 32200/136300 (epoch 24/100), loss = 0.116060 (0.225 sec/batch), lr: 0.459270
2019-12-27 13:02:17.571996: step 32220/136300 (epoch 24/100), loss = 0.233184 (0.170 sec/batch), lr: 0.459270
2019-12-27 13:02:22.458816: step 32240/136300 (epoch 24/100), loss = 0.298124 (0.201 sec/batch), lr: 0.459270
2019-12-27 13:02:27.403270: step 32260/136300 (epoch 24/100), loss = 0.255001 (0.235 sec/batch), lr: 0.459270
2019-12-27 13:02:32.184261: step 32280/136300 (epoch 24/100), loss = 0.290444 (0.202 sec/batch), lr: 0.459270
2019-12-27 13:02:37.117220: step 32300/136300 (epoch 24/100), loss = 0.344092 (0.236 sec/batch), lr: 0.459270
2019-12-27 13:02:42.032417: step 32320/136300 (epoch 24/100), loss = 0.377911 (0.227 sec/batch), lr: 0.459270
2019-12-27 13:02:46.823930: step 32340/136300 (epoch 24/100), loss = 0.426915 (0.243 sec/batch), lr: 0.459270
2019-12-27 13:02:53.246924: step 32360/136300 (epoch 24/100), loss = 0.393910 (0.223 sec/batch), lr: 0.459270
2019-12-27 13:02:58.116633: step 32380/136300 (epoch 24/100), loss = 0.387602 (0.247 sec/batch), lr: 0.459270
2019-12-27 13:03:02.712908: step 32400/136300 (epoch 24/100), loss = 0.358368 (0.238 sec/batch), lr: 0.459270
2019-12-27 13:03:07.398332: step 32420/136300 (epoch 24/100), loss = 0.294332 (0.240 sec/batch), lr: 0.459270
2019-12-27 13:03:12.185551: step 32440/136300 (epoch 24/100), loss = 0.166592 (0.203 sec/batch), lr: 0.459270
2019-12-27 13:03:17.026376: step 32460/136300 (epoch 24/100), loss = 0.452768 (0.243 sec/batch), lr: 0.459270
2019-12-27 13:03:21.873272: step 32480/136300 (epoch 24/100), loss = 0.534529 (0.224 sec/batch), lr: 0.459270
2019-12-27 13:03:26.577237: step 32500/136300 (epoch 24/100), loss = 0.397448 (0.198 sec/batch), lr: 0.459270
2019-12-27 13:03:31.297728: step 32520/136300 (epoch 24/100), loss = 0.395369 (0.241 sec/batch), lr: 0.459270
2019-12-27 13:03:36.046760: step 32540/136300 (epoch 24/100), loss = 0.394899 (0.244 sec/batch), lr: 0.459270
2019-12-27 13:03:42.127384: step 32560/136300 (epoch 24/100), loss = 0.392909 (0.214 sec/batch), lr: 0.459270
2019-12-27 13:03:47.016876: step 32580/136300 (epoch 24/100), loss = 0.343595 (0.200 sec/batch), lr: 0.459270
2019-12-27 13:03:51.941077: step 32600/136300 (epoch 24/100), loss = 0.438785 (0.234 sec/batch), lr: 0.459270
2019-12-27 13:03:56.532676: step 32620/136300 (epoch 24/100), loss = 0.293198 (0.195 sec/batch), lr: 0.459270
2019-12-27 13:04:01.345366: step 32640/136300 (epoch 24/100), loss = 0.395995 (0.240 sec/batch), lr: 0.459270
2019-12-27 13:04:06.095069: step 32660/136300 (epoch 24/100), loss = 0.334002 (0.235 sec/batch), lr: 0.459270
2019-12-27 13:04:11.019396: step 32680/136300 (epoch 24/100), loss = 0.238228 (0.238 sec/batch), lr: 0.459270
2019-12-27 13:04:15.821255: step 32700/136300 (epoch 24/100), loss = 0.330909 (0.239 sec/batch), lr: 0.459270
Evaluating on dev set...
Precision (micro): 75.100%
   Recall (micro): 51.987%
       F1 (micro): 61.441%
epoch 24: train_loss = 0.358971, dev_loss = 0.463418, dev_f1 = 0.6144
model saved to ./saved_models/01/checkpoint_epoch_24.pt

2019-12-27 13:04:54.450894: step 32720/136300 (epoch 25/100), loss = 0.380678 (0.203 sec/batch), lr: 0.413343
2019-12-27 13:05:00.470113: step 32740/136300 (epoch 25/100), loss = 0.276970 (0.241 sec/batch), lr: 0.413343
2019-12-27 13:05:05.292179: step 32760/136300 (epoch 25/100), loss = 0.286726 (0.246 sec/batch), lr: 0.413343
2019-12-27 13:05:09.961193: step 32780/136300 (epoch 25/100), loss = 0.570773 (0.201 sec/batch), lr: 0.413343
2019-12-27 13:05:14.865608: step 32800/136300 (epoch 25/100), loss = 0.241534 (0.238 sec/batch), lr: 0.413343
2019-12-27 13:05:19.665862: step 32820/136300 (epoch 25/100), loss = 0.363537 (0.232 sec/batch), lr: 0.413343
2019-12-27 13:05:24.375983: step 32840/136300 (epoch 25/100), loss = 0.401133 (0.230 sec/batch), lr: 0.413343
2019-12-27 13:05:29.233267: step 32860/136300 (epoch 25/100), loss = 0.225173 (0.229 sec/batch), lr: 0.413343
2019-12-27 13:05:33.998046: step 32880/136300 (epoch 25/100), loss = 0.174684 (0.210 sec/batch), lr: 0.413343
2019-12-27 13:05:38.861164: step 32900/136300 (epoch 25/100), loss = 0.290615 (0.202 sec/batch), lr: 0.413343
2019-12-27 13:05:43.675221: step 32920/136300 (epoch 25/100), loss = 0.443495 (0.232 sec/batch), lr: 0.413343
2019-12-27 13:05:49.614000: step 32940/136300 (epoch 25/100), loss = 0.491647 (0.237 sec/batch), lr: 0.413343
2019-12-27 13:05:54.434922: step 32960/136300 (epoch 25/100), loss = 0.442475 (0.239 sec/batch), lr: 0.413343
2019-12-27 13:05:59.221739: step 32980/136300 (epoch 25/100), loss = 0.162633 (0.243 sec/batch), lr: 0.413343
2019-12-27 13:06:03.985598: step 33000/136300 (epoch 25/100), loss = 0.524594 (0.240 sec/batch), lr: 0.413343
2019-12-27 13:06:08.768866: step 33020/136300 (epoch 25/100), loss = 0.389025 (0.230 sec/batch), lr: 0.413343
2019-12-27 13:06:13.494456: step 33040/136300 (epoch 25/100), loss = 0.202548 (0.215 sec/batch), lr: 0.413343
2019-12-27 13:06:18.272662: step 33060/136300 (epoch 25/100), loss = 0.187959 (0.207 sec/batch), lr: 0.413343
2019-12-27 13:06:23.155814: step 33080/136300 (epoch 25/100), loss = 0.338985 (0.248 sec/batch), lr: 0.413343
2019-12-27 13:06:27.854475: step 33100/136300 (epoch 25/100), loss = 0.270560 (0.178 sec/batch), lr: 0.413343
2019-12-27 13:06:32.528002: step 33120/136300 (epoch 25/100), loss = 0.345500 (0.250 sec/batch), lr: 0.413343
2019-12-27 13:06:38.612683: step 33140/136300 (epoch 25/100), loss = 0.226589 (0.217 sec/batch), lr: 0.413343
2019-12-27 13:06:43.539465: step 33160/136300 (epoch 25/100), loss = 0.359847 (0.220 sec/batch), lr: 0.413343
2019-12-27 13:06:48.377809: step 33180/136300 (epoch 25/100), loss = 0.299169 (0.174 sec/batch), lr: 0.413343
2019-12-27 13:06:53.174323: step 33200/136300 (epoch 25/100), loss = 0.349169 (0.231 sec/batch), lr: 0.413343
2019-12-27 13:06:58.014340: step 33220/136300 (epoch 25/100), loss = 0.301528 (0.240 sec/batch), lr: 0.413343
2019-12-27 13:07:02.832573: step 33240/136300 (epoch 25/100), loss = 0.498609 (0.218 sec/batch), lr: 0.413343
2019-12-27 13:07:07.731336: step 33260/136300 (epoch 25/100), loss = 0.260389 (0.179 sec/batch), lr: 0.413343
2019-12-27 13:07:12.493725: step 33280/136300 (epoch 25/100), loss = 0.553717 (0.168 sec/batch), lr: 0.413343
2019-12-27 13:07:17.264518: step 33300/136300 (epoch 25/100), loss = 0.374504 (0.228 sec/batch), lr: 0.413343
2019-12-27 13:07:23.367793: step 33320/136300 (epoch 25/100), loss = 0.289112 (0.180 sec/batch), lr: 0.413343
2019-12-27 13:07:28.124346: step 33340/136300 (epoch 25/100), loss = 0.491450 (0.220 sec/batch), lr: 0.413343
2019-12-27 13:07:32.854732: step 33360/136300 (epoch 25/100), loss = 0.265480 (0.205 sec/batch), lr: 0.413343
2019-12-27 13:07:37.648800: step 33380/136300 (epoch 25/100), loss = 0.419228 (0.218 sec/batch), lr: 0.413343
2019-12-27 13:07:42.548634: step 33400/136300 (epoch 25/100), loss = 0.441726 (0.250 sec/batch), lr: 0.413343
2019-12-27 13:07:47.424409: step 33420/136300 (epoch 25/100), loss = 0.484781 (0.230 sec/batch), lr: 0.413343
2019-12-27 13:07:52.312911: step 33440/136300 (epoch 25/100), loss = 0.240649 (0.230 sec/batch), lr: 0.413343
2019-12-27 13:07:57.172762: step 33460/136300 (epoch 25/100), loss = 0.491809 (0.216 sec/batch), lr: 0.413343
2019-12-27 13:08:01.861079: step 33480/136300 (epoch 25/100), loss = 0.442403 (0.170 sec/batch), lr: 0.413343
2019-12-27 13:08:06.763862: step 33500/136300 (epoch 25/100), loss = 0.406340 (0.241 sec/batch), lr: 0.413343
2019-12-27 13:08:12.990099: step 33520/136300 (epoch 25/100), loss = 0.322182 (0.216 sec/batch), lr: 0.413343
2019-12-27 13:08:17.713732: step 33540/136300 (epoch 25/100), loss = 0.394786 (0.234 sec/batch), lr: 0.413343
2019-12-27 13:08:22.662926: step 33560/136300 (epoch 25/100), loss = 0.569588 (0.225 sec/batch), lr: 0.413343
2019-12-27 13:08:27.371364: step 33580/136300 (epoch 25/100), loss = 0.254677 (0.235 sec/batch), lr: 0.413343
2019-12-27 13:08:32.149203: step 33600/136300 (epoch 25/100), loss = 0.168008 (0.239 sec/batch), lr: 0.413343
2019-12-27 13:08:37.039086: step 33620/136300 (epoch 25/100), loss = 0.353389 (0.239 sec/batch), lr: 0.413343
2019-12-27 13:08:41.835786: step 33640/136300 (epoch 25/100), loss = 0.494350 (0.234 sec/batch), lr: 0.413343
2019-12-27 13:08:46.685541: step 33660/136300 (epoch 25/100), loss = 0.609439 (0.239 sec/batch), lr: 0.413343
2019-12-27 13:08:51.631435: step 33680/136300 (epoch 25/100), loss = 0.192635 (0.245 sec/batch), lr: 0.413343
2019-12-27 13:08:56.397918: step 33700/136300 (epoch 25/100), loss = 0.295365 (0.175 sec/batch), lr: 0.413343
2019-12-27 13:09:02.626067: step 33720/136300 (epoch 25/100), loss = 0.396483 (0.215 sec/batch), lr: 0.413343
2019-12-27 13:09:07.527835: step 33740/136300 (epoch 25/100), loss = 0.357465 (0.237 sec/batch), lr: 0.413343
2019-12-27 13:09:12.113604: step 33760/136300 (epoch 25/100), loss = 0.334294 (0.234 sec/batch), lr: 0.413343
2019-12-27 13:09:16.821333: step 33780/136300 (epoch 25/100), loss = 0.333384 (0.180 sec/batch), lr: 0.413343
2019-12-27 13:09:21.678817: step 33800/136300 (epoch 25/100), loss = 0.554697 (0.237 sec/batch), lr: 0.413343
2019-12-27 13:09:26.440971: step 33820/136300 (epoch 25/100), loss = 0.366327 (0.202 sec/batch), lr: 0.413343
2019-12-27 13:09:31.327036: step 33840/136300 (epoch 25/100), loss = 0.600060 (0.239 sec/batch), lr: 0.413343
2019-12-27 13:09:36.077802: step 33860/136300 (epoch 25/100), loss = 0.353579 (0.230 sec/batch), lr: 0.413343
2019-12-27 13:09:40.792997: step 33880/136300 (epoch 25/100), loss = 0.190847 (0.212 sec/batch), lr: 0.413343
2019-12-27 13:09:45.528806: step 33900/136300 (epoch 25/100), loss = 0.451297 (0.230 sec/batch), lr: 0.413343
2019-12-27 13:09:51.845873: step 33920/136300 (epoch 25/100), loss = 0.264740 (0.201 sec/batch), lr: 0.413343
2019-12-27 13:09:56.764463: step 33940/136300 (epoch 25/100), loss = 0.160409 (0.232 sec/batch), lr: 0.413343
2019-12-27 13:10:01.620165: step 33960/136300 (epoch 25/100), loss = 0.348052 (0.238 sec/batch), lr: 0.413343
2019-12-27 13:10:06.298495: step 33980/136300 (epoch 25/100), loss = 0.446941 (0.217 sec/batch), lr: 0.413343
2019-12-27 13:10:11.064235: step 34000/136300 (epoch 25/100), loss = 0.661979 (0.249 sec/batch), lr: 0.413343
2019-12-27 13:10:15.771541: step 34020/136300 (epoch 25/100), loss = 0.165614 (0.217 sec/batch), lr: 0.413343
2019-12-27 13:10:20.693413: step 34040/136300 (epoch 25/100), loss = 0.308693 (0.243 sec/batch), lr: 0.413343
2019-12-27 13:10:25.500452: step 34060/136300 (epoch 25/100), loss = 0.218877 (0.195 sec/batch), lr: 0.413343
Evaluating on dev set...
Precision (micro): 73.432%
   Recall (micro): 53.845%
       F1 (micro): 62.131%
epoch 25: train_loss = 0.348483, dev_loss = 0.465123, dev_f1 = 0.6213
model saved to ./saved_models/01/checkpoint_epoch_25.pt
new best model saved.

2019-12-27 13:11:04.538094: step 34080/136300 (epoch 26/100), loss = 0.289373 (0.244 sec/batch), lr: 0.413343
2019-12-27 13:11:10.548834: step 34100/136300 (epoch 26/100), loss = 0.333871 (0.227 sec/batch), lr: 0.413343
2019-12-27 13:11:15.328032: step 34120/136300 (epoch 26/100), loss = 0.267036 (0.235 sec/batch), lr: 0.413343
2019-12-27 13:11:20.046123: step 34140/136300 (epoch 26/100), loss = 0.438164 (0.216 sec/batch), lr: 0.413343
2019-12-27 13:11:24.932031: step 34160/136300 (epoch 26/100), loss = 0.322539 (0.218 sec/batch), lr: 0.413343
2019-12-27 13:11:29.694810: step 34180/136300 (epoch 26/100), loss = 0.432355 (0.240 sec/batch), lr: 0.413343
2019-12-27 13:11:34.440072: step 34200/136300 (epoch 26/100), loss = 0.349646 (0.236 sec/batch), lr: 0.413343
2019-12-27 13:11:39.276702: step 34220/136300 (epoch 26/100), loss = 0.510719 (0.234 sec/batch), lr: 0.413343
2019-12-27 13:11:44.041687: step 34240/136300 (epoch 26/100), loss = 0.209498 (0.210 sec/batch), lr: 0.413343
2019-12-27 13:11:48.904382: step 34260/136300 (epoch 26/100), loss = 0.542274 (0.230 sec/batch), lr: 0.413343
2019-12-27 13:11:53.693530: step 34280/136300 (epoch 26/100), loss = 0.239229 (0.244 sec/batch), lr: 0.413343
2019-12-27 13:11:59.621389: step 34300/136300 (epoch 26/100), loss = 0.284751 (0.238 sec/batch), lr: 0.413343
2019-12-27 13:12:04.412942: step 34320/136300 (epoch 26/100), loss = 0.472719 (0.217 sec/batch), lr: 0.413343
2019-12-27 13:12:09.166647: step 34340/136300 (epoch 26/100), loss = 0.172848 (0.226 sec/batch), lr: 0.413343
2019-12-27 13:12:13.946790: step 34360/136300 (epoch 26/100), loss = 0.550604 (0.219 sec/batch), lr: 0.413343
2019-12-27 13:12:18.764778: step 34380/136300 (epoch 26/100), loss = 0.521449 (0.193 sec/batch), lr: 0.413343
2019-12-27 13:12:23.428379: step 34400/136300 (epoch 26/100), loss = 0.316661 (0.217 sec/batch), lr: 0.413343
2019-12-27 13:12:28.169943: step 34420/136300 (epoch 26/100), loss = 0.325879 (0.218 sec/batch), lr: 0.413343
2019-12-27 13:12:33.068978: step 34440/136300 (epoch 26/100), loss = 0.228968 (0.221 sec/batch), lr: 0.413343
2019-12-27 13:12:37.772566: step 34460/136300 (epoch 26/100), loss = 0.124026 (0.242 sec/batch), lr: 0.413343
2019-12-27 13:12:42.351011: step 34480/136300 (epoch 26/100), loss = 0.457938 (0.214 sec/batch), lr: 0.413343
2019-12-27 13:12:48.436638: step 34500/136300 (epoch 26/100), loss = 0.210799 (0.204 sec/batch), lr: 0.413343
2019-12-27 13:12:53.363512: step 34520/136300 (epoch 26/100), loss = 0.447786 (0.233 sec/batch), lr: 0.413343
2019-12-27 13:12:58.239751: step 34540/136300 (epoch 26/100), loss = 0.188768 (0.236 sec/batch), lr: 0.413343
2019-12-27 13:13:03.026876: step 34560/136300 (epoch 26/100), loss = 0.285537 (0.243 sec/batch), lr: 0.413343
2019-12-27 13:13:07.828698: step 34580/136300 (epoch 26/100), loss = 0.393073 (0.242 sec/batch), lr: 0.413343
2019-12-27 13:13:12.618801: step 34600/136300 (epoch 26/100), loss = 0.560642 (0.197 sec/batch), lr: 0.413343
2019-12-27 13:13:17.550405: step 34620/136300 (epoch 26/100), loss = 0.376377 (0.245 sec/batch), lr: 0.413343
2019-12-27 13:13:22.298822: step 34640/136300 (epoch 26/100), loss = 0.202959 (0.248 sec/batch), lr: 0.413343
2019-12-27 13:13:27.055789: step 34660/136300 (epoch 26/100), loss = 0.434590 (0.209 sec/batch), lr: 0.413343
2019-12-27 13:13:31.811295: step 34680/136300 (epoch 26/100), loss = 0.459917 (0.245 sec/batch), lr: 0.413343
2019-12-27 13:13:37.720319: step 34700/136300 (epoch 26/100), loss = 0.300292 (0.234 sec/batch), lr: 0.413343
2019-12-27 13:13:42.439036: step 34720/136300 (epoch 26/100), loss = 0.287190 (0.219 sec/batch), lr: 0.413343
2019-12-27 13:13:47.206163: step 34740/136300 (epoch 26/100), loss = 0.348733 (0.228 sec/batch), lr: 0.413343
2019-12-27 13:13:52.072246: step 34760/136300 (epoch 26/100), loss = 0.561951 (0.217 sec/batch), lr: 0.413343
2019-12-27 13:13:56.984255: step 34780/136300 (epoch 26/100), loss = 0.319358 (0.246 sec/batch), lr: 0.413343
2019-12-27 13:14:01.847797: step 34800/136300 (epoch 26/100), loss = 0.428468 (0.223 sec/batch), lr: 0.413343
2019-12-27 13:14:06.751666: step 34820/136300 (epoch 26/100), loss = 0.372249 (0.247 sec/batch), lr: 0.413343
2019-12-27 13:14:11.481261: step 34840/136300 (epoch 26/100), loss = 0.357744 (0.226 sec/batch), lr: 0.413343
2019-12-27 13:14:16.254972: step 34860/136300 (epoch 26/100), loss = 0.252939 (0.181 sec/batch), lr: 0.413343
2019-12-27 13:14:22.658376: step 34880/136300 (epoch 26/100), loss = 0.284092 (0.220 sec/batch), lr: 0.413343
2019-12-27 13:14:27.392196: step 34900/136300 (epoch 26/100), loss = 0.175187 (0.215 sec/batch), lr: 0.413343
2019-12-27 13:14:32.294218: step 34920/136300 (epoch 26/100), loss = 0.335015 (0.247 sec/batch), lr: 0.413343
2019-12-27 13:14:37.004526: step 34940/136300 (epoch 26/100), loss = 0.448644 (0.206 sec/batch), lr: 0.413343
2019-12-27 13:14:41.734406: step 34960/136300 (epoch 26/100), loss = 0.294793 (0.229 sec/batch), lr: 0.413343
2019-12-27 13:14:46.596859: step 34980/136300 (epoch 26/100), loss = 0.239494 (0.242 sec/batch), lr: 0.413343
2019-12-27 13:14:51.516918: step 35000/136300 (epoch 26/100), loss = 0.185705 (0.217 sec/batch), lr: 0.413343
2019-12-27 13:14:56.265792: step 35020/136300 (epoch 26/100), loss = 0.559460 (0.231 sec/batch), lr: 0.413343
2019-12-27 13:15:01.197872: step 35040/136300 (epoch 26/100), loss = 0.543222 (0.250 sec/batch), lr: 0.413343
2019-12-27 13:15:06.074823: step 35060/136300 (epoch 26/100), loss = 0.487884 (0.221 sec/batch), lr: 0.413343
2019-12-27 13:15:12.090480: step 35080/136300 (epoch 26/100), loss = 0.323722 (0.239 sec/batch), lr: 0.413343
2019-12-27 13:15:16.938519: step 35100/136300 (epoch 26/100), loss = 0.225247 (0.234 sec/batch), lr: 0.413343
2019-12-27 13:15:21.559350: step 35120/136300 (epoch 26/100), loss = 0.437713 (0.229 sec/batch), lr: 0.413343
2019-12-27 13:15:26.300296: step 35140/136300 (epoch 26/100), loss = 0.597123 (0.217 sec/batch), lr: 0.413343
2019-12-27 13:15:31.109036: step 35160/136300 (epoch 26/100), loss = 0.337019 (0.203 sec/batch), lr: 0.413343
2019-12-27 13:15:35.877757: step 35180/136300 (epoch 26/100), loss = 0.302500 (0.234 sec/batch), lr: 0.413343
2019-12-27 13:15:40.759908: step 35200/136300 (epoch 26/100), loss = 0.398495 (0.238 sec/batch), lr: 0.413343
2019-12-27 13:15:45.537016: step 35220/136300 (epoch 26/100), loss = 0.200399 (0.228 sec/batch), lr: 0.413343
2019-12-27 13:15:50.282835: step 35240/136300 (epoch 26/100), loss = 0.487186 (0.220 sec/batch), lr: 0.413343
2019-12-27 13:15:55.031344: step 35260/136300 (epoch 26/100), loss = 0.508379 (0.240 sec/batch), lr: 0.413343
2019-12-27 13:16:01.268595: step 35280/136300 (epoch 26/100), loss = 0.209290 (0.213 sec/batch), lr: 0.413343
2019-12-27 13:16:06.213431: step 35300/136300 (epoch 26/100), loss = 0.499988 (0.251 sec/batch), lr: 0.413343
2019-12-27 13:16:11.127832: step 35320/136300 (epoch 26/100), loss = 0.471954 (0.235 sec/batch), lr: 0.413343
2019-12-27 13:16:15.897162: step 35340/136300 (epoch 26/100), loss = 0.417346 (0.242 sec/batch), lr: 0.413343
2019-12-27 13:16:20.600391: step 35360/136300 (epoch 26/100), loss = 0.291598 (0.232 sec/batch), lr: 0.413343
2019-12-27 13:16:25.330084: step 35380/136300 (epoch 26/100), loss = 0.161978 (0.236 sec/batch), lr: 0.413343
2019-12-27 13:16:30.198919: step 35400/136300 (epoch 26/100), loss = 0.235790 (0.235 sec/batch), lr: 0.413343
2019-12-27 13:16:35.083768: step 35420/136300 (epoch 26/100), loss = 0.559973 (0.231 sec/batch), lr: 0.413343
Evaluating on dev set...
Precision (micro): 73.012%
   Recall (micro): 54.047%
       F1 (micro): 62.114%
epoch 26: train_loss = 0.341969, dev_loss = 0.451278, dev_f1 = 0.6211
model saved to ./saved_models/01/checkpoint_epoch_26.pt

2019-12-27 13:17:13.469468: step 35440/136300 (epoch 27/100), loss = 0.335389 (0.232 sec/batch), lr: 0.372009
2019-12-27 13:17:19.567640: step 35460/136300 (epoch 27/100), loss = 0.332671 (0.213 sec/batch), lr: 0.372009
2019-12-27 13:17:24.311867: step 35480/136300 (epoch 27/100), loss = 0.430625 (0.217 sec/batch), lr: 0.372009
2019-12-27 13:17:29.106224: step 35500/136300 (epoch 27/100), loss = 0.271021 (0.215 sec/batch), lr: 0.372009
2019-12-27 13:17:33.929527: step 35520/136300 (epoch 27/100), loss = 0.352625 (0.203 sec/batch), lr: 0.372009
2019-12-27 13:17:38.701102: step 35540/136300 (epoch 27/100), loss = 0.336078 (0.225 sec/batch), lr: 0.372009
2019-12-27 13:17:43.433092: step 35560/136300 (epoch 27/100), loss = 0.306974 (0.241 sec/batch), lr: 0.372009
2019-12-27 13:17:48.251238: step 35580/136300 (epoch 27/100), loss = 0.445207 (0.236 sec/batch), lr: 0.372009
2019-12-27 13:17:53.026406: step 35600/136300 (epoch 27/100), loss = 0.129881 (0.223 sec/batch), lr: 0.372009
2019-12-27 13:17:57.885422: step 35620/136300 (epoch 27/100), loss = 0.292356 (0.216 sec/batch), lr: 0.372009
2019-12-27 13:18:02.681652: step 35640/136300 (epoch 27/100), loss = 0.295884 (0.229 sec/batch), lr: 0.372009
2019-12-27 13:18:08.777958: step 35660/136300 (epoch 27/100), loss = 0.358612 (0.235 sec/batch), lr: 0.372009
2019-12-27 13:18:13.505063: step 35680/136300 (epoch 27/100), loss = 0.464816 (0.239 sec/batch), lr: 0.372009
2019-12-27 13:18:18.292859: step 35700/136300 (epoch 27/100), loss = 0.299348 (0.215 sec/batch), lr: 0.372009
2019-12-27 13:18:23.073694: step 35720/136300 (epoch 27/100), loss = 0.309681 (0.216 sec/batch), lr: 0.372009
2019-12-27 13:18:27.920152: step 35740/136300 (epoch 27/100), loss = 0.213431 (0.244 sec/batch), lr: 0.372009
2019-12-27 13:18:32.690754: step 35760/136300 (epoch 27/100), loss = 0.280326 (0.241 sec/batch), lr: 0.372009
2019-12-27 13:18:37.428152: step 35780/136300 (epoch 27/100), loss = 0.281118 (0.230 sec/batch), lr: 0.372009
2019-12-27 13:18:42.322795: step 35800/136300 (epoch 27/100), loss = 0.280020 (0.240 sec/batch), lr: 0.372009
2019-12-27 13:18:47.062129: step 35820/136300 (epoch 27/100), loss = 0.258657 (0.217 sec/batch), lr: 0.372009
2019-12-27 13:18:51.791629: step 35840/136300 (epoch 27/100), loss = 0.293254 (0.227 sec/batch), lr: 0.372009
2019-12-27 13:18:58.094740: step 35860/136300 (epoch 27/100), loss = 0.249829 (0.216 sec/batch), lr: 0.372009
2019-12-27 13:19:02.974103: step 35880/136300 (epoch 27/100), loss = 0.318519 (0.239 sec/batch), lr: 0.372009
2019-12-27 13:19:07.866296: step 35900/136300 (epoch 27/100), loss = 0.230457 (0.233 sec/batch), lr: 0.372009
2019-12-27 13:19:12.650641: step 35920/136300 (epoch 27/100), loss = 0.181860 (0.216 sec/batch), lr: 0.372009
2019-12-27 13:19:17.441596: step 35940/136300 (epoch 27/100), loss = 0.499345 (0.225 sec/batch), lr: 0.372009
2019-12-27 13:19:22.267801: step 35960/136300 (epoch 27/100), loss = 0.232017 (0.246 sec/batch), lr: 0.372009
2019-12-27 13:19:27.207346: step 35980/136300 (epoch 27/100), loss = 0.427967 (0.236 sec/batch), lr: 0.372009
2019-12-27 13:19:31.936891: step 36000/136300 (epoch 27/100), loss = 0.336041 (0.241 sec/batch), lr: 0.372009
2019-12-27 13:19:36.717748: step 36020/136300 (epoch 27/100), loss = 0.326315 (0.218 sec/batch), lr: 0.372009
2019-12-27 13:19:41.375569: step 36040/136300 (epoch 27/100), loss = 0.221993 (0.214 sec/batch), lr: 0.372009
2019-12-27 13:19:47.478606: step 36060/136300 (epoch 27/100), loss = 0.277421 (0.222 sec/batch), lr: 0.372009
2019-12-27 13:19:52.257769: step 36080/136300 (epoch 27/100), loss = 0.503700 (0.221 sec/batch), lr: 0.372009
2019-12-27 13:19:57.011579: step 36100/136300 (epoch 27/100), loss = 0.097132 (0.223 sec/batch), lr: 0.372009
2019-12-27 13:20:01.843214: step 36120/136300 (epoch 27/100), loss = 0.612545 (0.204 sec/batch), lr: 0.372009
2019-12-27 13:20:06.734022: step 36140/136300 (epoch 27/100), loss = 0.446438 (0.208 sec/batch), lr: 0.372009
2019-12-27 13:20:11.650754: step 36160/136300 (epoch 27/100), loss = 0.356783 (0.248 sec/batch), lr: 0.372009
2019-12-27 13:20:16.516324: step 36180/136300 (epoch 27/100), loss = 0.377744 (0.248 sec/batch), lr: 0.372009
2019-12-27 13:20:21.257354: step 36200/136300 (epoch 27/100), loss = 0.380710 (0.213 sec/batch), lr: 0.372009
2019-12-27 13:20:26.062368: step 36220/136300 (epoch 27/100), loss = 0.205221 (0.245 sec/batch), lr: 0.372009
2019-12-27 13:20:32.502542: step 36240/136300 (epoch 27/100), loss = 0.326331 (0.218 sec/batch), lr: 0.372009
2019-12-27 13:20:37.249964: step 36260/136300 (epoch 27/100), loss = 0.372671 (0.201 sec/batch), lr: 0.372009
2019-12-27 13:20:42.131961: step 36280/136300 (epoch 27/100), loss = 0.300025 (0.251 sec/batch), lr: 0.372009
2019-12-27 13:20:46.936659: step 36300/136300 (epoch 27/100), loss = 0.276481 (0.215 sec/batch), lr: 0.372009
2019-12-27 13:20:51.613320: step 36320/136300 (epoch 27/100), loss = 0.412853 (0.210 sec/batch), lr: 0.372009
2019-12-27 13:20:56.440435: step 36340/136300 (epoch 27/100), loss = 0.263201 (0.231 sec/batch), lr: 0.372009
2019-12-27 13:21:01.339457: step 36360/136300 (epoch 27/100), loss = 0.131716 (0.223 sec/batch), lr: 0.372009
2019-12-27 13:21:06.141681: step 36380/136300 (epoch 27/100), loss = 0.280901 (0.249 sec/batch), lr: 0.372009
2019-12-27 13:21:11.129596: step 36400/136300 (epoch 27/100), loss = 0.260629 (0.248 sec/batch), lr: 0.372009
2019-12-27 13:21:15.985957: step 36420/136300 (epoch 27/100), loss = 0.580332 (0.242 sec/batch), lr: 0.372009
2019-12-27 13:21:22.055199: step 36440/136300 (epoch 27/100), loss = 0.457470 (1.490 sec/batch), lr: 0.372009
2019-12-27 13:21:26.878329: step 36460/136300 (epoch 27/100), loss = 0.152004 (0.223 sec/batch), lr: 0.372009
2019-12-27 13:21:31.541986: step 36480/136300 (epoch 27/100), loss = 0.279668 (0.200 sec/batch), lr: 0.372009
2019-12-27 13:21:36.276619: step 36500/136300 (epoch 27/100), loss = 0.441593 (0.233 sec/batch), lr: 0.372009
2019-12-27 13:21:41.121985: step 36520/136300 (epoch 27/100), loss = 0.130416 (0.229 sec/batch), lr: 0.372009
2019-12-27 13:21:45.882100: step 36540/136300 (epoch 27/100), loss = 0.210282 (0.233 sec/batch), lr: 0.372009
2019-12-27 13:21:50.782143: step 36560/136300 (epoch 27/100), loss = 0.663183 (0.243 sec/batch), lr: 0.372009
2019-12-27 13:21:55.615401: step 36580/136300 (epoch 27/100), loss = 0.345920 (0.235 sec/batch), lr: 0.372009
2019-12-27 13:22:00.342442: step 36600/136300 (epoch 27/100), loss = 0.213253 (0.247 sec/batch), lr: 0.372009
2019-12-27 13:22:05.066017: step 36620/136300 (epoch 27/100), loss = 0.386533 (0.246 sec/batch), lr: 0.372009
2019-12-27 13:22:11.337183: step 36640/136300 (epoch 27/100), loss = 0.228886 (0.240 sec/batch), lr: 0.372009
2019-12-27 13:22:16.202365: step 36660/136300 (epoch 27/100), loss = 0.390409 (0.236 sec/batch), lr: 0.372009
2019-12-27 13:22:21.101873: step 36680/136300 (epoch 27/100), loss = 0.162475 (0.241 sec/batch), lr: 0.372009
2019-12-27 13:22:25.881682: step 36700/136300 (epoch 27/100), loss = 0.395961 (0.241 sec/batch), lr: 0.372009
2019-12-27 13:22:30.560747: step 36720/136300 (epoch 27/100), loss = 0.289989 (0.230 sec/batch), lr: 0.372009
2019-12-27 13:22:35.303237: step 36740/136300 (epoch 27/100), loss = 0.164915 (0.211 sec/batch), lr: 0.372009
2019-12-27 13:22:40.105632: step 36760/136300 (epoch 27/100), loss = 0.314369 (0.243 sec/batch), lr: 0.372009
2019-12-27 13:22:44.982554: step 36780/136300 (epoch 27/100), loss = 0.294414 (0.243 sec/batch), lr: 0.372009
2019-12-27 13:22:49.857646: step 36800/136300 (epoch 27/100), loss = 0.410205 (0.202 sec/batch), lr: 0.372009
Evaluating on dev set...
Precision (micro): 72.343%
   Recall (micro): 55.096%
       F1 (micro): 62.552%
epoch 27: train_loss = 0.336964, dev_loss = 0.451225, dev_f1 = 0.6255
model saved to ./saved_models/01/checkpoint_epoch_27.pt
new best model saved.

2019-12-27 13:23:29.697604: step 36820/136300 (epoch 28/100), loss = 0.197360 (0.230 sec/batch), lr: 0.372009
2019-12-27 13:23:34.379003: step 36840/136300 (epoch 28/100), loss = 0.215826 (0.214 sec/batch), lr: 0.372009
2019-12-27 13:23:39.205410: step 36860/136300 (epoch 28/100), loss = 0.280265 (0.215 sec/batch), lr: 0.372009
2019-12-27 13:23:44.058762: step 36880/136300 (epoch 28/100), loss = 0.206691 (0.237 sec/batch), lr: 0.372009
2019-12-27 13:23:48.858629: step 36900/136300 (epoch 28/100), loss = 0.146165 (0.242 sec/batch), lr: 0.372009
2019-12-27 13:23:53.622591: step 36920/136300 (epoch 28/100), loss = 0.276036 (0.225 sec/batch), lr: 0.372009
2019-12-27 13:23:58.417144: step 36940/136300 (epoch 28/100), loss = 0.432160 (0.217 sec/batch), lr: 0.372009
2019-12-27 13:24:03.234014: step 36960/136300 (epoch 28/100), loss = 0.240385 (0.231 sec/batch), lr: 0.372009
2019-12-27 13:24:08.136941: step 36980/136300 (epoch 28/100), loss = 0.189598 (0.241 sec/batch), lr: 0.372009
2019-12-27 13:24:12.983274: step 37000/136300 (epoch 28/100), loss = 0.509572 (0.221 sec/batch), lr: 0.372009
2019-12-27 13:24:19.185992: step 37020/136300 (epoch 28/100), loss = 0.474181 (0.227 sec/batch), lr: 0.372009
2019-12-27 13:24:23.921433: step 37040/136300 (epoch 28/100), loss = 0.425013 (0.244 sec/batch), lr: 0.372009
2019-12-27 13:24:28.702588: step 37060/136300 (epoch 28/100), loss = 0.399447 (0.237 sec/batch), lr: 0.372009
2019-12-27 13:24:33.542942: step 37080/136300 (epoch 28/100), loss = 0.256024 (0.231 sec/batch), lr: 0.372009
2019-12-27 13:24:38.298813: step 37100/136300 (epoch 28/100), loss = 0.382664 (0.211 sec/batch), lr: 0.372009
2019-12-27 13:24:43.036255: step 37120/136300 (epoch 28/100), loss = 0.369289 (0.240 sec/batch), lr: 0.372009
2019-12-27 13:24:47.804892: step 37140/136300 (epoch 28/100), loss = 0.390548 (0.197 sec/batch), lr: 0.372009
2019-12-27 13:24:52.633566: step 37160/136300 (epoch 28/100), loss = 0.280565 (0.241 sec/batch), lr: 0.372009
2019-12-27 13:24:57.337634: step 37180/136300 (epoch 28/100), loss = 0.316067 (0.242 sec/batch), lr: 0.372009
2019-12-27 13:25:02.050222: step 37200/136300 (epoch 28/100), loss = 0.303454 (0.232 sec/batch), lr: 0.372009
2019-12-27 13:25:08.078785: step 37220/136300 (epoch 28/100), loss = 0.397800 (0.239 sec/batch), lr: 0.372009
2019-12-27 13:25:12.888056: step 37240/136300 (epoch 28/100), loss = 0.358568 (0.234 sec/batch), lr: 0.372009
2019-12-27 13:25:17.704686: step 37260/136300 (epoch 28/100), loss = 0.158564 (0.204 sec/batch), lr: 0.372009
2019-12-27 13:25:22.513775: step 37280/136300 (epoch 28/100), loss = 0.315106 (0.237 sec/batch), lr: 0.372009
2019-12-27 13:25:27.280456: step 37300/136300 (epoch 28/100), loss = 0.167204 (0.227 sec/batch), lr: 0.372009
2019-12-27 13:25:32.135918: step 37320/136300 (epoch 28/100), loss = 0.274917 (0.230 sec/batch), lr: 0.372009
2019-12-27 13:25:37.027293: step 37340/136300 (epoch 28/100), loss = 0.313677 (0.211 sec/batch), lr: 0.372009
2019-12-27 13:25:41.817562: step 37360/136300 (epoch 28/100), loss = 0.294032 (0.242 sec/batch), lr: 0.372009
2019-12-27 13:25:46.615194: step 37380/136300 (epoch 28/100), loss = 0.290249 (0.234 sec/batch), lr: 0.372009
2019-12-27 13:25:51.293791: step 37400/136300 (epoch 28/100), loss = 0.440809 (0.222 sec/batch), lr: 0.372009
2019-12-27 13:25:57.268109: step 37420/136300 (epoch 28/100), loss = 0.297879 (0.240 sec/batch), lr: 0.372009
2019-12-27 13:26:02.087465: step 37440/136300 (epoch 28/100), loss = 0.534783 (0.226 sec/batch), lr: 0.372009
2019-12-27 13:26:06.795751: step 37460/136300 (epoch 28/100), loss = 0.288688 (0.224 sec/batch), lr: 0.372009
2019-12-27 13:26:11.617865: step 37480/136300 (epoch 28/100), loss = 0.327190 (0.239 sec/batch), lr: 0.372009
2019-12-27 13:26:16.503200: step 37500/136300 (epoch 28/100), loss = 0.218611 (0.228 sec/batch), lr: 0.372009
2019-12-27 13:26:21.370669: step 37520/136300 (epoch 28/100), loss = 0.191699 (0.238 sec/batch), lr: 0.372009
2019-12-27 13:26:26.263546: step 37540/136300 (epoch 28/100), loss = 0.517128 (0.247 sec/batch), lr: 0.372009
2019-12-27 13:26:30.968690: step 37560/136300 (epoch 28/100), loss = 0.264426 (0.212 sec/batch), lr: 0.372009
2019-12-27 13:26:35.727287: step 37580/136300 (epoch 28/100), loss = 0.366120 (0.173 sec/batch), lr: 0.372009
2019-12-27 13:26:42.004955: step 37600/136300 (epoch 28/100), loss = 0.315240 (0.243 sec/batch), lr: 0.372009
2019-12-27 13:26:46.732088: step 37620/136300 (epoch 28/100), loss = 0.366747 (0.218 sec/batch), lr: 0.372009
2019-12-27 13:26:51.574855: step 37640/136300 (epoch 28/100), loss = 0.342929 (0.236 sec/batch), lr: 0.372009
2019-12-27 13:26:56.436589: step 37660/136300 (epoch 28/100), loss = 0.468507 (0.209 sec/batch), lr: 0.372009
2019-12-27 13:27:01.126391: step 37680/136300 (epoch 28/100), loss = 0.468916 (0.236 sec/batch), lr: 0.372009
2019-12-27 13:27:05.957389: step 37700/136300 (epoch 28/100), loss = 0.305666 (0.210 sec/batch), lr: 0.372009
2019-12-27 13:27:10.843640: step 37720/136300 (epoch 28/100), loss = 0.318364 (0.218 sec/batch), lr: 0.372009
2019-12-27 13:27:15.653504: step 37740/136300 (epoch 28/100), loss = 0.557277 (0.243 sec/batch), lr: 0.372009
2019-12-27 13:27:20.584426: step 37760/136300 (epoch 28/100), loss = 0.503914 (0.230 sec/batch), lr: 0.372009
2019-12-27 13:27:25.444261: step 37780/136300 (epoch 28/100), loss = 0.157551 (0.244 sec/batch), lr: 0.372009
2019-12-27 13:27:30.208566: step 37800/136300 (epoch 28/100), loss = 0.500226 (0.234 sec/batch), lr: 0.372009
2019-12-27 13:27:36.336063: step 37820/136300 (epoch 28/100), loss = 0.141842 (0.224 sec/batch), lr: 0.372009
2019-12-27 13:27:41.084249: step 37840/136300 (epoch 28/100), loss = 0.562379 (0.201 sec/batch), lr: 0.372009
2019-12-27 13:27:45.771364: step 37860/136300 (epoch 28/100), loss = 0.532329 (0.215 sec/batch), lr: 0.372009
2019-12-27 13:27:50.534999: step 37880/136300 (epoch 28/100), loss = 0.444424 (0.236 sec/batch), lr: 0.372009
2019-12-27 13:27:55.279742: step 37900/136300 (epoch 28/100), loss = 0.383977 (0.238 sec/batch), lr: 0.372009
2019-12-27 13:28:00.178901: step 37920/136300 (epoch 28/100), loss = 0.281491 (0.239 sec/batch), lr: 0.372009
2019-12-27 13:28:04.985479: step 37940/136300 (epoch 28/100), loss = 0.237077 (0.238 sec/batch), lr: 0.372009
2019-12-27 13:28:09.710394: step 37960/136300 (epoch 28/100), loss = 0.459963 (0.233 sec/batch), lr: 0.372009
2019-12-27 13:28:14.395670: step 37980/136300 (epoch 28/100), loss = 0.241448 (0.233 sec/batch), lr: 0.372009
2019-12-27 13:28:19.199328: step 38000/136300 (epoch 28/100), loss = 0.364498 (0.230 sec/batch), lr: 0.372009
2019-12-27 13:28:25.478891: step 38020/136300 (epoch 28/100), loss = 0.289072 (0.237 sec/batch), lr: 0.372009
2019-12-27 13:28:30.364995: step 38040/136300 (epoch 28/100), loss = 0.334996 (0.243 sec/batch), lr: 0.372009
2019-12-27 13:28:35.181237: step 38060/136300 (epoch 28/100), loss = 0.411519 (0.219 sec/batch), lr: 0.372009
2019-12-27 13:28:39.854307: step 38080/136300 (epoch 28/100), loss = 0.267080 (0.211 sec/batch), lr: 0.372009
2019-12-27 13:28:44.679711: step 38100/136300 (epoch 28/100), loss = 0.254635 (0.235 sec/batch), lr: 0.372009
2019-12-27 13:28:49.422016: step 38120/136300 (epoch 28/100), loss = 0.352412 (0.238 sec/batch), lr: 0.372009
2019-12-27 13:28:54.289143: step 38140/136300 (epoch 28/100), loss = 0.290845 (0.198 sec/batch), lr: 0.372009
2019-12-27 13:28:59.217269: step 38160/136300 (epoch 28/100), loss = 0.452634 (0.239 sec/batch), lr: 0.372009
Evaluating on dev set...
Precision (micro): 72.714%
   Recall (micro): 55.298%
       F1 (micro): 62.821%
epoch 28: train_loss = 0.332051, dev_loss = 0.449453, dev_f1 = 0.6282
model saved to ./saved_models/01/checkpoint_epoch_28.pt
new best model saved.

2019-12-27 13:29:38.160226: step 38180/136300 (epoch 29/100), loss = 0.367528 (0.223 sec/batch), lr: 0.372009
2019-12-27 13:29:44.181397: step 38200/136300 (epoch 29/100), loss = 0.401233 (0.204 sec/batch), lr: 0.372009
2019-12-27 13:29:49.016613: step 38220/136300 (epoch 29/100), loss = 0.169430 (0.219 sec/batch), lr: 0.372009
2019-12-27 13:29:53.802264: step 38240/136300 (epoch 29/100), loss = 0.451264 (0.243 sec/batch), lr: 0.372009
2019-12-27 13:29:58.632337: step 38260/136300 (epoch 29/100), loss = 0.323922 (0.203 sec/batch), lr: 0.372009
2019-12-27 13:30:03.354237: step 38280/136300 (epoch 29/100), loss = 0.238733 (0.215 sec/batch), lr: 0.372009
2019-12-27 13:30:08.144667: step 38300/136300 (epoch 29/100), loss = 0.361706 (0.233 sec/batch), lr: 0.372009
2019-12-27 13:30:12.939740: step 38320/136300 (epoch 29/100), loss = 0.348282 (0.225 sec/batch), lr: 0.372009
2019-12-27 13:30:17.785992: step 38340/136300 (epoch 29/100), loss = 0.349849 (0.236 sec/batch), lr: 0.372009
2019-12-27 13:30:22.637042: step 38360/136300 (epoch 29/100), loss = 0.238698 (0.247 sec/batch), lr: 0.372009
2019-12-27 13:30:28.935023: step 38380/136300 (epoch 29/100), loss = 0.268335 (0.226 sec/batch), lr: 0.372009
2019-12-27 13:30:33.641780: step 38400/136300 (epoch 29/100), loss = 0.358652 (0.225 sec/batch), lr: 0.372009
2019-12-27 13:30:38.411364: step 38420/136300 (epoch 29/100), loss = 0.336309 (0.200 sec/batch), lr: 0.372009
2019-12-27 13:30:43.253715: step 38440/136300 (epoch 29/100), loss = 0.353234 (0.204 sec/batch), lr: 0.372009
2019-12-27 13:30:48.074767: step 38460/136300 (epoch 29/100), loss = 0.430076 (0.232 sec/batch), lr: 0.372009
2019-12-27 13:30:52.748997: step 38480/136300 (epoch 29/100), loss = 0.330902 (0.216 sec/batch), lr: 0.372009
2019-12-27 13:30:57.551230: step 38500/136300 (epoch 29/100), loss = 0.274474 (0.202 sec/batch), lr: 0.372009
2019-12-27 13:31:02.373481: step 38520/136300 (epoch 29/100), loss = 0.550604 (0.231 sec/batch), lr: 0.372009
2019-12-27 13:31:07.083666: step 38540/136300 (epoch 29/100), loss = 0.149609 (0.234 sec/batch), lr: 0.372009
2019-12-27 13:31:11.837573: step 38560/136300 (epoch 29/100), loss = 0.619039 (0.233 sec/batch), lr: 0.372009
2019-12-27 13:31:17.962796: step 38580/136300 (epoch 29/100), loss = 0.244871 (0.229 sec/batch), lr: 0.372009
2019-12-27 13:31:22.765600: step 38600/136300 (epoch 29/100), loss = 0.398570 (0.247 sec/batch), lr: 0.372009
2019-12-27 13:31:27.609521: step 38620/136300 (epoch 29/100), loss = 0.209760 (0.205 sec/batch), lr: 0.372009
2019-12-27 13:31:32.418360: step 38640/136300 (epoch 29/100), loss = 0.229134 (0.231 sec/batch), lr: 0.372009
2019-12-27 13:31:37.225962: step 38660/136300 (epoch 29/100), loss = 0.138740 (0.214 sec/batch), lr: 0.372009
2019-12-27 13:31:42.089510: step 38680/136300 (epoch 29/100), loss = 0.295307 (0.212 sec/batch), lr: 0.372009
2019-12-27 13:31:46.978972: step 38700/136300 (epoch 29/100), loss = 0.307010 (0.241 sec/batch), lr: 0.372009
2019-12-27 13:31:51.755566: step 38720/136300 (epoch 29/100), loss = 0.551071 (0.231 sec/batch), lr: 0.372009
2019-12-27 13:31:56.559808: step 38740/136300 (epoch 29/100), loss = 0.534115 (0.205 sec/batch), lr: 0.372009
2019-12-27 13:32:01.266798: step 38760/136300 (epoch 29/100), loss = 0.372364 (0.204 sec/batch), lr: 0.372009
2019-12-27 13:32:07.515214: step 38780/136300 (epoch 29/100), loss = 0.200992 (0.226 sec/batch), lr: 0.372009
2019-12-27 13:32:12.273325: step 38800/136300 (epoch 29/100), loss = 0.231049 (0.213 sec/batch), lr: 0.372009
2019-12-27 13:32:16.975940: step 38820/136300 (epoch 29/100), loss = 0.375142 (0.211 sec/batch), lr: 0.372009
2019-12-27 13:32:21.761648: step 38840/136300 (epoch 29/100), loss = 0.280117 (0.210 sec/batch), lr: 0.372009
2019-12-27 13:32:26.686184: step 38860/136300 (epoch 29/100), loss = 0.318122 (0.238 sec/batch), lr: 0.372009
2019-12-27 13:32:31.509955: step 38880/136300 (epoch 29/100), loss = 0.337005 (0.242 sec/batch), lr: 0.372009
2019-12-27 13:32:36.368595: step 38900/136300 (epoch 29/100), loss = 0.290937 (0.204 sec/batch), lr: 0.372009
2019-12-27 13:32:41.116242: step 38920/136300 (epoch 29/100), loss = 0.442216 (0.180 sec/batch), lr: 0.372009
2019-12-27 13:32:45.903394: step 38940/136300 (epoch 29/100), loss = 0.206505 (0.226 sec/batch), lr: 0.372009
2019-12-27 13:32:50.768688: step 38960/136300 (epoch 29/100), loss = 0.321617 (0.228 sec/batch), lr: 0.372009
2019-12-27 13:32:56.934484: step 38980/136300 (epoch 29/100), loss = 0.434001 (0.243 sec/batch), lr: 0.372009
2019-12-27 13:33:01.658731: step 39000/136300 (epoch 29/100), loss = 0.322522 (0.216 sec/batch), lr: 0.372009
2019-12-27 13:33:06.584840: step 39020/136300 (epoch 29/100), loss = 0.419612 (0.245 sec/batch), lr: 0.372009
2019-12-27 13:33:11.254031: step 39040/136300 (epoch 29/100), loss = 0.346596 (0.240 sec/batch), lr: 0.372009
2019-12-27 13:33:16.085206: step 39060/136300 (epoch 29/100), loss = 0.238961 (0.229 sec/batch), lr: 0.372009
2019-12-27 13:33:20.969638: step 39080/136300 (epoch 29/100), loss = 0.329333 (0.201 sec/batch), lr: 0.372009
2019-12-27 13:33:25.725769: step 39100/136300 (epoch 29/100), loss = 0.247609 (0.234 sec/batch), lr: 0.372009
2019-12-27 13:33:30.654006: step 39120/136300 (epoch 29/100), loss = 0.287623 (0.221 sec/batch), lr: 0.372009
2019-12-27 13:33:35.513888: step 39140/136300 (epoch 29/100), loss = 0.391587 (0.238 sec/batch), lr: 0.372009
2019-12-27 13:33:40.293239: step 39160/136300 (epoch 29/100), loss = 0.412636 (0.228 sec/batch), lr: 0.372009
2019-12-27 13:33:46.666288: step 39180/136300 (epoch 29/100), loss = 0.269385 (0.230 sec/batch), lr: 0.372009
2019-12-27 13:33:51.466763: step 39200/136300 (epoch 29/100), loss = 0.297541 (0.200 sec/batch), lr: 0.372009
2019-12-27 13:33:56.091119: step 39220/136300 (epoch 29/100), loss = 0.273943 (0.215 sec/batch), lr: 0.372009
2019-12-27 13:34:00.819938: step 39240/136300 (epoch 29/100), loss = 0.461149 (0.233 sec/batch), lr: 0.372009
2019-12-27 13:34:05.608183: step 39260/136300 (epoch 29/100), loss = 0.324499 (0.232 sec/batch), lr: 0.372009
2019-12-27 13:34:10.389936: step 39280/136300 (epoch 29/100), loss = 0.135184 (0.235 sec/batch), lr: 0.372009
2019-12-27 13:34:15.240202: step 39300/136300 (epoch 29/100), loss = 0.182656 (0.228 sec/batch), lr: 0.372009
2019-12-27 13:34:19.955617: step 39320/136300 (epoch 29/100), loss = 0.334959 (0.230 sec/batch), lr: 0.372009
2019-12-27 13:34:24.587179: step 39340/136300 (epoch 29/100), loss = 0.347260 (0.213 sec/batch), lr: 0.372009
2019-12-27 13:34:29.418760: step 39360/136300 (epoch 29/100), loss = 0.281444 (0.230 sec/batch), lr: 0.372009
2019-12-27 13:34:35.573926: step 39380/136300 (epoch 29/100), loss = 0.276132 (0.225 sec/batch), lr: 0.372009
2019-12-27 13:34:40.446095: step 39400/136300 (epoch 29/100), loss = 0.436868 (0.211 sec/batch), lr: 0.372009
2019-12-27 13:34:45.282692: step 39420/136300 (epoch 29/100), loss = 0.315776 (0.203 sec/batch), lr: 0.372009
2019-12-27 13:34:49.921906: step 39440/136300 (epoch 29/100), loss = 0.262587 (0.243 sec/batch), lr: 0.372009
2019-12-27 13:34:54.736992: step 39460/136300 (epoch 29/100), loss = 0.355231 (0.242 sec/batch), lr: 0.372009
2019-12-27 13:34:59.471271: step 39480/136300 (epoch 29/100), loss = 0.190401 (0.199 sec/batch), lr: 0.372009
2019-12-27 13:35:04.369792: step 39500/136300 (epoch 29/100), loss = 0.380739 (0.229 sec/batch), lr: 0.372009
2019-12-27 13:35:09.212123: step 39520/136300 (epoch 29/100), loss = 0.285360 (0.241 sec/batch), lr: 0.372009
Evaluating on dev set...
Precision (micro): 68.447%
   Recall (micro): 59.658%
       F1 (micro): 63.751%
epoch 29: train_loss = 0.331044, dev_loss = 0.446687, dev_f1 = 0.6375
model saved to ./saved_models/01/checkpoint_epoch_29.pt
new best model saved.

2019-12-27 13:35:47.412974: step 39540/136300 (epoch 30/100), loss = 0.353498 (0.236 sec/batch), lr: 0.372009
2019-12-27 13:35:53.395358: step 39560/136300 (epoch 30/100), loss = 0.492054 (0.248 sec/batch), lr: 0.372009
2019-12-27 13:35:58.189910: step 39580/136300 (epoch 30/100), loss = 0.232631 (0.241 sec/batch), lr: 0.372009
2019-12-27 13:36:02.866999: step 39600/136300 (epoch 30/100), loss = 0.312950 (0.219 sec/batch), lr: 0.372009
2019-12-27 13:36:07.767731: step 39620/136300 (epoch 30/100), loss = 0.375931 (0.236 sec/batch), lr: 0.372009
2019-12-27 13:36:12.465082: step 39640/136300 (epoch 30/100), loss = 0.266279 (0.217 sec/batch), lr: 0.372009
2019-12-27 13:36:17.245552: step 39660/136300 (epoch 30/100), loss = 0.443600 (0.235 sec/batch), lr: 0.372009
2019-12-27 13:36:21.979974: step 39680/136300 (epoch 30/100), loss = 0.336952 (0.181 sec/batch), lr: 0.372009
2019-12-27 13:36:26.830071: step 39700/136300 (epoch 30/100), loss = 0.205390 (0.231 sec/batch), lr: 0.372009
2019-12-27 13:36:31.683471: step 39720/136300 (epoch 30/100), loss = 0.363446 (0.202 sec/batch), lr: 0.372009
2019-12-27 13:36:37.901388: step 39740/136300 (epoch 30/100), loss = 0.213216 (1.637 sec/batch), lr: 0.372009
2019-12-27 13:36:42.563848: step 39760/136300 (epoch 30/100), loss = 0.444455 (0.179 sec/batch), lr: 0.372009
2019-12-27 13:36:47.327567: step 39780/136300 (epoch 30/100), loss = 0.281529 (0.239 sec/batch), lr: 0.372009
2019-12-27 13:36:52.190255: step 39800/136300 (epoch 30/100), loss = 0.499974 (0.230 sec/batch), lr: 0.372009
2019-12-27 13:36:56.973707: step 39820/136300 (epoch 30/100), loss = 0.275322 (0.239 sec/batch), lr: 0.372009
2019-12-27 13:37:01.646033: step 39840/136300 (epoch 30/100), loss = 0.496834 (0.198 sec/batch), lr: 0.372009
2019-12-27 13:37:06.418571: step 39860/136300 (epoch 30/100), loss = 0.259366 (0.233 sec/batch), lr: 0.372009
2019-12-27 13:37:11.221942: step 39880/136300 (epoch 30/100), loss = 0.144937 (0.244 sec/batch), lr: 0.372009
2019-12-27 13:37:15.929293: step 39900/136300 (epoch 30/100), loss = 0.289441 (0.179 sec/batch), lr: 0.372009
2019-12-27 13:37:20.655317: step 39920/136300 (epoch 30/100), loss = 0.322156 (0.208 sec/batch), lr: 0.372009
2019-12-27 13:37:26.600763: step 39940/136300 (epoch 30/100), loss = 0.225550 (0.232 sec/batch), lr: 0.372009
2019-12-27 13:37:31.400614: step 39960/136300 (epoch 30/100), loss = 0.425437 (0.238 sec/batch), lr: 0.372009
2019-12-27 13:37:36.247182: step 39980/136300 (epoch 30/100), loss = 0.227208 (0.212 sec/batch), lr: 0.372009
2019-12-27 13:37:41.045109: step 40000/136300 (epoch 30/100), loss = 0.300598 (0.200 sec/batch), lr: 0.372009
2019-12-27 13:37:45.850910: step 40020/136300 (epoch 30/100), loss = 0.224613 (0.226 sec/batch), lr: 0.372009
2019-12-27 13:37:50.706317: step 40040/136300 (epoch 30/100), loss = 0.417805 (0.245 sec/batch), lr: 0.372009
2019-12-27 13:37:55.532185: step 40060/136300 (epoch 30/100), loss = 0.447948 (0.224 sec/batch), lr: 0.372009
2019-12-27 13:38:00.348394: step 40080/136300 (epoch 30/100), loss = 0.312535 (0.214 sec/batch), lr: 0.372009
2019-12-27 13:38:05.142530: step 40100/136300 (epoch 30/100), loss = 0.284336 (0.236 sec/batch), lr: 0.372009
2019-12-27 13:38:09.909217: step 40120/136300 (epoch 30/100), loss = 0.667000 (0.229 sec/batch), lr: 0.372009
2019-12-27 13:38:15.877795: step 40140/136300 (epoch 30/100), loss = 0.287846 (0.216 sec/batch), lr: 0.372009
2019-12-27 13:38:20.636499: step 40160/136300 (epoch 30/100), loss = 0.304346 (0.216 sec/batch), lr: 0.372009
2019-12-27 13:38:25.362783: step 40180/136300 (epoch 30/100), loss = 0.336999 (0.235 sec/batch), lr: 0.372009
2019-12-27 13:38:30.154483: step 40200/136300 (epoch 30/100), loss = 0.257204 (0.238 sec/batch), lr: 0.372009
2019-12-27 13:38:35.032489: step 40220/136300 (epoch 30/100), loss = 0.431474 (0.221 sec/batch), lr: 0.372009
2019-12-27 13:38:39.877753: step 40240/136300 (epoch 30/100), loss = 0.352556 (0.230 sec/batch), lr: 0.372009
2019-12-27 13:38:44.768387: step 40260/136300 (epoch 30/100), loss = 0.380593 (0.237 sec/batch), lr: 0.372009
2019-12-27 13:38:49.568269: step 40280/136300 (epoch 30/100), loss = 0.490407 (0.227 sec/batch), lr: 0.372009
2019-12-27 13:38:54.281224: step 40300/136300 (epoch 30/100), loss = 0.389829 (0.213 sec/batch), lr: 0.372009
2019-12-27 13:38:59.180899: step 40320/136300 (epoch 30/100), loss = 0.246130 (0.246 sec/batch), lr: 0.372009
2019-12-27 13:39:05.237741: step 40340/136300 (epoch 30/100), loss = 0.281202 (0.217 sec/batch), lr: 0.372009
2019-12-27 13:39:09.988559: step 40360/136300 (epoch 30/100), loss = 0.468123 (0.220 sec/batch), lr: 0.372009
2019-12-27 13:39:14.871241: step 40380/136300 (epoch 30/100), loss = 0.496224 (0.211 sec/batch), lr: 0.372009
2019-12-27 13:39:19.505902: step 40400/136300 (epoch 30/100), loss = 0.278928 (0.228 sec/batch), lr: 0.372009
2019-12-27 13:39:24.365658: step 40420/136300 (epoch 30/100), loss = 0.206048 (0.216 sec/batch), lr: 0.372009
2019-12-27 13:39:29.271402: step 40440/136300 (epoch 30/100), loss = 0.249946 (0.229 sec/batch), lr: 0.372009
2019-12-27 13:39:34.033269: step 40460/136300 (epoch 30/100), loss = 0.327232 (0.225 sec/batch), lr: 0.372009
2019-12-27 13:39:38.949496: step 40480/136300 (epoch 30/100), loss = 0.396830 (0.231 sec/batch), lr: 0.372009
2019-12-27 13:39:43.793877: step 40500/136300 (epoch 30/100), loss = 0.425068 (0.205 sec/batch), lr: 0.372009
2019-12-27 13:39:48.606115: step 40520/136300 (epoch 30/100), loss = 0.291064 (0.226 sec/batch), lr: 0.372009
2019-12-27 13:39:54.996441: step 40540/136300 (epoch 30/100), loss = 0.179143 (0.238 sec/batch), lr: 0.372009
2019-12-27 13:39:59.785265: step 40560/136300 (epoch 30/100), loss = 0.345099 (0.178 sec/batch), lr: 0.372009
2019-12-27 13:40:04.431138: step 40580/136300 (epoch 30/100), loss = 0.148716 (0.233 sec/batch), lr: 0.372009
2019-12-27 13:40:09.120160: step 40600/136300 (epoch 30/100), loss = 0.310659 (0.217 sec/batch), lr: 0.372009
2019-12-27 13:40:13.934942: step 40620/136300 (epoch 30/100), loss = 0.313118 (0.239 sec/batch), lr: 0.372009
2019-12-27 13:40:18.698866: step 40640/136300 (epoch 30/100), loss = 0.371072 (0.212 sec/batch), lr: 0.372009
2019-12-27 13:40:23.555495: step 40660/136300 (epoch 30/100), loss = 0.478593 (0.203 sec/batch), lr: 0.372009
2019-12-27 13:40:28.261734: step 40680/136300 (epoch 30/100), loss = 0.260456 (0.213 sec/batch), lr: 0.372009
2019-12-27 13:40:32.945203: step 40700/136300 (epoch 30/100), loss = 0.310173 (0.245 sec/batch), lr: 0.372009
2019-12-27 13:40:37.728382: step 40720/136300 (epoch 30/100), loss = 0.447053 (0.232 sec/batch), lr: 0.372009
2019-12-27 13:40:43.956421: step 40740/136300 (epoch 30/100), loss = 0.373355 (0.240 sec/batch), lr: 0.372009
2019-12-27 13:40:48.830790: step 40760/136300 (epoch 30/100), loss = 0.311969 (0.247 sec/batch), lr: 0.372009
2019-12-27 13:40:53.690709: step 40780/136300 (epoch 30/100), loss = 0.144954 (0.203 sec/batch), lr: 0.372009
2019-12-27 13:40:58.298771: step 40800/136300 (epoch 30/100), loss = 0.356481 (0.242 sec/batch), lr: 0.372009
2019-12-27 13:41:03.072553: step 40820/136300 (epoch 30/100), loss = 0.456045 (0.209 sec/batch), lr: 0.372009
2019-12-27 13:41:07.845972: step 40840/136300 (epoch 30/100), loss = 0.181781 (0.239 sec/batch), lr: 0.372009
2019-12-27 13:41:12.721814: step 40860/136300 (epoch 30/100), loss = 0.286171 (0.236 sec/batch), lr: 0.372009
2019-12-27 13:41:17.548326: step 40880/136300 (epoch 30/100), loss = 0.386670 (0.244 sec/batch), lr: 0.372009
Evaluating on dev set...
Precision (micro): 73.054%
   Recall (micro): 53.863%
       F1 (micro): 62.008%
epoch 30: train_loss = 0.329791, dev_loss = 0.446386, dev_f1 = 0.6201
model saved to ./saved_models/01/checkpoint_epoch_30.pt

2019-12-27 13:41:55.653907: step 40900/136300 (epoch 31/100), loss = 0.289326 (0.175 sec/batch), lr: 0.334808
2019-12-27 13:42:01.850611: step 40920/136300 (epoch 31/100), loss = 0.373724 (0.242 sec/batch), lr: 0.334808
2019-12-27 13:42:06.613428: step 40940/136300 (epoch 31/100), loss = 0.284550 (0.213 sec/batch), lr: 0.334808
2019-12-27 13:42:11.297600: step 40960/136300 (epoch 31/100), loss = 0.301026 (0.231 sec/batch), lr: 0.334808
2019-12-27 13:42:16.209910: step 40980/136300 (epoch 31/100), loss = 0.237227 (0.235 sec/batch), lr: 0.334808
2019-12-27 13:42:20.961677: step 41000/136300 (epoch 31/100), loss = 0.222792 (0.194 sec/batch), lr: 0.334808
2019-12-27 13:42:25.692609: step 41020/136300 (epoch 31/100), loss = 0.396253 (0.248 sec/batch), lr: 0.334808
2019-12-27 13:42:30.470993: step 41040/136300 (epoch 31/100), loss = 0.306470 (0.163 sec/batch), lr: 0.334808
2019-12-27 13:42:35.306673: step 41060/136300 (epoch 31/100), loss = 0.404523 (0.239 sec/batch), lr: 0.334808
2019-12-27 13:42:40.160582: step 41080/136300 (epoch 31/100), loss = 0.408094 (0.221 sec/batch), lr: 0.334808
2019-12-27 13:42:44.914560: step 41100/136300 (epoch 31/100), loss = 0.461357 (0.236 sec/batch), lr: 0.334808
2019-12-27 13:42:51.115290: step 41120/136300 (epoch 31/100), loss = 0.294515 (0.239 sec/batch), lr: 0.334808
2019-12-27 13:42:55.849447: step 41140/136300 (epoch 31/100), loss = 0.262319 (0.174 sec/batch), lr: 0.334808
2019-12-27 13:43:00.695882: step 41160/136300 (epoch 31/100), loss = 0.413974 (0.208 sec/batch), lr: 0.334808
2019-12-27 13:43:05.477789: step 41180/136300 (epoch 31/100), loss = 0.355466 (0.244 sec/batch), lr: 0.334808
2019-12-27 13:43:10.175315: step 41200/136300 (epoch 31/100), loss = 0.341158 (0.237 sec/batch), lr: 0.334808
2019-12-27 13:43:14.911393: step 41220/136300 (epoch 31/100), loss = 0.355641 (0.204 sec/batch), lr: 0.334808
2019-12-27 13:43:19.712296: step 41240/136300 (epoch 31/100), loss = 0.252716 (0.226 sec/batch), lr: 0.334808
2019-12-27 13:43:24.525547: step 41260/136300 (epoch 31/100), loss = 0.231506 (0.180 sec/batch), lr: 0.334808
2019-12-27 13:43:29.199653: step 41280/136300 (epoch 31/100), loss = 0.420905 (0.203 sec/batch), lr: 0.334808
2019-12-27 13:43:35.303930: step 41300/136300 (epoch 31/100), loss = 0.182173 (0.210 sec/batch), lr: 0.334808
2019-12-27 13:43:40.041649: step 41320/136300 (epoch 31/100), loss = 0.194259 (0.232 sec/batch), lr: 0.334808
2019-12-27 13:43:44.943037: step 41340/136300 (epoch 31/100), loss = 0.268233 (0.227 sec/batch), lr: 0.334808
2019-12-27 13:43:49.778854: step 41360/136300 (epoch 31/100), loss = 0.222026 (0.244 sec/batch), lr: 0.334808
2019-12-27 13:43:54.500141: step 41380/136300 (epoch 31/100), loss = 0.453187 (0.224 sec/batch), lr: 0.334808
2019-12-27 13:43:59.353447: step 41400/136300 (epoch 31/100), loss = 0.281756 (0.237 sec/batch), lr: 0.334808
2019-12-27 13:44:04.178812: step 41420/136300 (epoch 31/100), loss = 0.353396 (0.240 sec/batch), lr: 0.334808
2019-12-27 13:44:09.002964: step 41440/136300 (epoch 31/100), loss = 0.459798 (0.237 sec/batch), lr: 0.334808
2019-12-27 13:44:13.777665: step 41460/136300 (epoch 31/100), loss = 0.340177 (0.204 sec/batch), lr: 0.334808
2019-12-27 13:44:18.573958: step 41480/136300 (epoch 31/100), loss = 0.243353 (0.241 sec/batch), lr: 0.334808
2019-12-27 13:44:24.627430: step 41500/136300 (epoch 31/100), loss = 0.355443 (0.247 sec/batch), lr: 0.334808
2019-12-27 13:44:29.401880: step 41520/136300 (epoch 31/100), loss = 0.294346 (0.239 sec/batch), lr: 0.334808
2019-12-27 13:44:34.087890: step 41540/136300 (epoch 31/100), loss = 0.196022 (0.219 sec/batch), lr: 0.334808
2019-12-27 13:44:38.895776: step 41560/136300 (epoch 31/100), loss = 0.199712 (0.229 sec/batch), lr: 0.334808
2019-12-27 13:44:43.755516: step 41580/136300 (epoch 31/100), loss = 0.207733 (0.229 sec/batch), lr: 0.334808
2019-12-27 13:44:48.600997: step 41600/136300 (epoch 31/100), loss = 0.670904 (0.213 sec/batch), lr: 0.334808
2019-12-27 13:44:53.467474: step 41620/136300 (epoch 31/100), loss = 0.303682 (0.208 sec/batch), lr: 0.334808
2019-12-27 13:44:58.310500: step 41640/136300 (epoch 31/100), loss = 0.342860 (0.199 sec/batch), lr: 0.334808
2019-12-27 13:45:02.989116: step 41660/136300 (epoch 31/100), loss = 0.094811 (0.219 sec/batch), lr: 0.334808
2019-12-27 13:45:07.849786: step 41680/136300 (epoch 31/100), loss = 0.263042 (0.225 sec/batch), lr: 0.334808
2019-12-27 13:45:14.135131: step 41700/136300 (epoch 31/100), loss = 0.279898 (0.235 sec/batch), lr: 0.334808
2019-12-27 13:45:18.902781: step 41720/136300 (epoch 31/100), loss = 0.275106 (0.234 sec/batch), lr: 0.334808
2019-12-27 13:45:23.806498: step 41740/136300 (epoch 31/100), loss = 0.321274 (0.230 sec/batch), lr: 0.334808
2019-12-27 13:45:28.424668: step 41760/136300 (epoch 31/100), loss = 0.341556 (0.219 sec/batch), lr: 0.334808
2019-12-27 13:45:33.249197: step 41780/136300 (epoch 31/100), loss = 0.225573 (0.237 sec/batch), lr: 0.334808
2019-12-27 13:45:38.120614: step 41800/136300 (epoch 31/100), loss = 0.356363 (0.225 sec/batch), lr: 0.334808
2019-12-27 13:45:42.892312: step 41820/136300 (epoch 31/100), loss = 0.337438 (0.237 sec/batch), lr: 0.334808
2019-12-27 13:45:47.756040: step 41840/136300 (epoch 31/100), loss = 0.162718 (0.241 sec/batch), lr: 0.334808
2019-12-27 13:45:52.643592: step 41860/136300 (epoch 31/100), loss = 0.478962 (0.218 sec/batch), lr: 0.334808
2019-12-27 13:45:57.409019: step 41880/136300 (epoch 31/100), loss = 0.176422 (0.238 sec/batch), lr: 0.334808
2019-12-27 13:46:03.582916: step 41900/136300 (epoch 31/100), loss = 0.230862 (0.241 sec/batch), lr: 0.334808
2019-12-27 13:46:08.415979: step 41920/136300 (epoch 31/100), loss = 0.215600 (0.192 sec/batch), lr: 0.334808
2019-12-27 13:46:13.006295: step 41940/136300 (epoch 31/100), loss = 0.244480 (0.217 sec/batch), lr: 0.334808
2019-12-27 13:46:17.681492: step 41960/136300 (epoch 31/100), loss = 0.376737 (0.208 sec/batch), lr: 0.334808
2019-12-27 13:46:22.498324: step 41980/136300 (epoch 31/100), loss = 0.321682 (0.214 sec/batch), lr: 0.334808
2019-12-27 13:46:27.290986: step 42000/136300 (epoch 31/100), loss = 0.277968 (0.250 sec/batch), lr: 0.334808
2019-12-27 13:46:32.128128: step 42020/136300 (epoch 31/100), loss = 0.473037 (0.231 sec/batch), lr: 0.334808
2019-12-27 13:46:36.847494: step 42040/136300 (epoch 31/100), loss = 0.247084 (0.225 sec/batch), lr: 0.334808
2019-12-27 13:46:41.508293: step 42060/136300 (epoch 31/100), loss = 0.327254 (0.231 sec/batch), lr: 0.334808
2019-12-27 13:46:46.244409: step 42080/136300 (epoch 31/100), loss = 0.295790 (0.230 sec/batch), lr: 0.334808
2019-12-27 13:46:52.460458: step 42100/136300 (epoch 31/100), loss = 0.132510 (0.238 sec/batch), lr: 0.334808
2019-12-27 13:46:57.348246: step 42120/136300 (epoch 31/100), loss = 0.339852 (0.230 sec/batch), lr: 0.334808
2019-12-27 13:47:02.224050: step 42140/136300 (epoch 31/100), loss = 0.329577 (0.239 sec/batch), lr: 0.334808
2019-12-27 13:47:06.839998: step 42160/136300 (epoch 31/100), loss = 0.276797 (0.212 sec/batch), lr: 0.334808
2019-12-27 13:47:11.585867: step 42180/136300 (epoch 31/100), loss = 0.224749 (0.178 sec/batch), lr: 0.334808
2019-12-27 13:47:16.319503: step 42200/136300 (epoch 31/100), loss = 0.313292 (0.215 sec/batch), lr: 0.334808
2019-12-27 13:47:21.213721: step 42220/136300 (epoch 31/100), loss = 0.329480 (0.237 sec/batch), lr: 0.334808
2019-12-27 13:47:25.992895: step 42240/136300 (epoch 31/100), loss = 0.126947 (0.205 sec/batch), lr: 0.334808
Evaluating on dev set...
Precision (micro): 74.775%
   Recall (micro): 55.132%
       F1 (micro): 63.469%
epoch 31: train_loss = 0.320921, dev_loss = 0.463702, dev_f1 = 0.6347
model saved to ./saved_models/01/checkpoint_epoch_31.pt

2019-12-27 13:48:04.069535: step 42260/136300 (epoch 32/100), loss = 0.673318 (0.229 sec/batch), lr: 0.334808
2019-12-27 13:48:10.264680: step 42280/136300 (epoch 32/100), loss = 0.194940 (0.227 sec/batch), lr: 0.334808
2019-12-27 13:48:15.079971: step 42300/136300 (epoch 32/100), loss = 0.389792 (0.243 sec/batch), lr: 0.334808
2019-12-27 13:48:19.780800: step 42320/136300 (epoch 32/100), loss = 0.191075 (0.240 sec/batch), lr: 0.334808
2019-12-27 13:48:24.640391: step 42340/136300 (epoch 32/100), loss = 0.291151 (0.214 sec/batch), lr: 0.334808
2019-12-27 13:48:29.433740: step 42360/136300 (epoch 32/100), loss = 0.200513 (0.228 sec/batch), lr: 0.334808
2019-12-27 13:48:34.130041: step 42380/136300 (epoch 32/100), loss = 0.265800 (0.230 sec/batch), lr: 0.334808
2019-12-27 13:48:38.969696: step 42400/136300 (epoch 32/100), loss = 0.183639 (0.244 sec/batch), lr: 0.334808
2019-12-27 13:48:43.737950: step 42420/136300 (epoch 32/100), loss = 0.313005 (0.220 sec/batch), lr: 0.334808
2019-12-27 13:48:48.593371: step 42440/136300 (epoch 32/100), loss = 0.220835 (0.229 sec/batch), lr: 0.334808
2019-12-27 13:48:53.364581: step 42460/136300 (epoch 32/100), loss = 0.232096 (0.236 sec/batch), lr: 0.334808
2019-12-27 13:48:59.400903: step 42480/136300 (epoch 32/100), loss = 0.433961 (0.213 sec/batch), lr: 0.334808
2019-12-27 13:49:04.221522: step 42500/136300 (epoch 32/100), loss = 0.200957 (0.210 sec/batch), lr: 0.334808
2019-12-27 13:49:08.998429: step 42520/136300 (epoch 32/100), loss = 0.212949 (0.240 sec/batch), lr: 0.334808
2019-12-27 13:49:13.757890: step 42540/136300 (epoch 32/100), loss = 0.393166 (0.240 sec/batch), lr: 0.334808
2019-12-27 13:49:18.548309: step 42560/136300 (epoch 32/100), loss = 0.306696 (0.215 sec/batch), lr: 0.334808
2019-12-27 13:49:23.280551: step 42580/136300 (epoch 32/100), loss = 0.369096 (0.245 sec/batch), lr: 0.334808
2019-12-27 13:49:28.055935: step 42600/136300 (epoch 32/100), loss = 0.191450 (0.238 sec/batch), lr: 0.334808
2019-12-27 13:49:32.884306: step 42620/136300 (epoch 32/100), loss = 0.449673 (0.177 sec/batch), lr: 0.334808
2019-12-27 13:49:37.639400: step 42640/136300 (epoch 32/100), loss = 0.188025 (0.240 sec/batch), lr: 0.334808
2019-12-27 13:49:42.225627: step 42660/136300 (epoch 32/100), loss = 0.169185 (0.232 sec/batch), lr: 0.334808
2019-12-27 13:49:48.525977: step 42680/136300 (epoch 32/100), loss = 0.266516 (0.242 sec/batch), lr: 0.334808
2019-12-27 13:49:53.430533: step 42700/136300 (epoch 32/100), loss = 0.169791 (0.236 sec/batch), lr: 0.334808
2019-12-27 13:49:58.290089: step 42720/136300 (epoch 32/100), loss = 0.380641 (0.231 sec/batch), lr: 0.334808
2019-12-27 13:50:03.009743: step 42740/136300 (epoch 32/100), loss = 0.204276 (0.227 sec/batch), lr: 0.334808
2019-12-27 13:50:07.819463: step 42760/136300 (epoch 32/100), loss = 0.151269 (0.226 sec/batch), lr: 0.334808
2019-12-27 13:50:12.637817: step 42780/136300 (epoch 32/100), loss = 0.307489 (0.230 sec/batch), lr: 0.334808
2019-12-27 13:50:17.554895: step 42800/136300 (epoch 32/100), loss = 0.272180 (0.239 sec/batch), lr: 0.334808
2019-12-27 13:50:22.311918: step 42820/136300 (epoch 32/100), loss = 0.416611 (0.242 sec/batch), lr: 0.334808
2019-12-27 13:50:27.017334: step 42840/136300 (epoch 32/100), loss = 0.194850 (0.233 sec/batch), lr: 0.334808
2019-12-27 13:50:33.036066: step 42860/136300 (epoch 32/100), loss = 0.425459 (1.543 sec/batch), lr: 0.334808
2019-12-27 13:50:37.732377: step 42880/136300 (epoch 32/100), loss = 0.339717 (0.217 sec/batch), lr: 0.334808
2019-12-27 13:50:42.461989: step 42900/136300 (epoch 32/100), loss = 0.327858 (0.206 sec/batch), lr: 0.334808
2019-12-27 13:50:47.235861: step 42920/136300 (epoch 32/100), loss = 0.337072 (0.238 sec/batch), lr: 0.334808
2019-12-27 13:50:52.093493: step 42940/136300 (epoch 32/100), loss = 0.276847 (0.238 sec/batch), lr: 0.334808
2019-12-27 13:50:56.973863: step 42960/136300 (epoch 32/100), loss = 0.215170 (0.233 sec/batch), lr: 0.334808
2019-12-27 13:51:01.858501: step 42980/136300 (epoch 32/100), loss = 0.363074 (0.239 sec/batch), lr: 0.334808
2019-12-27 13:51:06.715865: step 43000/136300 (epoch 32/100), loss = 0.639027 (0.198 sec/batch), lr: 0.334808
2019-12-27 13:51:11.430287: step 43020/136300 (epoch 32/100), loss = 0.288010 (0.199 sec/batch), lr: 0.334808
2019-12-27 13:51:16.251967: step 43040/136300 (epoch 32/100), loss = 0.316414 (0.247 sec/batch), lr: 0.334808
2019-12-27 13:51:22.714453: step 43060/136300 (epoch 32/100), loss = 0.178538 (0.244 sec/batch), lr: 0.334808
2019-12-27 13:51:27.401492: step 43080/136300 (epoch 32/100), loss = 0.236040 (0.237 sec/batch), lr: 0.334808
2019-12-27 13:51:32.334703: step 43100/136300 (epoch 32/100), loss = 0.321073 (0.233 sec/batch), lr: 0.334808
2019-12-27 13:51:37.006771: step 43120/136300 (epoch 32/100), loss = 0.500249 (0.232 sec/batch), lr: 0.334808
2019-12-27 13:51:41.756012: step 43140/136300 (epoch 32/100), loss = 0.303208 (0.239 sec/batch), lr: 0.334808
2019-12-27 13:51:46.622789: step 43160/136300 (epoch 32/100), loss = 0.337160 (0.242 sec/batch), lr: 0.334808
2019-12-27 13:51:51.401946: step 43180/136300 (epoch 32/100), loss = 0.329465 (0.160 sec/batch), lr: 0.334808
2019-12-27 13:51:56.227558: step 43200/136300 (epoch 32/100), loss = 0.660864 (0.237 sec/batch), lr: 0.334808
2019-12-27 13:52:01.145208: step 43220/136300 (epoch 32/100), loss = 0.401166 (0.220 sec/batch), lr: 0.334808
2019-12-27 13:52:05.966748: step 43240/136300 (epoch 32/100), loss = 0.488090 (0.227 sec/batch), lr: 0.334808
2019-12-27 13:52:12.074035: step 43260/136300 (epoch 32/100), loss = 0.375117 (0.221 sec/batch), lr: 0.334808
2019-12-27 13:52:16.937455: step 43280/136300 (epoch 32/100), loss = 0.268594 (0.236 sec/batch), lr: 0.334808
2019-12-27 13:52:21.517593: step 43300/136300 (epoch 32/100), loss = 0.359646 (0.228 sec/batch), lr: 0.334808
2019-12-27 13:52:26.265712: step 43320/136300 (epoch 32/100), loss = 0.249849 (0.233 sec/batch), lr: 0.334808
2019-12-27 13:52:31.051258: step 43340/136300 (epoch 32/100), loss = 0.293290 (0.236 sec/batch), lr: 0.334808
2019-12-27 13:52:35.832197: step 43360/136300 (epoch 32/100), loss = 0.416328 (0.242 sec/batch), lr: 0.334808
2019-12-27 13:52:40.673445: step 43380/136300 (epoch 32/100), loss = 0.351391 (0.197 sec/batch), lr: 0.334808
2019-12-27 13:52:45.410370: step 43400/136300 (epoch 32/100), loss = 0.522723 (0.231 sec/batch), lr: 0.334808
2019-12-27 13:52:50.132897: step 43420/136300 (epoch 32/100), loss = 0.344666 (0.215 sec/batch), lr: 0.334808
2019-12-27 13:52:54.841467: step 43440/136300 (epoch 32/100), loss = 0.377585 (0.163 sec/batch), lr: 0.334808
2019-12-27 13:53:01.281728: step 43460/136300 (epoch 32/100), loss = 0.295459 (0.232 sec/batch), lr: 0.334808
2019-12-27 13:53:06.162051: step 43480/136300 (epoch 32/100), loss = 0.237423 (0.229 sec/batch), lr: 0.334808
2019-12-27 13:53:10.989947: step 43500/136300 (epoch 32/100), loss = 0.537173 (0.228 sec/batch), lr: 0.334808
2019-12-27 13:53:15.671239: step 43520/136300 (epoch 32/100), loss = 0.239685 (0.170 sec/batch), lr: 0.334808
2019-12-27 13:53:20.385933: step 43540/136300 (epoch 32/100), loss = 0.243606 (0.195 sec/batch), lr: 0.334808
2019-12-27 13:53:25.101055: step 43560/136300 (epoch 32/100), loss = 0.175803 (0.197 sec/batch), lr: 0.334808
2019-12-27 13:53:29.982736: step 43580/136300 (epoch 32/100), loss = 0.361216 (0.234 sec/batch), lr: 0.334808
2019-12-27 13:53:34.823156: step 43600/136300 (epoch 32/100), loss = 0.254515 (0.223 sec/batch), lr: 0.334808
Evaluating on dev set...
Precision (micro): 76.046%
   Recall (micro): 52.502%
       F1 (micro): 62.118%
epoch 32: train_loss = 0.320979, dev_loss = 0.439278, dev_f1 = 0.6212
model saved to ./saved_models/01/checkpoint_epoch_32.pt

2019-12-27 13:54:12.965368: step 43620/136300 (epoch 33/100), loss = 0.289200 (0.203 sec/batch), lr: 0.301327
2019-12-27 13:54:18.982037: step 43640/136300 (epoch 33/100), loss = 0.572239 (0.222 sec/batch), lr: 0.301327
2019-12-27 13:54:23.743387: step 43660/136300 (epoch 33/100), loss = 0.536547 (0.219 sec/batch), lr: 0.301327
2019-12-27 13:54:28.473373: step 43680/136300 (epoch 33/100), loss = 0.368127 (0.197 sec/batch), lr: 0.301327
2019-12-27 13:54:33.355087: step 43700/136300 (epoch 33/100), loss = 0.393917 (0.231 sec/batch), lr: 0.301327
2019-12-27 13:54:38.091937: step 43720/136300 (epoch 33/100), loss = 0.345512 (0.216 sec/batch), lr: 0.301327
2019-12-27 13:54:42.831682: step 43740/136300 (epoch 33/100), loss = 0.261298 (0.205 sec/batch), lr: 0.301327
2019-12-27 13:54:47.667432: step 43760/136300 (epoch 33/100), loss = 0.835191 (0.216 sec/batch), lr: 0.301327
2019-12-27 13:54:52.451912: step 43780/136300 (epoch 33/100), loss = 0.188026 (0.242 sec/batch), lr: 0.301327
2019-12-27 13:54:57.290725: step 43800/136300 (epoch 33/100), loss = 0.414308 (0.231 sec/batch), lr: 0.301327
2019-12-27 13:55:02.058759: step 43820/136300 (epoch 33/100), loss = 0.249768 (0.174 sec/batch), lr: 0.301327
2019-12-27 13:55:08.056957: step 43840/136300 (epoch 33/100), loss = 0.575263 (0.184 sec/batch), lr: 0.301327
2019-12-27 13:55:12.873861: step 43860/136300 (epoch 33/100), loss = 0.225303 (0.235 sec/batch), lr: 0.301327
2019-12-27 13:55:17.613013: step 43880/136300 (epoch 33/100), loss = 0.178963 (0.232 sec/batch), lr: 0.301327
2019-12-27 13:55:22.394037: step 43900/136300 (epoch 33/100), loss = 0.219789 (0.200 sec/batch), lr: 0.301327
2019-12-27 13:55:27.236049: step 43920/136300 (epoch 33/100), loss = 0.566849 (0.221 sec/batch), lr: 0.301327
2019-12-27 13:55:31.915951: step 43940/136300 (epoch 33/100), loss = 0.235266 (0.202 sec/batch), lr: 0.301327
2019-12-27 13:55:36.688671: step 43960/136300 (epoch 33/100), loss = 0.240197 (0.232 sec/batch), lr: 0.301327
2019-12-27 13:55:41.575248: step 43980/136300 (epoch 33/100), loss = 0.198619 (0.248 sec/batch), lr: 0.301327
2019-12-27 13:55:46.277688: step 44000/136300 (epoch 33/100), loss = 0.348835 (0.230 sec/batch), lr: 0.301327
2019-12-27 13:55:50.895781: step 44020/136300 (epoch 33/100), loss = 0.205967 (0.168 sec/batch), lr: 0.301327
2019-12-27 13:55:57.078326: step 44040/136300 (epoch 33/100), loss = 0.153833 (0.200 sec/batch), lr: 0.301327
2019-12-27 13:56:01.969592: step 44060/136300 (epoch 33/100), loss = 0.521000 (0.211 sec/batch), lr: 0.301327
2019-12-27 13:56:06.829835: step 44080/136300 (epoch 33/100), loss = 0.392301 (0.224 sec/batch), lr: 0.301327
2019-12-27 13:56:11.597387: step 44100/136300 (epoch 33/100), loss = 0.493369 (0.217 sec/batch), lr: 0.301327
2019-12-27 13:56:16.388532: step 44120/136300 (epoch 33/100), loss = 0.243442 (0.229 sec/batch), lr: 0.301327
2019-12-27 13:56:21.207336: step 44140/136300 (epoch 33/100), loss = 0.255377 (0.227 sec/batch), lr: 0.301327
2019-12-27 13:56:26.089700: step 44160/136300 (epoch 33/100), loss = 0.334594 (0.175 sec/batch), lr: 0.301327
2019-12-27 13:56:30.820268: step 44180/136300 (epoch 33/100), loss = 0.176092 (0.180 sec/batch), lr: 0.301327
2019-12-27 13:56:35.605583: step 44200/136300 (epoch 33/100), loss = 0.228482 (0.232 sec/batch), lr: 0.301327
2019-12-27 13:56:40.313595: step 44220/136300 (epoch 33/100), loss = 0.370648 (0.219 sec/batch), lr: 0.301327
2019-12-27 13:56:46.201871: step 44240/136300 (epoch 33/100), loss = 0.257616 (0.226 sec/batch), lr: 0.301327
2019-12-27 13:56:50.930044: step 44260/136300 (epoch 33/100), loss = 0.341019 (0.201 sec/batch), lr: 0.301327
2019-12-27 13:56:55.676894: step 44280/136300 (epoch 33/100), loss = 0.377227 (0.200 sec/batch), lr: 0.301327
2019-12-27 13:57:00.542494: step 44300/136300 (epoch 33/100), loss = 0.250458 (0.234 sec/batch), lr: 0.301327
2019-12-27 13:57:05.419134: step 44320/136300 (epoch 33/100), loss = 0.206815 (0.217 sec/batch), lr: 0.301327
2019-12-27 13:57:10.305049: step 44340/136300 (epoch 33/100), loss = 0.292778 (0.203 sec/batch), lr: 0.301327
2019-12-27 13:57:15.178711: step 44360/136300 (epoch 33/100), loss = 0.215450 (0.237 sec/batch), lr: 0.301327
2019-12-27 13:57:19.917412: step 44380/136300 (epoch 33/100), loss = 0.167302 (0.241 sec/batch), lr: 0.301327
2019-12-27 13:57:24.722707: step 44400/136300 (epoch 33/100), loss = 0.294721 (0.244 sec/batch), lr: 0.301327
2019-12-27 13:57:31.041681: step 44420/136300 (epoch 33/100), loss = 0.257183 (0.232 sec/batch), lr: 0.301327
2019-12-27 13:57:35.781975: step 44440/136300 (epoch 33/100), loss = 0.277645 (0.238 sec/batch), lr: 0.301327
2019-12-27 13:57:40.649905: step 44460/136300 (epoch 33/100), loss = 0.247712 (0.232 sec/batch), lr: 0.301327
2019-12-27 13:57:45.399257: step 44480/136300 (epoch 33/100), loss = 0.317902 (0.208 sec/batch), lr: 0.301327
2019-12-27 13:57:50.103271: step 44500/136300 (epoch 33/100), loss = 0.425320 (0.234 sec/batch), lr: 0.301327
2019-12-27 13:57:54.943488: step 44520/136300 (epoch 33/100), loss = 0.234930 (0.230 sec/batch), lr: 0.301327
2019-12-27 13:57:59.857527: step 44540/136300 (epoch 33/100), loss = 0.370731 (0.240 sec/batch), lr: 0.301327
2019-12-27 13:58:04.602839: step 44560/136300 (epoch 33/100), loss = 0.330613 (0.217 sec/batch), lr: 0.301327
2019-12-27 13:58:09.512549: step 44580/136300 (epoch 33/100), loss = 0.332189 (0.231 sec/batch), lr: 0.301327
2019-12-27 13:58:14.383381: step 44600/136300 (epoch 33/100), loss = 0.237788 (0.214 sec/batch), lr: 0.301327
2019-12-27 13:58:20.295297: step 44620/136300 (epoch 33/100), loss = 0.353597 (0.214 sec/batch), lr: 0.301327
2019-12-27 13:58:25.159402: step 44640/136300 (epoch 33/100), loss = 0.191574 (0.242 sec/batch), lr: 0.301327
2019-12-27 13:58:29.765920: step 44660/136300 (epoch 33/100), loss = 0.366070 (0.228 sec/batch), lr: 0.301327
2019-12-27 13:58:34.505060: step 44680/136300 (epoch 33/100), loss = 0.159594 (0.228 sec/batch), lr: 0.301327
2019-12-27 13:58:39.333121: step 44700/136300 (epoch 33/100), loss = 0.429324 (0.227 sec/batch), lr: 0.301327
2019-12-27 13:58:44.100107: step 44720/136300 (epoch 33/100), loss = 0.187789 (0.244 sec/batch), lr: 0.301327
2019-12-27 13:58:49.017630: step 44740/136300 (epoch 33/100), loss = 0.406821 (0.244 sec/batch), lr: 0.301327
2019-12-27 13:58:53.801733: step 44760/136300 (epoch 33/100), loss = 0.322752 (0.200 sec/batch), lr: 0.301327
2019-12-27 13:58:58.559696: step 44780/136300 (epoch 33/100), loss = 0.249998 (0.171 sec/batch), lr: 0.301327
2019-12-27 13:59:03.324946: step 44800/136300 (epoch 33/100), loss = 0.421155 (0.234 sec/batch), lr: 0.301327
2019-12-27 13:59:09.784565: step 44820/136300 (epoch 33/100), loss = 0.448850 (0.232 sec/batch), lr: 0.301327
2019-12-27 13:59:14.673723: step 44840/136300 (epoch 33/100), loss = 0.304414 (0.234 sec/batch), lr: 0.301327
2019-12-27 13:59:19.573708: step 44860/136300 (epoch 33/100), loss = 0.426643 (0.246 sec/batch), lr: 0.301327
2019-12-27 13:59:24.357924: step 44880/136300 (epoch 33/100), loss = 0.255456 (0.217 sec/batch), lr: 0.301327
2019-12-27 13:59:29.079949: step 44900/136300 (epoch 33/100), loss = 0.211032 (0.240 sec/batch), lr: 0.301327
2019-12-27 13:59:33.820974: step 44920/136300 (epoch 33/100), loss = 0.150108 (0.227 sec/batch), lr: 0.301327
2019-12-27 13:59:38.712841: step 44940/136300 (epoch 33/100), loss = 0.502177 (0.244 sec/batch), lr: 0.301327
2019-12-27 13:59:43.616705: step 44960/136300 (epoch 33/100), loss = 0.276839 (0.225 sec/batch), lr: 0.301327
Evaluating on dev set...
Precision (micro): 70.075%
   Recall (micro): 60.265%
       F1 (micro): 64.801%
epoch 33: train_loss = 0.313872, dev_loss = 0.437954, dev_f1 = 0.6480
model saved to ./saved_models/01/checkpoint_epoch_33.pt
new best model saved.

2019-12-27 14:00:21.931381: step 44980/136300 (epoch 34/100), loss = 0.415297 (0.173 sec/batch), lr: 0.301327
2019-12-27 14:00:28.138580: step 45000/136300 (epoch 34/100), loss = 0.271155 (0.196 sec/batch), lr: 0.301327
2019-12-27 14:00:32.900504: step 45020/136300 (epoch 34/100), loss = 0.192257 (0.242 sec/batch), lr: 0.301327
2019-12-27 14:00:37.717909: step 45040/136300 (epoch 34/100), loss = 0.370721 (0.238 sec/batch), lr: 0.301327
2019-12-27 14:00:42.577391: step 45060/136300 (epoch 34/100), loss = 0.159214 (0.236 sec/batch), lr: 0.301327
2019-12-27 14:00:47.354921: step 45080/136300 (epoch 34/100), loss = 0.306934 (0.201 sec/batch), lr: 0.301327
2019-12-27 14:00:52.100276: step 45100/136300 (epoch 34/100), loss = 0.386020 (0.176 sec/batch), lr: 0.301327
2019-12-27 14:00:56.954292: step 45120/136300 (epoch 34/100), loss = 0.363202 (0.238 sec/batch), lr: 0.301327
2019-12-27 14:01:01.771447: step 45140/136300 (epoch 34/100), loss = 0.124229 (0.246 sec/batch), lr: 0.301327
2019-12-27 14:01:06.662655: step 45160/136300 (epoch 34/100), loss = 0.248066 (0.239 sec/batch), lr: 0.301327
2019-12-27 14:01:11.481140: step 45180/136300 (epoch 34/100), loss = 0.302454 (0.205 sec/batch), lr: 0.301327
2019-12-27 14:01:17.711293: step 45200/136300 (epoch 34/100), loss = 0.296599 (0.203 sec/batch), lr: 0.301327
2019-12-27 14:01:22.471231: step 45220/136300 (epoch 34/100), loss = 0.200743 (0.221 sec/batch), lr: 0.301327
2019-12-27 14:01:27.308541: step 45240/136300 (epoch 34/100), loss = 0.448855 (0.246 sec/batch), lr: 0.301327
2019-12-27 14:01:32.110066: step 45260/136300 (epoch 34/100), loss = 0.398581 (0.221 sec/batch), lr: 0.301327
2019-12-27 14:01:36.928089: step 45280/136300 (epoch 34/100), loss = 0.329675 (0.216 sec/batch), lr: 0.301327
2019-12-27 14:01:41.708413: step 45300/136300 (epoch 34/100), loss = 0.247974 (0.239 sec/batch), lr: 0.301327
2019-12-27 14:01:46.459817: step 45320/136300 (epoch 34/100), loss = 0.278572 (0.204 sec/batch), lr: 0.301327
2019-12-27 14:01:51.346311: step 45340/136300 (epoch 34/100), loss = 0.571204 (0.218 sec/batch), lr: 0.301327
2019-12-27 14:01:56.105855: step 45360/136300 (epoch 34/100), loss = 0.202309 (0.223 sec/batch), lr: 0.301327
2019-12-27 14:02:00.825256: step 45380/136300 (epoch 34/100), loss = 0.346921 (0.177 sec/batch), lr: 0.301327
2019-12-27 14:02:07.132546: step 45400/136300 (epoch 34/100), loss = 0.484017 (0.185 sec/batch), lr: 0.301327
2019-12-27 14:02:11.990119: step 45420/136300 (epoch 34/100), loss = 0.251037 (0.205 sec/batch), lr: 0.301327
2019-12-27 14:02:16.899899: step 45440/136300 (epoch 34/100), loss = 0.302319 (0.239 sec/batch), lr: 0.301327
2019-12-27 14:02:21.708228: step 45460/136300 (epoch 34/100), loss = 0.262080 (0.206 sec/batch), lr: 0.301327
2019-12-27 14:02:26.517463: step 45480/136300 (epoch 34/100), loss = 0.249006 (0.232 sec/batch), lr: 0.301327
2019-12-27 14:02:31.350235: step 45500/136300 (epoch 34/100), loss = 0.227655 (0.172 sec/batch), lr: 0.301327
2019-12-27 14:02:36.334946: step 45520/136300 (epoch 34/100), loss = 0.254424 (0.239 sec/batch), lr: 0.301327
2019-12-27 14:02:41.087361: step 45540/136300 (epoch 34/100), loss = 0.274510 (0.201 sec/batch), lr: 0.301327
2019-12-27 14:02:45.919480: step 45560/136300 (epoch 34/100), loss = 0.445188 (0.221 sec/batch), lr: 0.301327
2019-12-27 14:02:50.617773: step 45580/136300 (epoch 34/100), loss = 0.178340 (0.224 sec/batch), lr: 0.301327
2019-12-27 14:02:56.807716: step 45600/136300 (epoch 34/100), loss = 0.359278 (0.228 sec/batch), lr: 0.301327
2019-12-27 14:03:01.607036: step 45620/136300 (epoch 34/100), loss = 0.344269 (0.233 sec/batch), lr: 0.301327
2019-12-27 14:03:06.350365: step 45640/136300 (epoch 34/100), loss = 0.387833 (0.239 sec/batch), lr: 0.301327
2019-12-27 14:03:11.221010: step 45660/136300 (epoch 34/100), loss = 0.401445 (0.242 sec/batch), lr: 0.301327
2019-12-27 14:03:16.137357: step 45680/136300 (epoch 34/100), loss = 0.309615 (0.227 sec/batch), lr: 0.301327
2019-12-27 14:03:21.033111: step 45700/136300 (epoch 34/100), loss = 0.105075 (0.229 sec/batch), lr: 0.301327
2019-12-27 14:03:25.921109: step 45720/136300 (epoch 34/100), loss = 0.269132 (0.242 sec/batch), lr: 0.301327
2019-12-27 14:03:30.719783: step 45740/136300 (epoch 34/100), loss = 0.237623 (0.231 sec/batch), lr: 0.301327
2019-12-27 14:03:35.519752: step 45760/136300 (epoch 34/100), loss = 0.536486 (0.230 sec/batch), lr: 0.301327
2019-12-27 14:03:42.023449: step 45780/136300 (epoch 34/100), loss = 0.350138 (0.237 sec/batch), lr: 0.301327
2019-12-27 14:03:46.804231: step 45800/136300 (epoch 34/100), loss = 0.376326 (0.226 sec/batch), lr: 0.301327
2019-12-27 14:03:51.683612: step 45820/136300 (epoch 34/100), loss = 0.383034 (0.239 sec/batch), lr: 0.301327
2019-12-27 14:03:56.568523: step 45840/136300 (epoch 34/100), loss = 0.348358 (0.229 sec/batch), lr: 0.301327
2019-12-27 14:04:01.290834: step 45860/136300 (epoch 34/100), loss = 0.285642 (0.207 sec/batch), lr: 0.301327
2019-12-27 14:04:06.145271: step 45880/136300 (epoch 34/100), loss = 0.394354 (0.230 sec/batch), lr: 0.301327
2019-12-27 14:04:11.095651: step 45900/136300 (epoch 34/100), loss = 0.359851 (0.248 sec/batch), lr: 0.301327
2019-12-27 14:04:15.882407: step 45920/136300 (epoch 34/100), loss = 0.210506 (0.184 sec/batch), lr: 0.301327
2019-12-27 14:04:20.891177: step 45940/136300 (epoch 34/100), loss = 0.338306 (0.247 sec/batch), lr: 0.301327
2019-12-27 14:04:25.771806: step 45960/136300 (epoch 34/100), loss = 0.130615 (0.231 sec/batch), lr: 0.301327
2019-12-27 14:04:30.586440: step 45980/136300 (epoch 34/100), loss = 0.326009 (0.236 sec/batch), lr: 0.301327
2019-12-27 14:04:36.770608: step 46000/136300 (epoch 34/100), loss = 0.297106 (0.231 sec/batch), lr: 0.301327
2019-12-27 14:04:41.471249: step 46020/136300 (epoch 34/100), loss = 0.270981 (0.208 sec/batch), lr: 0.301327
2019-12-27 14:04:46.196446: step 46040/136300 (epoch 34/100), loss = 0.415087 (0.230 sec/batch), lr: 0.301327
2019-12-27 14:04:51.061343: step 46060/136300 (epoch 34/100), loss = 0.240996 (0.235 sec/batch), lr: 0.301327
2019-12-27 14:04:55.839132: step 46080/136300 (epoch 34/100), loss = 0.416182 (0.226 sec/batch), lr: 0.301327
2019-12-27 14:05:00.752422: step 46100/136300 (epoch 34/100), loss = 0.349580 (0.222 sec/batch), lr: 0.301327
2019-12-27 14:05:05.620167: step 46120/136300 (epoch 34/100), loss = 0.118914 (0.241 sec/batch), lr: 0.301327
2019-12-27 14:05:10.371155: step 46140/136300 (epoch 34/100), loss = 0.253679 (0.237 sec/batch), lr: 0.301327
2019-12-27 14:05:15.127059: step 46160/136300 (epoch 34/100), loss = 0.236275 (0.245 sec/batch), lr: 0.301327
2019-12-27 14:05:21.476036: step 46180/136300 (epoch 34/100), loss = 0.116195 (0.233 sec/batch), lr: 0.301327
2019-12-27 14:05:26.375771: step 46200/136300 (epoch 34/100), loss = 0.398385 (0.241 sec/batch), lr: 0.301327
2019-12-27 14:05:31.292240: step 46220/136300 (epoch 34/100), loss = 0.239321 (0.237 sec/batch), lr: 0.301327
2019-12-27 14:05:36.117354: step 46240/136300 (epoch 34/100), loss = 0.645679 (0.186 sec/batch), lr: 0.301327
2019-12-27 14:05:40.858269: step 46260/136300 (epoch 34/100), loss = 0.288271 (0.220 sec/batch), lr: 0.301327
2019-12-27 14:05:45.675546: step 46280/136300 (epoch 34/100), loss = 0.249341 (0.215 sec/batch), lr: 0.301327
2019-12-27 14:05:50.500527: step 46300/136300 (epoch 34/100), loss = 0.365302 (0.240 sec/batch), lr: 0.301327
2019-12-27 14:05:55.432342: step 46320/136300 (epoch 34/100), loss = 0.263297 (0.244 sec/batch), lr: 0.301327
2019-12-27 14:06:00.392503: step 46340/136300 (epoch 34/100), loss = 0.384600 (0.218 sec/batch), lr: 0.301327
Evaluating on dev set...
Precision (micro): 69.157%
   Recall (micro): 61.295%
       F1 (micro): 64.989%
epoch 34: train_loss = 0.310523, dev_loss = 0.435791, dev_f1 = 0.6499
model saved to ./saved_models/01/checkpoint_epoch_34.pt
new best model saved.

2019-12-27 14:06:40.762629: step 46360/136300 (epoch 35/100), loss = 0.212718 (1.821 sec/batch), lr: 0.301327
2019-12-27 14:06:45.493969: step 46380/136300 (epoch 35/100), loss = 0.369513 (0.221 sec/batch), lr: 0.301327
2019-12-27 14:06:50.349211: step 46400/136300 (epoch 35/100), loss = 0.341180 (0.234 sec/batch), lr: 0.301327
2019-12-27 14:06:55.211459: step 46420/136300 (epoch 35/100), loss = 0.177966 (0.243 sec/batch), lr: 0.301327
2019-12-27 14:07:00.034449: step 46440/136300 (epoch 35/100), loss = 0.256946 (0.225 sec/batch), lr: 0.301327
2019-12-27 14:07:04.836950: step 46460/136300 (epoch 35/100), loss = 0.446258 (0.239 sec/batch), lr: 0.301327
2019-12-27 14:07:09.654585: step 46480/136300 (epoch 35/100), loss = 0.374175 (0.231 sec/batch), lr: 0.301327
2019-12-27 14:07:14.476713: step 46500/136300 (epoch 35/100), loss = 0.251911 (0.218 sec/batch), lr: 0.301327
2019-12-27 14:07:19.392629: step 46520/136300 (epoch 35/100), loss = 0.336144 (0.240 sec/batch), lr: 0.301327
2019-12-27 14:07:24.272868: step 46540/136300 (epoch 35/100), loss = 0.405797 (0.241 sec/batch), lr: 0.301327
2019-12-27 14:07:30.564246: step 46560/136300 (epoch 35/100), loss = 0.129484 (0.225 sec/batch), lr: 0.301327
2019-12-27 14:07:35.290706: step 46580/136300 (epoch 35/100), loss = 0.271243 (0.228 sec/batch), lr: 0.301327
2019-12-27 14:07:40.107990: step 46600/136300 (epoch 35/100), loss = 0.355202 (0.243 sec/batch), lr: 0.301327
2019-12-27 14:07:44.975704: step 46620/136300 (epoch 35/100), loss = 0.175514 (0.240 sec/batch), lr: 0.301327
2019-12-27 14:07:49.772068: step 46640/136300 (epoch 35/100), loss = 0.195270 (0.237 sec/batch), lr: 0.301327
2019-12-27 14:07:54.537640: step 46660/136300 (epoch 35/100), loss = 0.258351 (0.237 sec/batch), lr: 0.301327
2019-12-27 14:07:59.406309: step 46680/136300 (epoch 35/100), loss = 0.516156 (0.239 sec/batch), lr: 0.301327
2019-12-27 14:08:04.232686: step 46700/136300 (epoch 35/100), loss = 0.328718 (0.215 sec/batch), lr: 0.301327
2019-12-27 14:08:08.981867: step 46720/136300 (epoch 35/100), loss = 0.212785 (0.223 sec/batch), lr: 0.301327
2019-12-27 14:08:13.744793: step 46740/136300 (epoch 35/100), loss = 0.368918 (0.206 sec/batch), lr: 0.301327
2019-12-27 14:08:19.970065: step 46760/136300 (epoch 35/100), loss = 0.172406 (0.213 sec/batch), lr: 0.301327
2019-12-27 14:08:24.831785: step 46780/136300 (epoch 35/100), loss = 0.333325 (0.232 sec/batch), lr: 0.301327
2019-12-27 14:08:29.722392: step 46800/136300 (epoch 35/100), loss = 0.191865 (0.243 sec/batch), lr: 0.301327
2019-12-27 14:08:34.541232: step 46820/136300 (epoch 35/100), loss = 0.214514 (0.234 sec/batch), lr: 0.301327
2019-12-27 14:08:39.360191: step 46840/136300 (epoch 35/100), loss = 0.327510 (0.236 sec/batch), lr: 0.301327
2019-12-27 14:08:44.242107: step 46860/136300 (epoch 35/100), loss = 0.411378 (0.217 sec/batch), lr: 0.301327
2019-12-27 14:08:49.191355: step 46880/136300 (epoch 35/100), loss = 0.166408 (0.243 sec/batch), lr: 0.301327
2019-12-27 14:08:53.989085: step 46900/136300 (epoch 35/100), loss = 0.344195 (0.238 sec/batch), lr: 0.301327
2019-12-27 14:08:58.826138: step 46920/136300 (epoch 35/100), loss = 0.185965 (0.242 sec/batch), lr: 0.301327
2019-12-27 14:09:03.559553: step 46940/136300 (epoch 35/100), loss = 0.231993 (0.239 sec/batch), lr: 0.301327
2019-12-27 14:09:09.723963: step 46960/136300 (epoch 35/100), loss = 0.374970 (0.216 sec/batch), lr: 0.301327
2019-12-27 14:09:14.584523: step 46980/136300 (epoch 35/100), loss = 0.318180 (0.232 sec/batch), lr: 0.301327
2019-12-27 14:09:19.336934: step 47000/136300 (epoch 35/100), loss = 0.106780 (0.239 sec/batch), lr: 0.301327
2019-12-27 14:09:24.190690: step 47020/136300 (epoch 35/100), loss = 0.176622 (0.246 sec/batch), lr: 0.301327
2019-12-27 14:09:29.130724: step 47040/136300 (epoch 35/100), loss = 0.457689 (0.202 sec/batch), lr: 0.301327
2019-12-27 14:09:34.032649: step 47060/136300 (epoch 35/100), loss = 0.254305 (0.227 sec/batch), lr: 0.301327
2019-12-27 14:09:38.969509: step 47080/136300 (epoch 35/100), loss = 0.387686 (0.236 sec/batch), lr: 0.301327
2019-12-27 14:09:43.759123: step 47100/136300 (epoch 35/100), loss = 0.473936 (0.222 sec/batch), lr: 0.301327
2019-12-27 14:09:48.603917: step 47120/136300 (epoch 35/100), loss = 0.249094 (0.233 sec/batch), lr: 0.301327
2019-12-27 14:09:54.973748: step 47140/136300 (epoch 35/100), loss = 0.238903 (1.664 sec/batch), lr: 0.301327
2019-12-27 14:09:59.762426: step 47160/136300 (epoch 35/100), loss = 0.312820 (0.174 sec/batch), lr: 0.301327
2019-12-27 14:10:04.628760: step 47180/136300 (epoch 35/100), loss = 0.325598 (0.246 sec/batch), lr: 0.301327
2019-12-27 14:10:09.559491: step 47200/136300 (epoch 35/100), loss = 0.312295 (0.217 sec/batch), lr: 0.301327
2019-12-27 14:10:14.254902: step 47220/136300 (epoch 35/100), loss = 0.349011 (0.234 sec/batch), lr: 0.301327
2019-12-27 14:10:19.147533: step 47240/136300 (epoch 35/100), loss = 0.415144 (0.236 sec/batch), lr: 0.301327
2019-12-27 14:10:24.058293: step 47260/136300 (epoch 35/100), loss = 0.303648 (0.241 sec/batch), lr: 0.301327
2019-12-27 14:10:28.861653: step 47280/136300 (epoch 35/100), loss = 0.160981 (0.237 sec/batch), lr: 0.301327
2019-12-27 14:10:33.833010: step 47300/136300 (epoch 35/100), loss = 0.159360 (0.247 sec/batch), lr: 0.301327
2019-12-27 14:10:38.719125: step 47320/136300 (epoch 35/100), loss = 0.269388 (0.235 sec/batch), lr: 0.301327
2019-12-27 14:10:43.530001: step 47340/136300 (epoch 35/100), loss = 0.356887 (0.217 sec/batch), lr: 0.301327
2019-12-27 14:10:49.632372: step 47360/136300 (epoch 35/100), loss = 0.282064 (0.247 sec/batch), lr: 0.301327
2019-12-27 14:10:54.434499: step 47380/136300 (epoch 35/100), loss = 0.243668 (0.202 sec/batch), lr: 0.301327
2019-12-27 14:10:59.143595: step 47400/136300 (epoch 35/100), loss = 0.464526 (0.231 sec/batch), lr: 0.301327
2019-12-27 14:11:03.929059: step 47420/136300 (epoch 35/100), loss = 0.359526 (0.214 sec/batch), lr: 0.301327
2019-12-27 14:11:08.708515: step 47440/136300 (epoch 35/100), loss = 0.276180 (0.182 sec/batch), lr: 0.301327
2019-12-27 14:11:13.647004: step 47460/136300 (epoch 35/100), loss = 0.304249 (0.242 sec/batch), lr: 0.301327
2019-12-27 14:11:18.484626: step 47480/136300 (epoch 35/100), loss = 0.410993 (0.225 sec/batch), lr: 0.301327
2019-12-27 14:11:23.266896: step 47500/136300 (epoch 35/100), loss = 0.447207 (0.220 sec/batch), lr: 0.301327
2019-12-27 14:11:28.006540: step 47520/136300 (epoch 35/100), loss = 0.269126 (0.241 sec/batch), lr: 0.301327
2019-12-27 14:11:32.847247: step 47540/136300 (epoch 35/100), loss = 0.434573 (0.227 sec/batch), lr: 0.301327
2019-12-27 14:11:39.194077: step 47560/136300 (epoch 35/100), loss = 0.229107 (0.249 sec/batch), lr: 0.301327
2019-12-27 14:11:44.104948: step 47580/136300 (epoch 35/100), loss = 0.357327 (0.238 sec/batch), lr: 0.301327
2019-12-27 14:11:48.970514: step 47600/136300 (epoch 35/100), loss = 0.427171 (0.237 sec/batch), lr: 0.301327
2019-12-27 14:11:53.682528: step 47620/136300 (epoch 35/100), loss = 0.244913 (0.228 sec/batch), lr: 0.301327
2019-12-27 14:11:58.430530: step 47640/136300 (epoch 35/100), loss = 0.314210 (0.213 sec/batch), lr: 0.301327
2019-12-27 14:12:03.194498: step 47660/136300 (epoch 35/100), loss = 0.374126 (0.233 sec/batch), lr: 0.301327
2019-12-27 14:12:08.128981: step 47680/136300 (epoch 35/100), loss = 0.234941 (0.243 sec/batch), lr: 0.301327
2019-12-27 14:12:13.047924: step 47700/136300 (epoch 35/100), loss = 0.426593 (0.242 sec/batch), lr: 0.301327
Evaluating on dev set...
Precision (micro): 71.555%
   Recall (micro): 59.603%
       F1 (micro): 65.034%
epoch 35: train_loss = 0.308363, dev_loss = 0.446777, dev_f1 = 0.6503
model saved to ./saved_models/01/checkpoint_epoch_35.pt
new best model saved.

2019-12-27 14:12:51.362711: step 47720/136300 (epoch 36/100), loss = 0.190464 (0.234 sec/batch), lr: 0.301327
2019-12-27 14:12:57.436505: step 47740/136300 (epoch 36/100), loss = 0.224649 (0.202 sec/batch), lr: 0.301327
2019-12-27 14:13:02.272491: step 47760/136300 (epoch 36/100), loss = 0.333193 (0.237 sec/batch), lr: 0.301327
2019-12-27 14:13:07.049205: step 47780/136300 (epoch 36/100), loss = 0.239908 (0.251 sec/batch), lr: 0.301327
2019-12-27 14:13:11.937148: step 47800/136300 (epoch 36/100), loss = 0.270636 (0.213 sec/batch), lr: 0.301327
2019-12-27 14:13:16.661327: step 47820/136300 (epoch 36/100), loss = 0.187744 (0.206 sec/batch), lr: 0.301327
2019-12-27 14:13:21.450745: step 47840/136300 (epoch 36/100), loss = 0.332825 (0.215 sec/batch), lr: 0.301327
2019-12-27 14:13:26.274356: step 47860/136300 (epoch 36/100), loss = 0.312128 (0.244 sec/batch), lr: 0.301327
2019-12-27 14:13:31.131461: step 47880/136300 (epoch 36/100), loss = 0.429035 (0.229 sec/batch), lr: 0.301327
2019-12-27 14:13:35.994143: step 47900/136300 (epoch 36/100), loss = 0.371443 (0.238 sec/batch), lr: 0.301327
2019-12-27 14:13:42.178564: step 47920/136300 (epoch 36/100), loss = 0.237066 (0.183 sec/batch), lr: 0.301327
2019-12-27 14:13:46.898247: step 47940/136300 (epoch 36/100), loss = 0.266495 (0.227 sec/batch), lr: 0.301327
2019-12-27 14:13:51.721186: step 47960/136300 (epoch 36/100), loss = 0.312889 (0.238 sec/batch), lr: 0.301327
2019-12-27 14:13:56.585896: step 47980/136300 (epoch 36/100), loss = 0.272454 (0.216 sec/batch), lr: 0.301327
2019-12-27 14:14:01.407426: step 48000/136300 (epoch 36/100), loss = 0.497371 (0.201 sec/batch), lr: 0.301327
2019-12-27 14:14:06.131209: step 48020/136300 (epoch 36/100), loss = 0.355681 (0.203 sec/batch), lr: 0.301327
2019-12-27 14:14:10.979348: step 48040/136300 (epoch 36/100), loss = 0.315444 (0.241 sec/batch), lr: 0.301327
2019-12-27 14:14:15.799221: step 48060/136300 (epoch 36/100), loss = 0.228397 (0.245 sec/batch), lr: 0.301327
2019-12-27 14:14:20.535241: step 48080/136300 (epoch 36/100), loss = 0.171454 (0.209 sec/batch), lr: 0.301327
2019-12-27 14:14:25.319850: step 48100/136300 (epoch 36/100), loss = 0.154743 (0.212 sec/batch), lr: 0.301327
2019-12-27 14:14:31.490709: step 48120/136300 (epoch 36/100), loss = 0.136814 (0.222 sec/batch), lr: 0.301327
2019-12-27 14:14:36.298599: step 48140/136300 (epoch 36/100), loss = 0.261058 (0.217 sec/batch), lr: 0.301327
2019-12-27 14:14:41.212164: step 48160/136300 (epoch 36/100), loss = 0.150141 (0.235 sec/batch), lr: 0.301327
2019-12-27 14:14:46.015485: step 48180/136300 (epoch 36/100), loss = 0.411449 (0.224 sec/batch), lr: 0.301327
2019-12-27 14:14:50.859762: step 48200/136300 (epoch 36/100), loss = 0.294666 (0.231 sec/batch), lr: 0.301327
2019-12-27 14:14:55.744083: step 48220/136300 (epoch 36/100), loss = 0.205461 (0.233 sec/batch), lr: 0.301327
2019-12-27 14:15:00.622311: step 48240/136300 (epoch 36/100), loss = 0.384503 (0.241 sec/batch), lr: 0.301327
2019-12-27 14:15:05.429602: step 48260/136300 (epoch 36/100), loss = 0.254711 (0.244 sec/batch), lr: 0.301327
2019-12-27 14:15:10.273665: step 48280/136300 (epoch 36/100), loss = 0.230388 (0.227 sec/batch), lr: 0.301327
2019-12-27 14:15:15.006658: step 48300/136300 (epoch 36/100), loss = 0.440877 (0.174 sec/batch), lr: 0.301327
2019-12-27 14:15:21.270256: step 48320/136300 (epoch 36/100), loss = 0.343748 (0.226 sec/batch), lr: 0.301327
2019-12-27 14:15:26.075839: step 48340/136300 (epoch 36/100), loss = 0.238469 (0.241 sec/batch), lr: 0.301327
2019-12-27 14:15:30.816046: step 48360/136300 (epoch 36/100), loss = 0.226433 (0.232 sec/batch), lr: 0.301327
2019-12-27 14:15:35.641722: step 48380/136300 (epoch 36/100), loss = 0.302198 (0.230 sec/batch), lr: 0.301327
2019-12-27 14:15:40.571890: step 48400/136300 (epoch 36/100), loss = 0.252384 (0.250 sec/batch), lr: 0.301327
2019-12-27 14:15:45.434380: step 48420/136300 (epoch 36/100), loss = 0.220366 (0.240 sec/batch), lr: 0.301327
2019-12-27 14:15:50.370761: step 48440/136300 (epoch 36/100), loss = 0.270478 (0.241 sec/batch), lr: 0.301327
2019-12-27 14:15:55.194222: step 48460/136300 (epoch 36/100), loss = 0.202692 (0.209 sec/batch), lr: 0.301327
2019-12-27 14:16:00.011365: step 48480/136300 (epoch 36/100), loss = 0.586226 (0.247 sec/batch), lr: 0.301327
2019-12-27 14:16:04.940319: step 48500/136300 (epoch 36/100), loss = 0.305882 (0.235 sec/batch), lr: 0.301327
2019-12-27 14:16:11.501058: step 48520/136300 (epoch 36/100), loss = 0.340811 (0.233 sec/batch), lr: 0.301327
2019-12-27 14:16:16.318044: step 48540/136300 (epoch 36/100), loss = 0.212562 (0.237 sec/batch), lr: 0.301327
2019-12-27 14:16:21.267346: step 48560/136300 (epoch 36/100), loss = 0.360276 (0.235 sec/batch), lr: 0.301327
2019-12-27 14:16:25.991857: step 48580/136300 (epoch 36/100), loss = 0.245101 (0.248 sec/batch), lr: 0.301327
2019-12-27 14:16:30.893839: step 48600/136300 (epoch 36/100), loss = 0.362891 (0.242 sec/batch), lr: 0.301327
2019-12-27 14:16:35.865307: step 48620/136300 (epoch 36/100), loss = 0.291557 (0.248 sec/batch), lr: 0.301327
2019-12-27 14:16:40.647661: step 48640/136300 (epoch 36/100), loss = 0.274160 (0.217 sec/batch), lr: 0.301327
2019-12-27 14:16:45.634421: step 48660/136300 (epoch 36/100), loss = 0.371814 (0.230 sec/batch), lr: 0.301327
2019-12-27 14:16:50.516109: step 48680/136300 (epoch 36/100), loss = 0.187814 (0.235 sec/batch), lr: 0.301327
2019-12-27 14:16:55.362634: step 48700/136300 (epoch 36/100), loss = 0.230293 (0.207 sec/batch), lr: 0.301327
2019-12-27 14:17:01.745534: step 48720/136300 (epoch 36/100), loss = 0.277550 (0.236 sec/batch), lr: 0.301327
2019-12-27 14:17:06.618713: step 48740/136300 (epoch 36/100), loss = 0.275468 (0.242 sec/batch), lr: 0.301327
2019-12-27 14:17:11.273901: step 48760/136300 (epoch 36/100), loss = 0.203221 (0.230 sec/batch), lr: 0.301327
2019-12-27 14:17:16.030261: step 48780/136300 (epoch 36/100), loss = 0.358973 (0.222 sec/batch), lr: 0.301327
2019-12-27 14:17:20.882215: step 48800/136300 (epoch 36/100), loss = 0.255894 (0.237 sec/batch), lr: 0.301327
2019-12-27 14:17:25.712005: step 48820/136300 (epoch 36/100), loss = 0.268787 (0.216 sec/batch), lr: 0.301327
2019-12-27 14:17:30.638661: step 48840/136300 (epoch 36/100), loss = 0.150173 (0.225 sec/batch), lr: 0.301327
2019-12-27 14:17:35.414645: step 48860/136300 (epoch 36/100), loss = 0.508921 (0.228 sec/batch), lr: 0.301327
2019-12-27 14:17:40.117698: step 48880/136300 (epoch 36/100), loss = 0.263057 (0.227 sec/batch), lr: 0.301327
2019-12-27 14:17:44.986287: step 48900/136300 (epoch 36/100), loss = 0.344570 (0.250 sec/batch), lr: 0.301327
2019-12-27 14:17:51.451282: step 48920/136300 (epoch 36/100), loss = 0.239434 (0.228 sec/batch), lr: 0.301327
2019-12-27 14:17:56.383297: step 48940/136300 (epoch 36/100), loss = 0.335033 (0.228 sec/batch), lr: 0.301327
2019-12-27 14:18:01.271318: step 48960/136300 (epoch 36/100), loss = 0.390009 (0.215 sec/batch), lr: 0.301327
2019-12-27 14:18:05.902762: step 48980/136300 (epoch 36/100), loss = 0.281659 (0.178 sec/batch), lr: 0.301327
2019-12-27 14:18:10.750211: step 49000/136300 (epoch 36/100), loss = 0.536111 (0.236 sec/batch), lr: 0.301327
2019-12-27 14:18:15.574721: step 49020/136300 (epoch 36/100), loss = 0.281094 (0.245 sec/batch), lr: 0.301327
2019-12-27 14:18:20.484108: step 49040/136300 (epoch 36/100), loss = 0.422459 (0.240 sec/batch), lr: 0.301327
2019-12-27 14:18:25.339236: step 49060/136300 (epoch 36/100), loss = 0.318112 (0.236 sec/batch), lr: 0.301327
Evaluating on dev set...
Precision (micro): 70.827%
   Recall (micro): 61.589%
       F1 (micro): 65.886%
epoch 36: train_loss = 0.308162, dev_loss = 0.442695, dev_f1 = 0.6589
model saved to ./saved_models/01/checkpoint_epoch_36.pt
new best model saved.

2019-12-27 14:19:03.799356: step 49080/136300 (epoch 37/100), loss = 0.233859 (0.233 sec/batch), lr: 0.301327
2019-12-27 14:19:09.821542: step 49100/136300 (epoch 37/100), loss = 0.164068 (0.218 sec/batch), lr: 0.301327
2019-12-27 14:19:14.661361: step 49120/136300 (epoch 37/100), loss = 0.427134 (0.218 sec/batch), lr: 0.301327
2019-12-27 14:19:19.409857: step 49140/136300 (epoch 37/100), loss = 0.199461 (0.248 sec/batch), lr: 0.301327
2019-12-27 14:19:24.326595: step 49160/136300 (epoch 37/100), loss = 0.369629 (0.203 sec/batch), lr: 0.301327
2019-12-27 14:19:29.082483: step 49180/136300 (epoch 37/100), loss = 0.301542 (0.220 sec/batch), lr: 0.301327
2019-12-27 14:19:33.894246: step 49200/136300 (epoch 37/100), loss = 0.237484 (0.231 sec/batch), lr: 0.301327
2019-12-27 14:19:38.722551: step 49220/136300 (epoch 37/100), loss = 0.300727 (0.207 sec/batch), lr: 0.301327
2019-12-27 14:19:43.567816: step 49240/136300 (epoch 37/100), loss = 0.449394 (0.207 sec/batch), lr: 0.301327
2019-12-27 14:19:48.490885: step 49260/136300 (epoch 37/100), loss = 0.217561 (0.222 sec/batch), lr: 0.301327
2019-12-27 14:19:53.316504: step 49280/136300 (epoch 37/100), loss = 0.149908 (0.234 sec/batch), lr: 0.301327
2019-12-27 14:19:59.540614: step 49300/136300 (epoch 37/100), loss = 0.379898 (0.240 sec/batch), lr: 0.301327
2019-12-27 14:20:04.272563: step 49320/136300 (epoch 37/100), loss = 0.376847 (0.205 sec/batch), lr: 0.301327
2019-12-27 14:20:09.192004: step 49340/136300 (epoch 37/100), loss = 0.342832 (0.235 sec/batch), lr: 0.301327
2019-12-27 14:20:14.015653: step 49360/136300 (epoch 37/100), loss = 0.308401 (0.218 sec/batch), lr: 0.301327
2019-12-27 14:20:18.779873: step 49380/136300 (epoch 37/100), loss = 0.396840 (0.231 sec/batch), lr: 0.301327
2019-12-27 14:20:23.560754: step 49400/136300 (epoch 37/100), loss = 0.349118 (0.246 sec/batch), lr: 0.301327
2019-12-27 14:20:28.402643: step 49420/136300 (epoch 37/100), loss = 0.215902 (0.234 sec/batch), lr: 0.301327
2019-12-27 14:20:33.224645: step 49440/136300 (epoch 37/100), loss = 0.394424 (0.198 sec/batch), lr: 0.301327
2019-12-27 14:20:37.965387: step 49460/136300 (epoch 37/100), loss = 0.168108 (0.223 sec/batch), lr: 0.301327
2019-12-27 14:20:44.174312: step 49480/136300 (epoch 37/100), loss = 0.111471 (0.225 sec/batch), lr: 0.301327
2019-12-27 14:20:49.012275: step 49500/136300 (epoch 37/100), loss = 0.518963 (0.247 sec/batch), lr: 0.301327
2019-12-27 14:20:53.928270: step 49520/136300 (epoch 37/100), loss = 0.141541 (0.227 sec/batch), lr: 0.301327
2019-12-27 14:20:58.785321: step 49540/136300 (epoch 37/100), loss = 0.644262 (0.228 sec/batch), lr: 0.301327
2019-12-27 14:21:03.610604: step 49560/136300 (epoch 37/100), loss = 0.195010 (0.226 sec/batch), lr: 0.301327
2019-12-27 14:21:08.499664: step 49580/136300 (epoch 37/100), loss = 0.325293 (0.241 sec/batch), lr: 0.301327
2019-12-27 14:21:13.393091: step 49600/136300 (epoch 37/100), loss = 0.280765 (0.238 sec/batch), lr: 0.301327
2019-12-27 14:21:18.268033: step 49620/136300 (epoch 37/100), loss = 0.213028 (0.240 sec/batch), lr: 0.301327
2019-12-27 14:21:22.947773: step 49640/136300 (epoch 37/100), loss = 0.261891 (0.234 sec/batch), lr: 0.301327
2019-12-27 14:21:27.760937: step 49660/136300 (epoch 37/100), loss = 0.248302 (0.179 sec/batch), lr: 0.301327
2019-12-27 14:21:33.817627: step 49680/136300 (epoch 37/100), loss = 0.235248 (0.222 sec/batch), lr: 0.301327
2019-12-27 14:21:38.630606: step 49700/136300 (epoch 37/100), loss = 0.289979 (0.237 sec/batch), lr: 0.301327
2019-12-27 14:21:43.386573: step 49720/136300 (epoch 37/100), loss = 0.341095 (0.222 sec/batch), lr: 0.301327
2019-12-27 14:21:48.229144: step 49740/136300 (epoch 37/100), loss = 0.173179 (0.232 sec/batch), lr: 0.301327
2019-12-27 14:21:53.171439: step 49760/136300 (epoch 37/100), loss = 0.393118 (0.237 sec/batch), lr: 0.301327
2019-12-27 14:21:58.058383: step 49780/136300 (epoch 37/100), loss = 0.456655 (0.220 sec/batch), lr: 0.301327
2019-12-27 14:22:02.907089: step 49800/136300 (epoch 37/100), loss = 0.335370 (0.240 sec/batch), lr: 0.301327
2019-12-27 14:22:07.770647: step 49820/136300 (epoch 37/100), loss = 0.237819 (0.234 sec/batch), lr: 0.301327
2019-12-27 14:22:12.540996: step 49840/136300 (epoch 37/100), loss = 0.432828 (0.230 sec/batch), lr: 0.301327
2019-12-27 14:22:17.454178: step 49860/136300 (epoch 37/100), loss = 0.290781 (0.234 sec/batch), lr: 0.301327
2019-12-27 14:22:23.658444: step 49880/136300 (epoch 37/100), loss = 0.300532 (0.217 sec/batch), lr: 0.301327
2019-12-27 14:22:28.448602: step 49900/136300 (epoch 37/100), loss = 0.368203 (0.246 sec/batch), lr: 0.301327
2019-12-27 14:22:33.390659: step 49920/136300 (epoch 37/100), loss = 0.314446 (0.178 sec/batch), lr: 0.301327
2019-12-27 14:22:38.057665: step 49940/136300 (epoch 37/100), loss = 0.195515 (0.228 sec/batch), lr: 0.301327
2019-12-27 14:22:42.977543: step 49960/136300 (epoch 37/100), loss = 0.410810 (0.236 sec/batch), lr: 0.301327
2019-12-27 14:22:47.914629: step 49980/136300 (epoch 37/100), loss = 0.149321 (0.218 sec/batch), lr: 0.301327
2019-12-27 14:22:52.696442: step 50000/136300 (epoch 37/100), loss = 0.373163 (0.238 sec/batch), lr: 0.301327
2019-12-27 14:22:57.653354: step 50020/136300 (epoch 37/100), loss = 0.213189 (0.241 sec/batch), lr: 0.301327
2019-12-27 14:23:02.570646: step 50040/136300 (epoch 37/100), loss = 0.384461 (0.225 sec/batch), lr: 0.301327
2019-12-27 14:23:07.402544: step 50060/136300 (epoch 37/100), loss = 0.204719 (0.227 sec/batch), lr: 0.301327
2019-12-27 14:23:13.824972: step 50080/136300 (epoch 37/100), loss = 0.217499 (0.216 sec/batch), lr: 0.301327
2019-12-27 14:23:18.720651: step 50100/136300 (epoch 37/100), loss = 0.170431 (0.201 sec/batch), lr: 0.301327
2019-12-27 14:23:23.358627: step 50120/136300 (epoch 37/100), loss = 0.209055 (0.204 sec/batch), lr: 0.301327
2019-12-27 14:23:28.110404: step 50140/136300 (epoch 37/100), loss = 0.346949 (0.229 sec/batch), lr: 0.301327
2019-12-27 14:23:32.958094: step 50160/136300 (epoch 37/100), loss = 0.344849 (0.238 sec/batch), lr: 0.301327
2019-12-27 14:23:37.805299: step 50180/136300 (epoch 37/100), loss = 0.249702 (0.202 sec/batch), lr: 0.301327
2019-12-27 14:23:42.719279: step 50200/136300 (epoch 37/100), loss = 0.379966 (0.236 sec/batch), lr: 0.301327
2019-12-27 14:23:47.469028: step 50220/136300 (epoch 37/100), loss = 0.299373 (0.236 sec/batch), lr: 0.301327
2019-12-27 14:23:52.165657: step 50240/136300 (epoch 37/100), loss = 0.190561 (0.178 sec/batch), lr: 0.301327
2019-12-27 14:23:57.010586: step 50260/136300 (epoch 37/100), loss = 0.201853 (0.238 sec/batch), lr: 0.301327
2019-12-27 14:24:03.230827: step 50280/136300 (epoch 37/100), loss = 0.282108 (0.246 sec/batch), lr: 0.301327
2019-12-27 14:24:08.150215: step 50300/136300 (epoch 37/100), loss = 0.263604 (0.234 sec/batch), lr: 0.301327
2019-12-27 14:24:13.082684: step 50320/136300 (epoch 37/100), loss = 0.157851 (0.226 sec/batch), lr: 0.301327
2019-12-27 14:24:17.697459: step 50340/136300 (epoch 37/100), loss = 0.464061 (0.221 sec/batch), lr: 0.301327
2019-12-27 14:24:22.550226: step 50360/136300 (epoch 37/100), loss = 0.191371 (0.232 sec/batch), lr: 0.301327
2019-12-27 14:24:27.338028: step 50380/136300 (epoch 37/100), loss = 0.263987 (0.234 sec/batch), lr: 0.301327
2019-12-27 14:24:32.264867: step 50400/136300 (epoch 37/100), loss = 0.250202 (0.204 sec/batch), lr: 0.301327
2019-12-27 14:24:37.130267: step 50420/136300 (epoch 37/100), loss = 0.139516 (0.233 sec/batch), lr: 0.301327
Evaluating on dev set...
Precision (micro): 70.004%
   Recall (micro): 62.123%
       F1 (micro): 65.828%
epoch 37: train_loss = 0.307309, dev_loss = 0.433266, dev_f1 = 0.6583
model saved to ./saved_models/01/checkpoint_epoch_37.pt

2019-12-27 14:25:15.530382: step 50440/136300 (epoch 38/100), loss = 0.342558 (0.220 sec/batch), lr: 0.271194
2019-12-27 14:25:21.705749: step 50460/136300 (epoch 38/100), loss = 0.271039 (0.241 sec/batch), lr: 0.271194
2019-12-27 14:25:26.540490: step 50480/136300 (epoch 38/100), loss = 0.473614 (0.216 sec/batch), lr: 0.271194
2019-12-27 14:25:31.253889: step 50500/136300 (epoch 38/100), loss = 0.238415 (0.234 sec/batch), lr: 0.271194
2019-12-27 14:25:36.209856: step 50520/136300 (epoch 38/100), loss = 0.353495 (0.244 sec/batch), lr: 0.271194
2019-12-27 14:25:41.047769: step 50540/136300 (epoch 38/100), loss = 0.491062 (0.246 sec/batch), lr: 0.271194
2019-12-27 14:25:45.758542: step 50560/136300 (epoch 38/100), loss = 0.397258 (0.220 sec/batch), lr: 0.271194
2019-12-27 14:25:50.661791: step 50580/136300 (epoch 38/100), loss = 0.236888 (0.234 sec/batch), lr: 0.271194
2019-12-27 14:25:55.456927: step 50600/136300 (epoch 38/100), loss = 0.366331 (0.234 sec/batch), lr: 0.271194
2019-12-27 14:26:00.368918: step 50620/136300 (epoch 38/100), loss = 0.274280 (0.247 sec/batch), lr: 0.271194
2019-12-27 14:26:05.153843: step 50640/136300 (epoch 38/100), loss = 0.214839 (0.175 sec/batch), lr: 0.271194
2019-12-27 14:26:11.430317: step 50660/136300 (epoch 38/100), loss = 0.166157 (0.214 sec/batch), lr: 0.271194
2019-12-27 14:26:16.278401: step 50680/136300 (epoch 38/100), loss = 0.177783 (0.202 sec/batch), lr: 0.271194
2019-12-27 14:26:21.134519: step 50700/136300 (epoch 38/100), loss = 0.232891 (0.237 sec/batch), lr: 0.271194
2019-12-27 14:26:25.936155: step 50720/136300 (epoch 38/100), loss = 0.279118 (0.237 sec/batch), lr: 0.271194
2019-12-27 14:26:30.694630: step 50740/136300 (epoch 38/100), loss = 0.386500 (0.172 sec/batch), lr: 0.271194
2019-12-27 14:26:35.510561: step 50760/136300 (epoch 38/100), loss = 0.232527 (0.235 sec/batch), lr: 0.271194
2019-12-27 14:26:40.330322: step 50780/136300 (epoch 38/100), loss = 0.307964 (0.248 sec/batch), lr: 0.271194
2019-12-27 14:26:45.227641: step 50800/136300 (epoch 38/100), loss = 0.397853 (0.242 sec/batch), lr: 0.271194
2019-12-27 14:26:49.924542: step 50820/136300 (epoch 38/100), loss = 0.351907 (0.219 sec/batch), lr: 0.271194
2019-12-27 14:26:56.008171: step 50840/136300 (epoch 38/100), loss = 0.291624 (1.605 sec/batch), lr: 0.271194
2019-12-27 14:27:00.747117: step 50860/136300 (epoch 38/100), loss = 0.213563 (0.233 sec/batch), lr: 0.271194
2019-12-27 14:27:05.693565: step 50880/136300 (epoch 38/100), loss = 0.363360 (0.229 sec/batch), lr: 0.271194
2019-12-27 14:27:10.565468: step 50900/136300 (epoch 38/100), loss = 0.455907 (0.237 sec/batch), lr: 0.271194
2019-12-27 14:27:15.354564: step 50920/136300 (epoch 38/100), loss = 0.451559 (0.201 sec/batch), lr: 0.271194
2019-12-27 14:27:20.233909: step 50940/136300 (epoch 38/100), loss = 0.461430 (0.212 sec/batch), lr: 0.271194
2019-12-27 14:27:25.107338: step 50960/136300 (epoch 38/100), loss = 0.388546 (0.240 sec/batch), lr: 0.271194
2019-12-27 14:27:29.986573: step 50980/136300 (epoch 38/100), loss = 0.349557 (0.182 sec/batch), lr: 0.271194
2019-12-27 14:27:34.850452: step 51000/136300 (epoch 38/100), loss = 0.277378 (0.242 sec/batch), lr: 0.271194
2019-12-27 14:27:39.653060: step 51020/136300 (epoch 38/100), loss = 0.352437 (0.232 sec/batch), lr: 0.271194
2019-12-27 14:27:45.744249: step 51040/136300 (epoch 38/100), loss = 0.415381 (0.175 sec/batch), lr: 0.271194
2019-12-27 14:27:50.569855: step 51060/136300 (epoch 38/100), loss = 0.260245 (0.216 sec/batch), lr: 0.271194
2019-12-27 14:27:55.336236: step 51080/136300 (epoch 38/100), loss = 0.239942 (0.222 sec/batch), lr: 0.271194
2019-12-27 14:28:00.161951: step 51100/136300 (epoch 38/100), loss = 0.224484 (0.236 sec/batch), lr: 0.271194
2019-12-27 14:28:05.054636: step 51120/136300 (epoch 38/100), loss = 0.361721 (0.214 sec/batch), lr: 0.271194
2019-12-27 14:28:09.962414: step 51140/136300 (epoch 38/100), loss = 0.480401 (0.222 sec/batch), lr: 0.271194
2019-12-27 14:28:14.871266: step 51160/136300 (epoch 38/100), loss = 0.239830 (0.220 sec/batch), lr: 0.271194
2019-12-27 14:28:19.768046: step 51180/136300 (epoch 38/100), loss = 0.174972 (0.235 sec/batch), lr: 0.271194
2019-12-27 14:28:24.466695: step 51200/136300 (epoch 38/100), loss = 0.145233 (0.232 sec/batch), lr: 0.271194
2019-12-27 14:28:29.365720: step 51220/136300 (epoch 38/100), loss = 0.360093 (0.205 sec/batch), lr: 0.271194
2019-12-27 14:28:35.710391: step 51240/136300 (epoch 38/100), loss = 0.252327 (0.182 sec/batch), lr: 0.271194
2019-12-27 14:28:40.519401: step 51260/136300 (epoch 38/100), loss = 0.276573 (0.247 sec/batch), lr: 0.271194
2019-12-27 14:28:45.478160: step 51280/136300 (epoch 38/100), loss = 0.330082 (0.238 sec/batch), lr: 0.271194
2019-12-27 14:28:50.152515: step 51300/136300 (epoch 38/100), loss = 0.239993 (0.181 sec/batch), lr: 0.271194
2019-12-27 14:28:55.011699: step 51320/136300 (epoch 38/100), loss = 0.709167 (0.231 sec/batch), lr: 0.271194
2019-12-27 14:28:59.943911: step 51340/136300 (epoch 38/100), loss = 0.390898 (0.244 sec/batch), lr: 0.271194
2019-12-27 14:29:04.758005: step 51360/136300 (epoch 38/100), loss = 0.312250 (0.224 sec/batch), lr: 0.271194
2019-12-27 14:29:09.674557: step 51380/136300 (epoch 38/100), loss = 0.283647 (0.248 sec/batch), lr: 0.271194
2019-12-27 14:29:14.628778: step 51400/136300 (epoch 38/100), loss = 0.192300 (0.230 sec/batch), lr: 0.271194
2019-12-27 14:29:19.429151: step 51420/136300 (epoch 38/100), loss = 0.266098 (0.226 sec/batch), lr: 0.271194
2019-12-27 14:29:25.776903: step 51440/136300 (epoch 38/100), loss = 0.287373 (0.242 sec/batch), lr: 0.271194
2019-12-27 14:29:30.705936: step 51460/136300 (epoch 38/100), loss = 0.497096 (0.236 sec/batch), lr: 0.271194
2019-12-27 14:29:35.322973: step 51480/136300 (epoch 38/100), loss = 0.331287 (0.234 sec/batch), lr: 0.271194
2019-12-27 14:29:40.046577: step 51500/136300 (epoch 38/100), loss = 0.289743 (0.239 sec/batch), lr: 0.271194
2019-12-27 14:29:44.906569: step 51520/136300 (epoch 38/100), loss = 0.288463 (0.213 sec/batch), lr: 0.271194
2019-12-27 14:29:49.705915: step 51540/136300 (epoch 38/100), loss = 0.425872 (0.229 sec/batch), lr: 0.271194
2019-12-27 14:29:54.619376: step 51560/136300 (epoch 38/100), loss = 0.227888 (0.233 sec/batch), lr: 0.271194
2019-12-27 14:29:59.396919: step 51580/136300 (epoch 38/100), loss = 0.577414 (0.239 sec/batch), lr: 0.271194
2019-12-27 14:30:04.103151: step 51600/136300 (epoch 38/100), loss = 0.107806 (0.202 sec/batch), lr: 0.271194
2019-12-27 14:30:08.886407: step 51620/136300 (epoch 38/100), loss = 0.459962 (0.210 sec/batch), lr: 0.271194
2019-12-27 14:30:15.243610: step 51640/136300 (epoch 38/100), loss = 0.316139 (0.241 sec/batch), lr: 0.271194
2019-12-27 14:30:20.181774: step 51660/136300 (epoch 38/100), loss = 0.313996 (0.220 sec/batch), lr: 0.271194
2019-12-27 14:30:25.083500: step 51680/136300 (epoch 38/100), loss = 0.307160 (0.238 sec/batch), lr: 0.271194
2019-12-27 14:30:29.766917: step 51700/136300 (epoch 38/100), loss = 0.105161 (0.205 sec/batch), lr: 0.271194
2019-12-27 14:30:34.592281: step 51720/136300 (epoch 38/100), loss = 0.166359 (0.230 sec/batch), lr: 0.271194
2019-12-27 14:30:39.344336: step 51740/136300 (epoch 38/100), loss = 0.221212 (0.244 sec/batch), lr: 0.271194
2019-12-27 14:30:44.274520: step 51760/136300 (epoch 38/100), loss = 0.190555 (0.209 sec/batch), lr: 0.271194
2019-12-27 14:30:49.143146: step 51780/136300 (epoch 38/100), loss = 0.330989 (0.230 sec/batch), lr: 0.271194
Evaluating on dev set...
Precision (micro): 72.887%
   Recall (micro): 58.057%
       F1 (micro): 64.632%
epoch 38: train_loss = 0.300482, dev_loss = 0.441515, dev_f1 = 0.6463
model saved to ./saved_models/01/checkpoint_epoch_38.pt

2019-12-27 14:31:27.566492: step 51800/136300 (epoch 39/100), loss = 0.175746 (0.246 sec/batch), lr: 0.244075
2019-12-27 14:31:33.625194: step 51820/136300 (epoch 39/100), loss = 0.256552 (0.210 sec/batch), lr: 0.244075
2019-12-27 14:31:38.470210: step 51840/136300 (epoch 39/100), loss = 0.319839 (0.235 sec/batch), lr: 0.244075
2019-12-27 14:31:43.224765: step 51860/136300 (epoch 39/100), loss = 0.257124 (0.228 sec/batch), lr: 0.244075
2019-12-27 14:31:48.153365: step 51880/136300 (epoch 39/100), loss = 0.237968 (0.230 sec/batch), lr: 0.244075
2019-12-27 14:31:52.982799: step 51900/136300 (epoch 39/100), loss = 0.190992 (0.247 sec/batch), lr: 0.244075
2019-12-27 14:31:57.729201: step 51920/136300 (epoch 39/100), loss = 0.259262 (0.208 sec/batch), lr: 0.244075
2019-12-27 14:32:02.607163: step 51940/136300 (epoch 39/100), loss = 0.310723 (0.206 sec/batch), lr: 0.244075
2019-12-27 14:32:07.451921: step 51960/136300 (epoch 39/100), loss = 0.246824 (0.234 sec/batch), lr: 0.244075
2019-12-27 14:32:12.347083: step 51980/136300 (epoch 39/100), loss = 0.367311 (0.225 sec/batch), lr: 0.244075
2019-12-27 14:32:17.163831: step 52000/136300 (epoch 39/100), loss = 0.215000 (0.210 sec/batch), lr: 0.244075
2019-12-27 14:32:23.341326: step 52020/136300 (epoch 39/100), loss = 0.187707 (0.207 sec/batch), lr: 0.244075
2019-12-27 14:32:28.200007: step 52040/136300 (epoch 39/100), loss = 0.111972 (0.238 sec/batch), lr: 0.244075
2019-12-27 14:32:32.994301: step 52060/136300 (epoch 39/100), loss = 0.333418 (0.245 sec/batch), lr: 0.244075
2019-12-27 14:32:37.801921: step 52080/136300 (epoch 39/100), loss = 0.253863 (0.237 sec/batch), lr: 0.244075
2019-12-27 14:32:42.652132: step 52100/136300 (epoch 39/100), loss = 0.356318 (0.235 sec/batch), lr: 0.244075
2019-12-27 14:32:47.393937: step 52120/136300 (epoch 39/100), loss = 0.510154 (0.232 sec/batch), lr: 0.244075
2019-12-27 14:32:52.209277: step 52140/136300 (epoch 39/100), loss = 0.105528 (0.240 sec/batch), lr: 0.244075
2019-12-27 14:32:57.135556: step 52160/136300 (epoch 39/100), loss = 0.345983 (0.246 sec/batch), lr: 0.244075
2019-12-27 14:33:01.869547: step 52180/136300 (epoch 39/100), loss = 0.124657 (0.214 sec/batch), lr: 0.244075
2019-12-27 14:33:06.505960: step 52200/136300 (epoch 39/100), loss = 0.157272 (0.221 sec/batch), lr: 0.244075
2019-12-27 14:33:12.753216: step 52220/136300 (epoch 39/100), loss = 0.572539 (0.230 sec/batch), lr: 0.244075
2019-12-27 14:33:17.723908: step 52240/136300 (epoch 39/100), loss = 0.237150 (0.226 sec/batch), lr: 0.244075
2019-12-27 14:33:22.672424: step 52260/136300 (epoch 39/100), loss = 0.329007 (0.234 sec/batch), lr: 0.244075
2019-12-27 14:33:27.481998: step 52280/136300 (epoch 39/100), loss = 0.244611 (0.179 sec/batch), lr: 0.244075
2019-12-27 14:33:32.376198: step 52300/136300 (epoch 39/100), loss = 0.454495 (0.203 sec/batch), lr: 0.244075
2019-12-27 14:33:37.270639: step 52320/136300 (epoch 39/100), loss = 0.291771 (0.237 sec/batch), lr: 0.244075
2019-12-27 14:33:42.242282: step 52340/136300 (epoch 39/100), loss = 0.334184 (0.228 sec/batch), lr: 0.244075
2019-12-27 14:33:47.068997: step 52360/136300 (epoch 39/100), loss = 0.094110 (0.254 sec/batch), lr: 0.244075
2019-12-27 14:33:51.858004: step 52380/136300 (epoch 39/100), loss = 0.317452 (0.229 sec/batch), lr: 0.244075
2019-12-27 14:33:56.644879: step 52400/136300 (epoch 39/100), loss = 0.448952 (0.202 sec/batch), lr: 0.244075
2019-12-27 14:34:02.959742: step 52420/136300 (epoch 39/100), loss = 0.177453 (0.206 sec/batch), lr: 0.244075
2019-12-27 14:34:07.749262: step 52440/136300 (epoch 39/100), loss = 0.224681 (0.239 sec/batch), lr: 0.244075
2019-12-27 14:34:12.531245: step 52460/136300 (epoch 39/100), loss = 0.440649 (0.233 sec/batch), lr: 0.244075
2019-12-27 14:34:17.453352: step 52480/136300 (epoch 39/100), loss = 0.266205 (0.236 sec/batch), lr: 0.244075
2019-12-27 14:34:22.398011: step 52500/136300 (epoch 39/100), loss = 0.281975 (0.210 sec/batch), lr: 0.244075
2019-12-27 14:34:27.328306: step 52520/136300 (epoch 39/100), loss = 0.230340 (0.228 sec/batch), lr: 0.244075
2019-12-27 14:34:32.278567: step 52540/136300 (epoch 39/100), loss = 0.215212 (0.234 sec/batch), lr: 0.244075
2019-12-27 14:34:37.063241: step 52560/136300 (epoch 39/100), loss = 0.325559 (0.234 sec/batch), lr: 0.244075
2019-12-27 14:34:41.895199: step 52580/136300 (epoch 39/100), loss = 0.316762 (0.250 sec/batch), lr: 0.244075
2019-12-27 14:34:48.461512: step 52600/136300 (epoch 39/100), loss = 0.236349 (0.243 sec/batch), lr: 0.244075
2019-12-27 14:34:53.228031: step 52620/136300 (epoch 39/100), loss = 0.567365 (0.208 sec/batch), lr: 0.244075
2019-12-27 14:34:58.227521: step 52640/136300 (epoch 39/100), loss = 0.225559 (0.242 sec/batch), lr: 0.244075
2019-12-27 14:35:02.956863: step 52660/136300 (epoch 39/100), loss = 0.324549 (0.211 sec/batch), lr: 0.244075
2019-12-27 14:35:07.758695: step 52680/136300 (epoch 39/100), loss = 0.225760 (0.233 sec/batch), lr: 0.244075
2019-12-27 14:35:12.690199: step 52700/136300 (epoch 39/100), loss = 0.405558 (0.243 sec/batch), lr: 0.244075
2019-12-27 14:35:17.620516: step 52720/136300 (epoch 39/100), loss = 0.177542 (0.221 sec/batch), lr: 0.244075
2019-12-27 14:35:22.437307: step 52740/136300 (epoch 39/100), loss = 0.427615 (0.221 sec/batch), lr: 0.244075
2019-12-27 14:35:27.428093: step 52760/136300 (epoch 39/100), loss = 0.379746 (0.232 sec/batch), lr: 0.244075
2019-12-27 14:35:32.297125: step 52780/136300 (epoch 39/100), loss = 0.358269 (0.208 sec/batch), lr: 0.244075
2019-12-27 14:35:38.729666: step 52800/136300 (epoch 39/100), loss = 0.416684 (0.237 sec/batch), lr: 0.244075
2019-12-27 14:35:43.636970: step 52820/136300 (epoch 39/100), loss = 0.186692 (0.237 sec/batch), lr: 0.244075
2019-12-27 14:35:48.280276: step 52840/136300 (epoch 39/100), loss = 0.317031 (0.226 sec/batch), lr: 0.244075
2019-12-27 14:35:53.065757: step 52860/136300 (epoch 39/100), loss = 0.417045 (0.236 sec/batch), lr: 0.244075
2019-12-27 14:35:57.899914: step 52880/136300 (epoch 39/100), loss = 0.223193 (0.207 sec/batch), lr: 0.244075
2019-12-27 14:36:02.719142: step 52900/136300 (epoch 39/100), loss = 0.127429 (0.227 sec/batch), lr: 0.244075
2019-12-27 14:36:07.647819: step 52920/136300 (epoch 39/100), loss = 0.350316 (0.225 sec/batch), lr: 0.244075
2019-12-27 14:36:12.386724: step 52940/136300 (epoch 39/100), loss = 0.280907 (0.204 sec/batch), lr: 0.244075
2019-12-27 14:36:17.162797: step 52960/136300 (epoch 39/100), loss = 0.208114 (0.226 sec/batch), lr: 0.244075
2019-12-27 14:36:21.970683: step 52980/136300 (epoch 39/100), loss = 0.218718 (0.247 sec/batch), lr: 0.244075
2019-12-27 14:36:28.476016: step 53000/136300 (epoch 39/100), loss = 0.319160 (0.246 sec/batch), lr: 0.244075
2019-12-27 14:36:33.412319: step 53020/136300 (epoch 39/100), loss = 0.151810 (0.249 sec/batch), lr: 0.244075
2019-12-27 14:36:38.276893: step 53040/136300 (epoch 39/100), loss = 0.288610 (0.236 sec/batch), lr: 0.244075
2019-12-27 14:36:43.064460: step 53060/136300 (epoch 39/100), loss = 0.276200 (0.217 sec/batch), lr: 0.244075
2019-12-27 14:36:47.815555: step 53080/136300 (epoch 39/100), loss = 0.297719 (0.232 sec/batch), lr: 0.244075
2019-12-27 14:36:52.589424: step 53100/136300 (epoch 39/100), loss = 0.321848 (0.244 sec/batch), lr: 0.244075
2019-12-27 14:36:57.478574: step 53120/136300 (epoch 39/100), loss = 0.378979 (0.245 sec/batch), lr: 0.244075
2019-12-27 14:37:02.386531: step 53140/136300 (epoch 39/100), loss = 0.332681 (0.237 sec/batch), lr: 0.244075
Evaluating on dev set...
Precision (micro): 71.456%
   Recall (micro): 60.559%
       F1 (micro): 65.558%
epoch 39: train_loss = 0.294974, dev_loss = 0.438459, dev_f1 = 0.6556
model saved to ./saved_models/01/checkpoint_epoch_39.pt

2019-12-27 14:37:41.271893: step 53160/136300 (epoch 40/100), loss = 0.332969 (0.215 sec/batch), lr: 0.244075
2019-12-27 14:37:47.560289: step 53180/136300 (epoch 40/100), loss = 0.310243 (0.214 sec/batch), lr: 0.244075
2019-12-27 14:37:52.366366: step 53200/136300 (epoch 40/100), loss = 0.445645 (0.241 sec/batch), lr: 0.244075
2019-12-27 14:37:57.165408: step 53220/136300 (epoch 40/100), loss = 0.267121 (0.203 sec/batch), lr: 0.244075
2019-12-27 14:38:02.068563: step 53240/136300 (epoch 40/100), loss = 0.408767 (0.233 sec/batch), lr: 0.244075
2019-12-27 14:38:06.864481: step 53260/136300 (epoch 40/100), loss = 0.229456 (0.214 sec/batch), lr: 0.244075
2019-12-27 14:38:11.671351: step 53280/136300 (epoch 40/100), loss = 0.213687 (0.232 sec/batch), lr: 0.244075
2019-12-27 14:38:16.538290: step 53300/136300 (epoch 40/100), loss = 0.395223 (0.239 sec/batch), lr: 0.244075
2019-12-27 14:38:21.350049: step 53320/136300 (epoch 40/100), loss = 0.289684 (0.225 sec/batch), lr: 0.244075
2019-12-27 14:38:26.241873: step 53340/136300 (epoch 40/100), loss = 0.238361 (0.221 sec/batch), lr: 0.244075
2019-12-27 14:38:31.111658: step 53360/136300 (epoch 40/100), loss = 0.485785 (0.248 sec/batch), lr: 0.244075
2019-12-27 14:38:37.585290: step 53380/136300 (epoch 40/100), loss = 0.336415 (0.217 sec/batch), lr: 0.244075
2019-12-27 14:38:42.372112: step 53400/136300 (epoch 40/100), loss = 0.378062 (0.239 sec/batch), lr: 0.244075
2019-12-27 14:38:47.151587: step 53420/136300 (epoch 40/100), loss = 0.161595 (0.204 sec/batch), lr: 0.244075
2019-12-27 14:38:51.997657: step 53440/136300 (epoch 40/100), loss = 0.222682 (0.241 sec/batch), lr: 0.244075
2019-12-27 14:38:56.857101: step 53460/136300 (epoch 40/100), loss = 0.508257 (0.233 sec/batch), lr: 0.244075
2019-12-27 14:39:01.589806: step 53480/136300 (epoch 40/100), loss = 0.265597 (0.185 sec/batch), lr: 0.244075
2019-12-27 14:39:06.361605: step 53500/136300 (epoch 40/100), loss = 0.181691 (0.217 sec/batch), lr: 0.244075
2019-12-27 14:39:11.256721: step 53520/136300 (epoch 40/100), loss = 0.337653 (0.216 sec/batch), lr: 0.244075
2019-12-27 14:39:16.017258: step 53540/136300 (epoch 40/100), loss = 0.269563 (0.232 sec/batch), lr: 0.244075
2019-12-27 14:39:20.741439: step 53560/136300 (epoch 40/100), loss = 0.160009 (0.216 sec/batch), lr: 0.244075
2019-12-27 14:39:27.086270: step 53580/136300 (epoch 40/100), loss = 0.389038 (0.217 sec/batch), lr: 0.244075
2019-12-27 14:39:32.001561: step 53600/136300 (epoch 40/100), loss = 0.289620 (0.242 sec/batch), lr: 0.244075
2019-12-27 14:39:36.872123: step 53620/136300 (epoch 40/100), loss = 0.399504 (0.222 sec/batch), lr: 0.244075
2019-12-27 14:39:41.662399: step 53640/136300 (epoch 40/100), loss = 0.163898 (0.240 sec/batch), lr: 0.244075
2019-12-27 14:39:46.450697: step 53660/136300 (epoch 40/100), loss = 0.180812 (0.235 sec/batch), lr: 0.244075
2019-12-27 14:39:51.282350: step 53680/136300 (epoch 40/100), loss = 0.363083 (0.230 sec/batch), lr: 0.244075
2019-12-27 14:39:56.244547: step 53700/136300 (epoch 40/100), loss = 0.235465 (0.225 sec/batch), lr: 0.244075
2019-12-27 14:40:01.050805: step 53720/136300 (epoch 40/100), loss = 0.272084 (0.231 sec/batch), lr: 0.244075
2019-12-27 14:40:05.833214: step 53740/136300 (epoch 40/100), loss = 0.176220 (0.178 sec/batch), lr: 0.244075
2019-12-27 14:40:10.606658: step 53760/136300 (epoch 40/100), loss = 0.296701 (0.237 sec/batch), lr: 0.244075
2019-12-27 14:40:17.230027: step 53780/136300 (epoch 40/100), loss = 0.181286 (0.234 sec/batch), lr: 0.244075
2019-12-27 14:40:22.022132: step 53800/136300 (epoch 40/100), loss = 0.336847 (0.221 sec/batch), lr: 0.244075
2019-12-27 14:40:26.822014: step 53820/136300 (epoch 40/100), loss = 0.374150 (0.221 sec/batch), lr: 0.244075
2019-12-27 14:40:31.719989: step 53840/136300 (epoch 40/100), loss = 0.193872 (0.231 sec/batch), lr: 0.244075
2019-12-27 14:40:36.674499: step 53860/136300 (epoch 40/100), loss = 0.304442 (0.243 sec/batch), lr: 0.244075
2019-12-27 14:40:41.623902: step 53880/136300 (epoch 40/100), loss = 0.217515 (0.241 sec/batch), lr: 0.244075
2019-12-27 14:40:46.518644: step 53900/136300 (epoch 40/100), loss = 0.381135 (0.231 sec/batch), lr: 0.244075
2019-12-27 14:40:51.303995: step 53920/136300 (epoch 40/100), loss = 0.317404 (0.238 sec/batch), lr: 0.244075
2019-12-27 14:40:56.150459: step 53940/136300 (epoch 40/100), loss = 0.271496 (0.241 sec/batch), lr: 0.244075
2019-12-27 14:41:02.740768: step 53960/136300 (epoch 40/100), loss = 0.223525 (0.249 sec/batch), lr: 0.244075
2019-12-27 14:41:07.526468: step 53980/136300 (epoch 40/100), loss = 0.456554 (0.250 sec/batch), lr: 0.244075
2019-12-27 14:41:12.455800: step 54000/136300 (epoch 40/100), loss = 0.605112 (0.239 sec/batch), lr: 0.244075
2019-12-27 14:41:17.289495: step 54020/136300 (epoch 40/100), loss = 0.413125 (0.214 sec/batch), lr: 0.244075
2019-12-27 14:41:22.017287: step 54040/136300 (epoch 40/100), loss = 0.188357 (0.219 sec/batch), lr: 0.244075
2019-12-27 14:41:26.916769: step 54060/136300 (epoch 40/100), loss = 0.193966 (0.237 sec/batch), lr: 0.244075
2019-12-27 14:41:31.868722: step 54080/136300 (epoch 40/100), loss = 0.122807 (0.249 sec/batch), lr: 0.244075
2019-12-27 14:41:36.682965: step 54100/136300 (epoch 40/100), loss = 0.330585 (0.243 sec/batch), lr: 0.244075
2019-12-27 14:41:41.634149: step 54120/136300 (epoch 40/100), loss = 0.258418 (0.180 sec/batch), lr: 0.244075
2019-12-27 14:41:46.571454: step 54140/136300 (epoch 40/100), loss = 0.169395 (0.237 sec/batch), lr: 0.244075
2019-12-27 14:41:52.853105: step 54160/136300 (epoch 40/100), loss = 0.437295 (0.200 sec/batch), lr: 0.244075
2019-12-27 14:41:57.729669: step 54180/136300 (epoch 40/100), loss = 0.225333 (0.237 sec/batch), lr: 0.244075
2019-12-27 14:42:02.387393: step 54200/136300 (epoch 40/100), loss = 0.476005 (0.213 sec/batch), lr: 0.244075
2019-12-27 14:42:07.165681: step 54220/136300 (epoch 40/100), loss = 0.320990 (0.238 sec/batch), lr: 0.244075
2019-12-27 14:42:12.025131: step 54240/136300 (epoch 40/100), loss = 0.237428 (0.245 sec/batch), lr: 0.244075
2019-12-27 14:42:16.770520: step 54260/136300 (epoch 40/100), loss = 0.360428 (0.218 sec/batch), lr: 0.244075
2019-12-27 14:42:21.678923: step 54280/136300 (epoch 40/100), loss = 0.253558 (0.221 sec/batch), lr: 0.244075
2019-12-27 14:42:26.503388: step 54300/136300 (epoch 40/100), loss = 0.288953 (0.183 sec/batch), lr: 0.244075
2019-12-27 14:42:31.295613: step 54320/136300 (epoch 40/100), loss = 0.164065 (0.218 sec/batch), lr: 0.244075
2019-12-27 14:42:36.002583: step 54340/136300 (epoch 40/100), loss = 0.322781 (0.166 sec/batch), lr: 0.244075
2019-12-27 14:42:42.267500: step 54360/136300 (epoch 40/100), loss = 0.313070 (0.236 sec/batch), lr: 0.244075
2019-12-27 14:42:47.165961: step 54380/136300 (epoch 40/100), loss = 0.360934 (0.232 sec/batch), lr: 0.244075
2019-12-27 14:42:52.071667: step 54400/136300 (epoch 40/100), loss = 0.385324 (0.207 sec/batch), lr: 0.244075
2019-12-27 14:42:56.901141: step 54420/136300 (epoch 40/100), loss = 0.417928 (0.222 sec/batch), lr: 0.244075
2019-12-27 14:43:01.617221: step 54440/136300 (epoch 40/100), loss = 0.297761 (0.220 sec/batch), lr: 0.244075
2019-12-27 14:43:06.385637: step 54460/136300 (epoch 40/100), loss = 0.355268 (0.206 sec/batch), lr: 0.244075
2019-12-27 14:43:11.269471: step 54480/136300 (epoch 40/100), loss = 0.183889 (0.234 sec/batch), lr: 0.244075
2019-12-27 14:43:16.197797: step 54500/136300 (epoch 40/100), loss = 0.172361 (0.241 sec/batch), lr: 0.244075
2019-12-27 14:43:20.981010: step 54520/136300 (epoch 40/100), loss = 0.065976 (0.131 sec/batch), lr: 0.244075
Evaluating on dev set...
Precision (micro): 71.601%
   Recall (micro): 60.946%
       F1 (micro): 65.845%
epoch 40: train_loss = 0.293898, dev_loss = 0.437918, dev_f1 = 0.6585
model saved to ./saved_models/01/checkpoint_epoch_40.pt

2019-12-27 14:44:01.252703: step 54540/136300 (epoch 41/100), loss = 0.262038 (0.220 sec/batch), lr: 0.244075
2019-12-27 14:44:05.984942: step 54560/136300 (epoch 41/100), loss = 0.119853 (0.236 sec/batch), lr: 0.244075
2019-12-27 14:44:10.818700: step 54580/136300 (epoch 41/100), loss = 0.171442 (0.220 sec/batch), lr: 0.244075
2019-12-27 14:44:15.698793: step 54600/136300 (epoch 41/100), loss = 0.206027 (0.224 sec/batch), lr: 0.244075
2019-12-27 14:44:20.522825: step 54620/136300 (epoch 41/100), loss = 0.143405 (0.234 sec/batch), lr: 0.244075
2019-12-27 14:44:25.307510: step 54640/136300 (epoch 41/100), loss = 0.226434 (0.246 sec/batch), lr: 0.244075
2019-12-27 14:44:30.106661: step 54660/136300 (epoch 41/100), loss = 0.240800 (0.246 sec/batch), lr: 0.244075
2019-12-27 14:44:34.927702: step 54680/136300 (epoch 41/100), loss = 0.138763 (0.243 sec/batch), lr: 0.244075
2019-12-27 14:44:39.828753: step 54700/136300 (epoch 41/100), loss = 0.134426 (0.235 sec/batch), lr: 0.244075
2019-12-27 14:44:44.691777: step 54720/136300 (epoch 41/100), loss = 0.268116 (0.239 sec/batch), lr: 0.244075
2019-12-27 14:44:50.902887: step 54740/136300 (epoch 41/100), loss = 0.266402 (0.205 sec/batch), lr: 0.244075
2019-12-27 14:44:55.652110: step 54760/136300 (epoch 41/100), loss = 0.317266 (0.202 sec/batch), lr: 0.244075
2019-12-27 14:45:00.461418: step 54780/136300 (epoch 41/100), loss = 0.202745 (0.208 sec/batch), lr: 0.244075
2019-12-27 14:45:05.301340: step 54800/136300 (epoch 41/100), loss = 0.137818 (0.184 sec/batch), lr: 0.244075
2019-12-27 14:45:10.153905: step 54820/136300 (epoch 41/100), loss = 0.185869 (0.241 sec/batch), lr: 0.244075
2019-12-27 14:45:14.922217: step 54840/136300 (epoch 41/100), loss = 0.162220 (0.227 sec/batch), lr: 0.244075
2019-12-27 14:45:19.725631: step 54860/136300 (epoch 41/100), loss = 0.493599 (0.211 sec/batch), lr: 0.244075
2019-12-27 14:45:24.610446: step 54880/136300 (epoch 41/100), loss = 0.561570 (0.219 sec/batch), lr: 0.244075
2019-12-27 14:45:29.390165: step 54900/136300 (epoch 41/100), loss = 0.434822 (0.235 sec/batch), lr: 0.244075
2019-12-27 14:45:34.166617: step 54920/136300 (epoch 41/100), loss = 0.309293 (0.246 sec/batch), lr: 0.244075
2019-12-27 14:45:40.462648: step 54940/136300 (epoch 41/100), loss = 0.297366 (0.234 sec/batch), lr: 0.244075
2019-12-27 14:45:45.317702: step 54960/136300 (epoch 41/100), loss = 0.492735 (0.230 sec/batch), lr: 0.244075
2019-12-27 14:45:50.178332: step 54980/136300 (epoch 41/100), loss = 0.292432 (0.249 sec/batch), lr: 0.244075
2019-12-27 14:45:55.028274: step 55000/136300 (epoch 41/100), loss = 0.377645 (0.247 sec/batch), lr: 0.244075
2019-12-27 14:45:59.816486: step 55020/136300 (epoch 41/100), loss = 0.376791 (0.230 sec/batch), lr: 0.244075
2019-12-27 14:46:04.723766: step 55040/136300 (epoch 41/100), loss = 0.382540 (0.244 sec/batch), lr: 0.244075
2019-12-27 14:46:09.648324: step 55060/136300 (epoch 41/100), loss = 0.259036 (0.241 sec/batch), lr: 0.244075
2019-12-27 14:46:14.442314: step 55080/136300 (epoch 41/100), loss = 0.393808 (0.218 sec/batch), lr: 0.244075
2019-12-27 14:46:19.257966: step 55100/136300 (epoch 41/100), loss = 0.183165 (0.214 sec/batch), lr: 0.244075
2019-12-27 14:46:23.968905: step 55120/136300 (epoch 41/100), loss = 0.460561 (0.207 sec/batch), lr: 0.244075
2019-12-27 14:46:30.216456: step 55140/136300 (epoch 41/100), loss = 0.341873 (0.230 sec/batch), lr: 0.244075
2019-12-27 14:46:35.021077: step 55160/136300 (epoch 41/100), loss = 0.261449 (0.182 sec/batch), lr: 0.244075
2019-12-27 14:46:39.802181: step 55180/136300 (epoch 41/100), loss = 0.305418 (0.209 sec/batch), lr: 0.244075
2019-12-27 14:46:44.674303: step 55200/136300 (epoch 41/100), loss = 0.416032 (0.228 sec/batch), lr: 0.244075
2019-12-27 14:46:49.613786: step 55220/136300 (epoch 41/100), loss = 0.281444 (0.242 sec/batch), lr: 0.244075
2019-12-27 14:46:54.509112: step 55240/136300 (epoch 41/100), loss = 0.348391 (0.229 sec/batch), lr: 0.244075
2019-12-27 14:46:59.393676: step 55260/136300 (epoch 41/100), loss = 0.159203 (0.174 sec/batch), lr: 0.244075
2019-12-27 14:47:04.210737: step 55280/136300 (epoch 41/100), loss = 0.346616 (0.245 sec/batch), lr: 0.244075
2019-12-27 14:47:09.020745: step 55300/136300 (epoch 41/100), loss = 0.260352 (0.250 sec/batch), lr: 0.244075
2019-12-27 14:47:15.596226: step 55320/136300 (epoch 41/100), loss = 0.530588 (0.208 sec/batch), lr: 0.244075
2019-12-27 14:47:20.394288: step 55340/136300 (epoch 41/100), loss = 0.479535 (0.246 sec/batch), lr: 0.244075
2019-12-27 14:47:25.253973: step 55360/136300 (epoch 41/100), loss = 0.301914 (0.232 sec/batch), lr: 0.244075
2019-12-27 14:47:30.146880: step 55380/136300 (epoch 41/100), loss = 0.273247 (0.227 sec/batch), lr: 0.244075
2019-12-27 14:47:34.887015: step 55400/136300 (epoch 41/100), loss = 0.213404 (0.247 sec/batch), lr: 0.244075
2019-12-27 14:47:39.708956: step 55420/136300 (epoch 41/100), loss = 0.392807 (0.211 sec/batch), lr: 0.244075
2019-12-27 14:47:44.635899: step 55440/136300 (epoch 41/100), loss = 0.311724 (0.223 sec/batch), lr: 0.244075
2019-12-27 14:47:49.482806: step 55460/136300 (epoch 41/100), loss = 0.117906 (0.234 sec/batch), lr: 0.244075
2019-12-27 14:47:54.426382: step 55480/136300 (epoch 41/100), loss = 0.381916 (0.217 sec/batch), lr: 0.244075
2019-12-27 14:47:59.318273: step 55500/136300 (epoch 41/100), loss = 0.270075 (0.218 sec/batch), lr: 0.244075
2019-12-27 14:48:04.140694: step 55520/136300 (epoch 41/100), loss = 0.237638 (0.241 sec/batch), lr: 0.244075
2019-12-27 14:48:10.350449: step 55540/136300 (epoch 41/100), loss = 0.264024 (0.234 sec/batch), lr: 0.244075
2019-12-27 14:48:15.079598: step 55560/136300 (epoch 41/100), loss = 0.316609 (0.186 sec/batch), lr: 0.244075
2019-12-27 14:48:19.772157: step 55580/136300 (epoch 41/100), loss = 0.282607 (0.170 sec/batch), lr: 0.244075
2019-12-27 14:48:24.626180: step 55600/136300 (epoch 41/100), loss = 0.413359 (0.230 sec/batch), lr: 0.244075
2019-12-27 14:48:29.405480: step 55620/136300 (epoch 41/100), loss = 0.449174 (0.236 sec/batch), lr: 0.244075
2019-12-27 14:48:34.318436: step 55640/136300 (epoch 41/100), loss = 0.303989 (0.217 sec/batch), lr: 0.244075
2019-12-27 14:48:39.166436: step 55660/136300 (epoch 41/100), loss = 0.112931 (0.226 sec/batch), lr: 0.244075
2019-12-27 14:48:43.920593: step 55680/136300 (epoch 41/100), loss = 0.209148 (0.215 sec/batch), lr: 0.244075
2019-12-27 14:48:48.673219: step 55700/136300 (epoch 41/100), loss = 0.256955 (0.234 sec/batch), lr: 0.244075
2019-12-27 14:48:55.063223: step 55720/136300 (epoch 41/100), loss = 0.188547 (1.801 sec/batch), lr: 0.244075
2019-12-27 14:48:59.956701: step 55740/136300 (epoch 41/100), loss = 0.409654 (0.221 sec/batch), lr: 0.244075
2019-12-27 14:49:04.875247: step 55760/136300 (epoch 41/100), loss = 0.294704 (0.231 sec/batch), lr: 0.244075
2019-12-27 14:49:09.734913: step 55780/136300 (epoch 41/100), loss = 0.327832 (0.247 sec/batch), lr: 0.244075
2019-12-27 14:49:14.430963: step 55800/136300 (epoch 41/100), loss = 0.091356 (0.237 sec/batch), lr: 0.244075
2019-12-27 14:49:19.232412: step 55820/136300 (epoch 41/100), loss = 0.541330 (0.179 sec/batch), lr: 0.244075
2019-12-27 14:49:24.028441: step 55840/136300 (epoch 41/100), loss = 0.165192 (0.198 sec/batch), lr: 0.244075
2019-12-27 14:49:28.941105: step 55860/136300 (epoch 41/100), loss = 0.237402 (0.220 sec/batch), lr: 0.244075
2019-12-27 14:49:33.921118: step 55880/136300 (epoch 41/100), loss = 0.233900 (0.230 sec/batch), lr: 0.244075
Evaluating on dev set...
Precision (micro): 72.928%
   Recall (micro): 59.419%
       F1 (micro): 65.484%
epoch 41: train_loss = 0.291049, dev_loss = 0.446728, dev_f1 = 0.6548
model saved to ./saved_models/01/checkpoint_epoch_41.pt

2019-12-27 14:50:12.252828: step 55900/136300 (epoch 42/100), loss = 0.250874 (0.248 sec/batch), lr: 0.219667
2019-12-27 14:50:18.315700: step 55920/136300 (epoch 42/100), loss = 0.146415 (0.250 sec/batch), lr: 0.219667
2019-12-27 14:50:23.148997: step 55940/136300 (epoch 42/100), loss = 0.290288 (0.219 sec/batch), lr: 0.219667
2019-12-27 14:50:28.000798: step 55960/136300 (epoch 42/100), loss = 0.379087 (0.243 sec/batch), lr: 0.219667
2019-12-27 14:50:32.851650: step 55980/136300 (epoch 42/100), loss = 0.282351 (0.219 sec/batch), lr: 0.219667
2019-12-27 14:50:37.622863: step 56000/136300 (epoch 42/100), loss = 0.165665 (0.231 sec/batch), lr: 0.219667
2019-12-27 14:50:42.443044: step 56020/136300 (epoch 42/100), loss = 0.410963 (0.224 sec/batch), lr: 0.219667
2019-12-27 14:50:47.277475: step 56040/136300 (epoch 42/100), loss = 0.478667 (0.230 sec/batch), lr: 0.219667
2019-12-27 14:50:52.162057: step 56060/136300 (epoch 42/100), loss = 0.356549 (0.236 sec/batch), lr: 0.219667
2019-12-27 14:50:57.047101: step 56080/136300 (epoch 42/100), loss = 0.352133 (0.235 sec/batch), lr: 0.219667
2019-12-27 14:51:03.258279: step 56100/136300 (epoch 42/100), loss = 0.164848 (0.243 sec/batch), lr: 0.219667
2019-12-27 14:51:07.993887: step 56120/136300 (epoch 42/100), loss = 0.330267 (0.238 sec/batch), lr: 0.219667
2019-12-27 14:51:12.795081: step 56140/136300 (epoch 42/100), loss = 0.341043 (0.235 sec/batch), lr: 0.219667
2019-12-27 14:51:17.667613: step 56160/136300 (epoch 42/100), loss = 0.278111 (0.229 sec/batch), lr: 0.219667
2019-12-27 14:51:22.477786: step 56180/136300 (epoch 42/100), loss = 0.268385 (0.179 sec/batch), lr: 0.219667
2019-12-27 14:51:27.231210: step 56200/136300 (epoch 42/100), loss = 0.153092 (0.221 sec/batch), lr: 0.219667
2019-12-27 14:51:32.084288: step 56220/136300 (epoch 42/100), loss = 0.195547 (0.238 sec/batch), lr: 0.219667
2019-12-27 14:51:36.937019: step 56240/136300 (epoch 42/100), loss = 0.176759 (0.233 sec/batch), lr: 0.219667
2019-12-27 14:51:41.677797: step 56260/136300 (epoch 42/100), loss = 0.246994 (0.229 sec/batch), lr: 0.219667
2019-12-27 14:51:46.463612: step 56280/136300 (epoch 42/100), loss = 0.268410 (0.229 sec/batch), lr: 0.219667
2019-12-27 14:51:52.738556: step 56300/136300 (epoch 42/100), loss = 0.267437 (0.244 sec/batch), lr: 0.219667
2019-12-27 14:51:57.581527: step 56320/136300 (epoch 42/100), loss = 0.334563 (0.243 sec/batch), lr: 0.219667
2019-12-27 14:52:02.469636: step 56340/136300 (epoch 42/100), loss = 0.359987 (0.244 sec/batch), lr: 0.219667
2019-12-27 14:52:07.312456: step 56360/136300 (epoch 42/100), loss = 0.533441 (0.239 sec/batch), lr: 0.219667
2019-12-27 14:52:12.140976: step 56380/136300 (epoch 42/100), loss = 0.371713 (0.218 sec/batch), lr: 0.219667
2019-12-27 14:52:17.061809: step 56400/136300 (epoch 42/100), loss = 0.366413 (0.236 sec/batch), lr: 0.219667
2019-12-27 14:52:21.990568: step 56420/136300 (epoch 42/100), loss = 0.484710 (0.243 sec/batch), lr: 0.219667
2019-12-27 14:52:26.794353: step 56440/136300 (epoch 42/100), loss = 0.530184 (0.238 sec/batch), lr: 0.219667
2019-12-27 14:52:31.642412: step 56460/136300 (epoch 42/100), loss = 0.323654 (0.240 sec/batch), lr: 0.219667
2019-12-27 14:52:36.375273: step 56480/136300 (epoch 42/100), loss = 0.388691 (0.228 sec/batch), lr: 0.219667
2019-12-27 14:52:42.774741: step 56500/136300 (epoch 42/100), loss = 0.254822 (0.200 sec/batch), lr: 0.219667
2019-12-27 14:52:47.614323: step 56520/136300 (epoch 42/100), loss = 0.179969 (0.233 sec/batch), lr: 0.219667
2019-12-27 14:52:52.350490: step 56540/136300 (epoch 42/100), loss = 0.105300 (0.229 sec/batch), lr: 0.219667
2019-12-27 14:52:57.177122: step 56560/136300 (epoch 42/100), loss = 0.302261 (0.232 sec/batch), lr: 0.219667
2019-12-27 14:53:02.145900: step 56580/136300 (epoch 42/100), loss = 0.451020 (0.233 sec/batch), lr: 0.219667
2019-12-27 14:53:07.013831: step 56600/136300 (epoch 42/100), loss = 0.249443 (0.229 sec/batch), lr: 0.219667
2019-12-27 14:53:11.917235: step 56620/136300 (epoch 42/100), loss = 0.381443 (0.237 sec/batch), lr: 0.219667
2019-12-27 14:53:16.706044: step 56640/136300 (epoch 42/100), loss = 0.400851 (0.233 sec/batch), lr: 0.219667
2019-12-27 14:53:21.524333: step 56660/136300 (epoch 42/100), loss = 0.289529 (0.217 sec/batch), lr: 0.219667
2019-12-27 14:53:26.461550: step 56680/136300 (epoch 42/100), loss = 0.373396 (0.239 sec/batch), lr: 0.219667
2019-12-27 14:53:32.758736: step 56700/136300 (epoch 42/100), loss = 0.200016 (0.210 sec/batch), lr: 0.219667
2019-12-27 14:53:37.551031: step 56720/136300 (epoch 42/100), loss = 0.121670 (0.231 sec/batch), lr: 0.219667
2019-12-27 14:53:42.501553: step 56740/136300 (epoch 42/100), loss = 0.389249 (0.229 sec/batch), lr: 0.219667
2019-12-27 14:53:47.182438: step 56760/136300 (epoch 42/100), loss = 0.427571 (0.207 sec/batch), lr: 0.219667
2019-12-27 14:53:52.076715: step 56780/136300 (epoch 42/100), loss = 0.165184 (0.229 sec/batch), lr: 0.219667
2019-12-27 14:53:56.985535: step 56800/136300 (epoch 42/100), loss = 0.233213 (0.215 sec/batch), lr: 0.219667
2019-12-27 14:54:01.806319: step 56820/136300 (epoch 42/100), loss = 0.205336 (0.237 sec/batch), lr: 0.219667
2019-12-27 14:54:06.774220: step 56840/136300 (epoch 42/100), loss = 0.205895 (0.243 sec/batch), lr: 0.219667
2019-12-27 14:54:11.676600: step 56860/136300 (epoch 42/100), loss = 0.503795 (0.235 sec/batch), lr: 0.219667
2019-12-27 14:54:16.523237: step 56880/136300 (epoch 42/100), loss = 0.122797 (0.244 sec/batch), lr: 0.219667
2019-12-27 14:54:23.011399: step 56900/136300 (epoch 42/100), loss = 0.171335 (0.215 sec/batch), lr: 0.219667
2019-12-27 14:54:27.854335: step 56920/136300 (epoch 42/100), loss = 0.241320 (0.214 sec/batch), lr: 0.219667
2019-12-27 14:54:32.525722: step 56940/136300 (epoch 42/100), loss = 0.153955 (0.216 sec/batch), lr: 0.219667
2019-12-27 14:54:37.317051: step 56960/136300 (epoch 42/100), loss = 0.230306 (0.234 sec/batch), lr: 0.219667
2019-12-27 14:54:42.126396: step 56980/136300 (epoch 42/100), loss = 0.218122 (0.203 sec/batch), lr: 0.219667
2019-12-27 14:54:46.990780: step 57000/136300 (epoch 42/100), loss = 0.166734 (0.241 sec/batch), lr: 0.219667
2019-12-27 14:54:51.846365: step 57020/136300 (epoch 42/100), loss = 0.445315 (0.202 sec/batch), lr: 0.219667
2019-12-27 14:54:56.614169: step 57040/136300 (epoch 42/100), loss = 0.448802 (0.213 sec/batch), lr: 0.219667
2019-12-27 14:55:01.318482: step 57060/136300 (epoch 42/100), loss = 0.118192 (0.242 sec/batch), lr: 0.219667
2019-12-27 14:55:06.161256: step 57080/136300 (epoch 42/100), loss = 0.248151 (0.211 sec/batch), lr: 0.219667
2019-12-27 14:55:12.493993: step 57100/136300 (epoch 42/100), loss = 0.217100 (0.212 sec/batch), lr: 0.219667
2019-12-27 14:55:17.414605: step 57120/136300 (epoch 42/100), loss = 0.240406 (0.217 sec/batch), lr: 0.219667
2019-12-27 14:55:22.285248: step 57140/136300 (epoch 42/100), loss = 0.333523 (0.207 sec/batch), lr: 0.219667
2019-12-27 14:55:27.011517: step 57160/136300 (epoch 42/100), loss = 0.339738 (0.248 sec/batch), lr: 0.219667
2019-12-27 14:55:31.860576: step 57180/136300 (epoch 42/100), loss = 0.216925 (0.243 sec/batch), lr: 0.219667
2019-12-27 14:55:36.619247: step 57200/136300 (epoch 42/100), loss = 0.301496 (0.215 sec/batch), lr: 0.219667
2019-12-27 14:55:41.552068: step 57220/136300 (epoch 42/100), loss = 0.579070 (0.201 sec/batch), lr: 0.219667
2019-12-27 14:55:46.474790: step 57240/136300 (epoch 42/100), loss = 0.381495 (0.241 sec/batch), lr: 0.219667
Evaluating on dev set...
Precision (micro): 71.704%
   Recall (micro): 60.835%
       F1 (micro): 65.824%
epoch 42: train_loss = 0.286185, dev_loss = 0.441657, dev_f1 = 0.6582
model saved to ./saved_models/01/checkpoint_epoch_42.pt

2019-12-27 14:56:25.038549: step 57260/136300 (epoch 43/100), loss = 0.305515 (0.232 sec/batch), lr: 0.219667
2019-12-27 14:56:31.308505: step 57280/136300 (epoch 43/100), loss = 0.219730 (0.184 sec/batch), lr: 0.219667
2019-12-27 14:56:36.137868: step 57300/136300 (epoch 43/100), loss = 0.305981 (0.169 sec/batch), lr: 0.219667
2019-12-27 14:56:40.922111: step 57320/136300 (epoch 43/100), loss = 0.305824 (0.222 sec/batch), lr: 0.219667
2019-12-27 14:56:45.862282: step 57340/136300 (epoch 43/100), loss = 0.281330 (0.227 sec/batch), lr: 0.219667
2019-12-27 14:56:50.621739: step 57360/136300 (epoch 43/100), loss = 0.318481 (0.246 sec/batch), lr: 0.219667
2019-12-27 14:56:55.419718: step 57380/136300 (epoch 43/100), loss = 0.214695 (0.220 sec/batch), lr: 0.219667
2019-12-27 14:57:00.227315: step 57400/136300 (epoch 43/100), loss = 0.291978 (0.243 sec/batch), lr: 0.219667
2019-12-27 14:57:05.117792: step 57420/136300 (epoch 43/100), loss = 0.200006 (0.242 sec/batch), lr: 0.219667
2019-12-27 14:57:09.971221: step 57440/136300 (epoch 43/100), loss = 0.295648 (0.213 sec/batch), lr: 0.219667
2019-12-27 14:57:16.468821: step 57460/136300 (epoch 43/100), loss = 0.107098 (0.240 sec/batch), lr: 0.219667
2019-12-27 14:57:21.164514: step 57480/136300 (epoch 43/100), loss = 0.471080 (0.225 sec/batch), lr: 0.219667
2019-12-27 14:57:25.977141: step 57500/136300 (epoch 43/100), loss = 0.235171 (0.237 sec/batch), lr: 0.219667
2019-12-27 14:57:30.866403: step 57520/136300 (epoch 43/100), loss = 0.201343 (0.224 sec/batch), lr: 0.219667
2019-12-27 14:57:35.707473: step 57540/136300 (epoch 43/100), loss = 0.203583 (0.239 sec/batch), lr: 0.219667
2019-12-27 14:57:40.429343: step 57560/136300 (epoch 43/100), loss = 0.238279 (0.243 sec/batch), lr: 0.219667
2019-12-27 14:57:45.240191: step 57580/136300 (epoch 43/100), loss = 0.569746 (0.237 sec/batch), lr: 0.219667
2019-12-27 14:57:50.062930: step 57600/136300 (epoch 43/100), loss = 0.222248 (0.207 sec/batch), lr: 0.219667
2019-12-27 14:57:54.842716: step 57620/136300 (epoch 43/100), loss = 0.131109 (0.230 sec/batch), lr: 0.219667
2019-12-27 14:57:59.629501: step 57640/136300 (epoch 43/100), loss = 0.194989 (0.248 sec/batch), lr: 0.219667
2019-12-27 14:58:05.670443: step 57660/136300 (epoch 43/100), loss = 0.195695 (0.243 sec/batch), lr: 0.219667
2019-12-27 14:58:10.503729: step 57680/136300 (epoch 43/100), loss = 0.321808 (0.231 sec/batch), lr: 0.219667
2019-12-27 14:58:15.415868: step 57700/136300 (epoch 43/100), loss = 0.518446 (0.247 sec/batch), lr: 0.219667
2019-12-27 14:58:20.249145: step 57720/136300 (epoch 43/100), loss = 0.479275 (0.232 sec/batch), lr: 0.219667
2019-12-27 14:58:25.105829: step 57740/136300 (epoch 43/100), loss = 0.255559 (0.236 sec/batch), lr: 0.219667
2019-12-27 14:58:30.009619: step 57760/136300 (epoch 43/100), loss = 0.163658 (0.234 sec/batch), lr: 0.219667
2019-12-27 14:58:34.895269: step 57780/136300 (epoch 43/100), loss = 0.156394 (0.244 sec/batch), lr: 0.219667
2019-12-27 14:58:39.718384: step 57800/136300 (epoch 43/100), loss = 0.393312 (0.202 sec/batch), lr: 0.219667
2019-12-27 14:58:44.590448: step 57820/136300 (epoch 43/100), loss = 0.146134 (0.242 sec/batch), lr: 0.219667
2019-12-27 14:58:49.398916: step 57840/136300 (epoch 43/100), loss = 0.191075 (0.237 sec/batch), lr: 0.219667
2019-12-27 14:58:55.503245: step 57860/136300 (epoch 43/100), loss = 0.385226 (0.250 sec/batch), lr: 0.219667
2019-12-27 14:59:00.311362: step 57880/136300 (epoch 43/100), loss = 0.178651 (0.250 sec/batch), lr: 0.219667
2019-12-27 14:59:05.068704: step 57900/136300 (epoch 43/100), loss = 0.190567 (0.242 sec/batch), lr: 0.219667
2019-12-27 14:59:09.889934: step 57920/136300 (epoch 43/100), loss = 0.292499 (0.234 sec/batch), lr: 0.219667
2019-12-27 14:59:14.814600: step 57940/136300 (epoch 43/100), loss = 0.250623 (0.240 sec/batch), lr: 0.219667
2019-12-27 14:59:19.691494: step 57960/136300 (epoch 43/100), loss = 0.460122 (0.231 sec/batch), lr: 0.219667
2019-12-27 14:59:24.636496: step 57980/136300 (epoch 43/100), loss = 0.240674 (0.235 sec/batch), lr: 0.219667
2019-12-27 14:59:29.490756: step 58000/136300 (epoch 43/100), loss = 0.390494 (0.244 sec/batch), lr: 0.219667
2019-12-27 14:59:34.243045: step 58020/136300 (epoch 43/100), loss = 0.279154 (0.239 sec/batch), lr: 0.219667
2019-12-27 14:59:39.118500: step 58040/136300 (epoch 43/100), loss = 0.311694 (0.218 sec/batch), lr: 0.219667
2019-12-27 14:59:45.025330: step 58060/136300 (epoch 43/100), loss = 0.496241 (0.223 sec/batch), lr: 0.219667
2019-12-27 14:59:49.823074: step 58080/136300 (epoch 43/100), loss = 0.195789 (0.219 sec/batch), lr: 0.219667
2019-12-27 14:59:54.755538: step 58100/136300 (epoch 43/100), loss = 0.315664 (0.228 sec/batch), lr: 0.219667
2019-12-27 14:59:59.452236: step 58120/136300 (epoch 43/100), loss = 0.276821 (0.242 sec/batch), lr: 0.219667
2019-12-27 15:00:04.342175: step 58140/136300 (epoch 43/100), loss = 0.219530 (0.230 sec/batch), lr: 0.219667
2019-12-27 15:00:09.281971: step 58160/136300 (epoch 43/100), loss = 0.292467 (0.221 sec/batch), lr: 0.219667
2019-12-27 15:00:14.079438: step 58180/136300 (epoch 43/100), loss = 0.323586 (0.211 sec/batch), lr: 0.219667
2019-12-27 15:00:19.040630: step 58200/136300 (epoch 43/100), loss = 0.160342 (0.217 sec/batch), lr: 0.219667
2019-12-27 15:00:23.925686: step 58220/136300 (epoch 43/100), loss = 0.268951 (0.215 sec/batch), lr: 0.219667
2019-12-27 15:00:28.787611: step 58240/136300 (epoch 43/100), loss = 0.362190 (0.221 sec/batch), lr: 0.219667
2019-12-27 15:00:35.036774: step 58260/136300 (epoch 43/100), loss = 0.326391 (0.200 sec/batch), lr: 0.219667
2019-12-27 15:00:39.906038: step 58280/136300 (epoch 43/100), loss = 0.087702 (0.240 sec/batch), lr: 0.219667
2019-12-27 15:00:44.570489: step 58300/136300 (epoch 43/100), loss = 0.204852 (0.214 sec/batch), lr: 0.219667
2019-12-27 15:00:49.330552: step 58320/136300 (epoch 43/100), loss = 0.313946 (0.242 sec/batch), lr: 0.219667
2019-12-27 15:00:54.150366: step 58340/136300 (epoch 43/100), loss = 0.261858 (0.204 sec/batch), lr: 0.219667
2019-12-27 15:00:59.010121: step 58360/136300 (epoch 43/100), loss = 0.297919 (0.242 sec/batch), lr: 0.219667
2019-12-27 15:01:03.906027: step 58380/136300 (epoch 43/100), loss = 0.384604 (0.236 sec/batch), lr: 0.219667
2019-12-27 15:01:08.671635: step 58400/136300 (epoch 43/100), loss = 0.372996 (0.239 sec/batch), lr: 0.219667
2019-12-27 15:01:13.363317: step 58420/136300 (epoch 43/100), loss = 0.323550 (0.206 sec/batch), lr: 0.219667
2019-12-27 15:01:18.206047: step 58440/136300 (epoch 43/100), loss = 0.362423 (0.217 sec/batch), lr: 0.219667
2019-12-27 15:01:24.441368: step 58460/136300 (epoch 43/100), loss = 0.347793 (0.198 sec/batch), lr: 0.219667
2019-12-27 15:01:29.367036: step 58480/136300 (epoch 43/100), loss = 0.296025 (0.213 sec/batch), lr: 0.219667
2019-12-27 15:01:34.265809: step 58500/136300 (epoch 43/100), loss = 0.267275 (0.217 sec/batch), lr: 0.219667
2019-12-27 15:01:38.937509: step 58520/136300 (epoch 43/100), loss = 0.432550 (0.247 sec/batch), lr: 0.219667
2019-12-27 15:01:43.737046: step 58540/136300 (epoch 43/100), loss = 0.317661 (0.229 sec/batch), lr: 0.219667
2019-12-27 15:01:48.554032: step 58560/136300 (epoch 43/100), loss = 0.327182 (0.229 sec/batch), lr: 0.219667
2019-12-27 15:01:53.472445: step 58580/136300 (epoch 43/100), loss = 0.064294 (0.228 sec/batch), lr: 0.219667
2019-12-27 15:01:58.348493: step 58600/136300 (epoch 43/100), loss = 0.303576 (0.232 sec/batch), lr: 0.219667
Evaluating on dev set...
Precision (micro): 72.001%
   Recall (micro): 60.835%
       F1 (micro): 65.949%
epoch 43: train_loss = 0.283462, dev_loss = 0.431865, dev_f1 = 0.6595
model saved to ./saved_models/01/checkpoint_epoch_43.pt
new best model saved.

2019-12-27 15:02:36.642345: step 58620/136300 (epoch 44/100), loss = 0.106773 (0.242 sec/batch), lr: 0.219667
2019-12-27 15:02:42.842607: step 58640/136300 (epoch 44/100), loss = 0.309284 (0.210 sec/batch), lr: 0.219667
2019-12-27 15:02:47.673752: step 58660/136300 (epoch 44/100), loss = 0.360980 (0.243 sec/batch), lr: 0.219667
2019-12-27 15:02:52.389147: step 58680/136300 (epoch 44/100), loss = 0.264125 (0.232 sec/batch), lr: 0.219667
2019-12-27 15:02:57.351617: step 58700/136300 (epoch 44/100), loss = 0.243478 (0.234 sec/batch), lr: 0.219667
2019-12-27 15:03:02.087932: step 58720/136300 (epoch 44/100), loss = 0.443410 (0.180 sec/batch), lr: 0.219667
2019-12-27 15:03:06.884700: step 58740/136300 (epoch 44/100), loss = 0.391931 (0.210 sec/batch), lr: 0.219667
2019-12-27 15:03:11.742748: step 58760/136300 (epoch 44/100), loss = 0.232980 (0.249 sec/batch), lr: 0.219667
2019-12-27 15:03:16.580615: step 58780/136300 (epoch 44/100), loss = 0.453837 (0.216 sec/batch), lr: 0.219667
2019-12-27 15:03:21.477999: step 58800/136300 (epoch 44/100), loss = 0.227264 (0.226 sec/batch), lr: 0.219667
2019-12-27 15:03:26.284371: step 58820/136300 (epoch 44/100), loss = 0.299913 (0.240 sec/batch), lr: 0.219667
2019-12-27 15:03:32.606414: step 58840/136300 (epoch 44/100), loss = 0.338889 (0.245 sec/batch), lr: 0.219667
2019-12-27 15:03:37.382573: step 58860/136300 (epoch 44/100), loss = 0.313400 (0.241 sec/batch), lr: 0.219667
2019-12-27 15:03:42.262630: step 58880/136300 (epoch 44/100), loss = 0.230790 (0.233 sec/batch), lr: 0.219667
2019-12-27 15:03:47.099933: step 58900/136300 (epoch 44/100), loss = 0.228766 (0.236 sec/batch), lr: 0.219667
2019-12-27 15:03:51.849147: step 58920/136300 (epoch 44/100), loss = 0.120962 (0.238 sec/batch), lr: 0.219667
2019-12-27 15:03:56.622726: step 58940/136300 (epoch 44/100), loss = 0.236341 (0.222 sec/batch), lr: 0.219667
2019-12-27 15:04:01.480182: step 58960/136300 (epoch 44/100), loss = 0.261307 (0.234 sec/batch), lr: 0.219667
2019-12-27 15:04:06.338575: step 58980/136300 (epoch 44/100), loss = 0.202262 (0.232 sec/batch), lr: 0.219667
2019-12-27 15:04:11.054955: step 59000/136300 (epoch 44/100), loss = 0.372211 (0.218 sec/batch), lr: 0.219667
2019-12-27 15:04:17.183960: step 59020/136300 (epoch 44/100), loss = 0.346932 (0.216 sec/batch), lr: 0.219667
2019-12-27 15:04:21.998051: step 59040/136300 (epoch 44/100), loss = 0.285868 (0.244 sec/batch), lr: 0.219667
2019-12-27 15:04:26.949294: step 59060/136300 (epoch 44/100), loss = 0.267665 (0.232 sec/batch), lr: 0.219667
2019-12-27 15:04:31.810553: step 59080/136300 (epoch 44/100), loss = 0.431888 (0.196 sec/batch), lr: 0.219667
2019-12-27 15:04:36.631898: step 59100/136300 (epoch 44/100), loss = 0.259933 (0.248 sec/batch), lr: 0.219667
2019-12-27 15:04:41.523889: step 59120/136300 (epoch 44/100), loss = 0.311596 (0.222 sec/batch), lr: 0.219667
2019-12-27 15:04:46.434194: step 59140/136300 (epoch 44/100), loss = 0.475726 (0.240 sec/batch), lr: 0.219667
2019-12-27 15:04:51.314215: step 59160/136300 (epoch 44/100), loss = 0.255602 (0.235 sec/batch), lr: 0.219667
2019-12-27 15:04:56.157159: step 59180/136300 (epoch 44/100), loss = 0.150612 (0.229 sec/batch), lr: 0.219667
2019-12-27 15:05:01.030477: step 59200/136300 (epoch 44/100), loss = 0.144253 (0.250 sec/batch), lr: 0.219667
2019-12-27 15:05:07.193214: step 59220/136300 (epoch 44/100), loss = 0.387683 (0.246 sec/batch), lr: 0.219667
2019-12-27 15:05:11.973121: step 59240/136300 (epoch 44/100), loss = 0.129442 (0.217 sec/batch), lr: 0.219667
2019-12-27 15:05:16.744824: step 59260/136300 (epoch 44/100), loss = 0.145128 (0.237 sec/batch), lr: 0.219667
2019-12-27 15:05:21.590305: step 59280/136300 (epoch 44/100), loss = 0.379895 (0.208 sec/batch), lr: 0.219667
2019-12-27 15:05:26.548830: step 59300/136300 (epoch 44/100), loss = 0.203700 (0.231 sec/batch), lr: 0.219667
2019-12-27 15:05:31.459088: step 59320/136300 (epoch 44/100), loss = 0.209413 (0.235 sec/batch), lr: 0.219667
2019-12-27 15:05:36.378482: step 59340/136300 (epoch 44/100), loss = 0.345566 (0.234 sec/batch), lr: 0.219667
2019-12-27 15:05:41.249084: step 59360/136300 (epoch 44/100), loss = 0.274098 (0.205 sec/batch), lr: 0.219667
2019-12-27 15:05:46.034280: step 59380/136300 (epoch 44/100), loss = 0.341466 (0.253 sec/batch), lr: 0.219667
2019-12-27 15:05:50.950491: step 59400/136300 (epoch 44/100), loss = 0.227597 (0.250 sec/batch), lr: 0.219667
2019-12-27 15:05:57.374928: step 59420/136300 (epoch 44/100), loss = 0.566932 (0.236 sec/batch), lr: 0.219667
2019-12-27 15:06:02.133974: step 59440/136300 (epoch 44/100), loss = 0.254358 (0.178 sec/batch), lr: 0.219667
2019-12-27 15:06:07.131744: step 59460/136300 (epoch 44/100), loss = 0.091565 (0.226 sec/batch), lr: 0.219667
2019-12-27 15:06:11.740272: step 59480/136300 (epoch 44/100), loss = 0.156319 (0.170 sec/batch), lr: 0.219667
2019-12-27 15:06:16.639378: step 59500/136300 (epoch 44/100), loss = 0.242879 (0.200 sec/batch), lr: 0.219667
2019-12-27 15:06:21.592124: step 59520/136300 (epoch 44/100), loss = 0.269358 (0.235 sec/batch), lr: 0.219667
2019-12-27 15:06:26.374378: step 59540/136300 (epoch 44/100), loss = 0.258293 (0.202 sec/batch), lr: 0.219667
2019-12-27 15:06:31.321616: step 59560/136300 (epoch 44/100), loss = 0.281575 (0.236 sec/batch), lr: 0.219667
2019-12-27 15:06:36.245783: step 59580/136300 (epoch 44/100), loss = 0.338236 (0.227 sec/batch), lr: 0.219667
2019-12-27 15:06:41.075014: step 59600/136300 (epoch 44/100), loss = 0.297462 (0.244 sec/batch), lr: 0.219667
2019-12-27 15:06:47.374845: step 59620/136300 (epoch 44/100), loss = 0.347184 (0.224 sec/batch), lr: 0.219667
2019-12-27 15:06:52.320631: step 59640/136300 (epoch 44/100), loss = 0.355959 (0.249 sec/batch), lr: 0.219667
2019-12-27 15:06:56.959242: step 59660/136300 (epoch 44/100), loss = 0.290170 (0.238 sec/batch), lr: 0.219667
2019-12-27 15:07:01.678382: step 59680/136300 (epoch 44/100), loss = 0.312146 (0.240 sec/batch), lr: 0.219667
2019-12-27 15:07:06.521637: step 59700/136300 (epoch 44/100), loss = 0.085223 (0.205 sec/batch), lr: 0.219667
2019-12-27 15:07:11.422636: step 59720/136300 (epoch 44/100), loss = 0.353951 (0.245 sec/batch), lr: 0.219667
2019-12-27 15:07:16.314184: step 59740/136300 (epoch 44/100), loss = 0.392397 (0.225 sec/batch), lr: 0.219667
2019-12-27 15:07:21.080219: step 59760/136300 (epoch 44/100), loss = 0.280323 (0.203 sec/batch), lr: 0.219667
2019-12-27 15:07:25.857305: step 59780/136300 (epoch 44/100), loss = 0.362321 (0.243 sec/batch), lr: 0.219667
2019-12-27 15:07:30.660223: step 59800/136300 (epoch 44/100), loss = 0.440655 (0.244 sec/batch), lr: 0.219667
2019-12-27 15:07:37.051465: step 59820/136300 (epoch 44/100), loss = 0.348001 (0.213 sec/batch), lr: 0.219667
2019-12-27 15:07:41.966086: step 59840/136300 (epoch 44/100), loss = 0.359266 (0.200 sec/batch), lr: 0.219667
2019-12-27 15:07:46.930412: step 59860/136300 (epoch 44/100), loss = 0.238570 (0.234 sec/batch), lr: 0.219667
2019-12-27 15:07:51.534518: step 59880/136300 (epoch 44/100), loss = 0.258371 (0.196 sec/batch), lr: 0.219667
2019-12-27 15:07:56.380278: step 59900/136300 (epoch 44/100), loss = 0.249025 (0.241 sec/batch), lr: 0.219667
2019-12-27 15:08:01.174324: step 59920/136300 (epoch 44/100), loss = 0.259250 (0.236 sec/batch), lr: 0.219667
2019-12-27 15:08:06.124682: step 59940/136300 (epoch 44/100), loss = 0.194359 (0.240 sec/batch), lr: 0.219667
2019-12-27 15:08:10.949741: step 59960/136300 (epoch 44/100), loss = 0.302967 (0.240 sec/batch), lr: 0.219667
Evaluating on dev set...
Precision (micro): 73.346%
   Recall (micro): 58.315%
       F1 (micro): 64.972%
epoch 44: train_loss = 0.284629, dev_loss = 0.455733, dev_f1 = 0.6497
model saved to ./saved_models/01/checkpoint_epoch_44.pt

2019-12-27 15:08:49.682367: step 59980/136300 (epoch 45/100), loss = 0.285579 (0.204 sec/batch), lr: 0.197701
2019-12-27 15:08:55.883302: step 60000/136300 (epoch 45/100), loss = 0.233700 (0.242 sec/batch), lr: 0.197701
2019-12-27 15:09:00.746959: step 60020/136300 (epoch 45/100), loss = 0.275864 (0.245 sec/batch), lr: 0.197701
2019-12-27 15:09:05.442463: step 60040/136300 (epoch 45/100), loss = 0.388697 (0.199 sec/batch), lr: 0.197701
2019-12-27 15:09:10.385636: step 60060/136300 (epoch 45/100), loss = 0.210154 (0.238 sec/batch), lr: 0.197701
2019-12-27 15:09:15.222407: step 60080/136300 (epoch 45/100), loss = 0.314302 (0.232 sec/batch), lr: 0.197701
2019-12-27 15:09:19.966851: step 60100/136300 (epoch 45/100), loss = 0.340775 (0.230 sec/batch), lr: 0.197701
2019-12-27 15:09:24.863324: step 60120/136300 (epoch 45/100), loss = 0.225746 (0.230 sec/batch), lr: 0.197701
2019-12-27 15:09:29.668733: step 60140/136300 (epoch 45/100), loss = 0.177463 (0.210 sec/batch), lr: 0.197701
2019-12-27 15:09:34.569799: step 60160/136300 (epoch 45/100), loss = 0.233369 (0.201 sec/batch), lr: 0.197701
2019-12-27 15:09:39.426552: step 60180/136300 (epoch 45/100), loss = 0.292765 (0.233 sec/batch), lr: 0.197701
2019-12-27 15:09:45.559888: step 60200/136300 (epoch 45/100), loss = 0.329300 (0.237 sec/batch), lr: 0.197701
2019-12-27 15:09:50.407708: step 60220/136300 (epoch 45/100), loss = 0.331256 (0.240 sec/batch), lr: 0.197701
2019-12-27 15:09:55.226378: step 60240/136300 (epoch 45/100), loss = 0.105479 (0.244 sec/batch), lr: 0.197701
2019-12-27 15:10:00.025010: step 60260/136300 (epoch 45/100), loss = 0.340012 (0.240 sec/batch), lr: 0.197701
2019-12-27 15:10:04.828945: step 60280/136300 (epoch 45/100), loss = 0.327484 (0.230 sec/batch), lr: 0.197701
2019-12-27 15:10:09.582084: step 60300/136300 (epoch 45/100), loss = 0.141436 (0.215 sec/batch), lr: 0.197701
2019-12-27 15:10:14.373413: step 60320/136300 (epoch 45/100), loss = 0.115179 (0.208 sec/batch), lr: 0.197701
2019-12-27 15:10:19.281631: step 60340/136300 (epoch 45/100), loss = 0.301154 (0.249 sec/batch), lr: 0.197701
2019-12-27 15:10:23.996023: step 60360/136300 (epoch 45/100), loss = 0.238362 (0.178 sec/batch), lr: 0.197701
2019-12-27 15:10:28.693671: step 60380/136300 (epoch 45/100), loss = 0.330923 (0.251 sec/batch), lr: 0.197701
2019-12-27 15:10:34.793317: step 60400/136300 (epoch 45/100), loss = 0.086298 (0.216 sec/batch), lr: 0.197701
2019-12-27 15:10:39.746223: step 60420/136300 (epoch 45/100), loss = 0.255966 (0.220 sec/batch), lr: 0.197701
2019-12-27 15:10:44.604916: step 60440/136300 (epoch 45/100), loss = 0.194980 (0.173 sec/batch), lr: 0.197701
2019-12-27 15:10:49.424022: step 60460/136300 (epoch 45/100), loss = 0.255207 (0.233 sec/batch), lr: 0.197701
2019-12-27 15:10:54.293984: step 60480/136300 (epoch 45/100), loss = 0.271306 (0.240 sec/batch), lr: 0.197701
2019-12-27 15:10:59.130037: step 60500/136300 (epoch 45/100), loss = 0.361611 (0.217 sec/batch), lr: 0.197701
2019-12-27 15:11:04.061179: step 60520/136300 (epoch 45/100), loss = 0.220787 (0.180 sec/batch), lr: 0.197701
2019-12-27 15:11:08.857711: step 60540/136300 (epoch 45/100), loss = 0.521922 (0.171 sec/batch), lr: 0.197701
2019-12-27 15:11:13.675707: step 60560/136300 (epoch 45/100), loss = 0.285342 (0.229 sec/batch), lr: 0.197701
2019-12-27 15:11:19.914747: step 60580/136300 (epoch 45/100), loss = 0.218896 (0.177 sec/batch), lr: 0.197701
2019-12-27 15:11:24.693281: step 60600/136300 (epoch 45/100), loss = 0.354665 (0.220 sec/batch), lr: 0.197701
2019-12-27 15:11:29.443458: step 60620/136300 (epoch 45/100), loss = 0.339374 (0.205 sec/batch), lr: 0.197701
2019-12-27 15:11:34.263252: step 60640/136300 (epoch 45/100), loss = 0.358740 (0.217 sec/batch), lr: 0.197701
2019-12-27 15:11:39.189459: step 60660/136300 (epoch 45/100), loss = 0.259754 (0.251 sec/batch), lr: 0.197701
2019-12-27 15:11:44.098227: step 60680/136300 (epoch 45/100), loss = 0.323474 (0.230 sec/batch), lr: 0.197701
2019-12-27 15:11:49.001756: step 60700/136300 (epoch 45/100), loss = 0.302201 (0.231 sec/batch), lr: 0.197701
2019-12-27 15:11:53.890187: step 60720/136300 (epoch 45/100), loss = 0.447376 (0.217 sec/batch), lr: 0.197701
2019-12-27 15:11:58.597704: step 60740/136300 (epoch 45/100), loss = 0.248680 (0.171 sec/batch), lr: 0.197701
2019-12-27 15:12:03.530994: step 60760/136300 (epoch 45/100), loss = 0.351279 (0.241 sec/batch), lr: 0.197701
2019-12-27 15:12:09.986144: step 60780/136300 (epoch 45/100), loss = 0.215871 (0.216 sec/batch), lr: 0.197701
2019-12-27 15:12:14.734414: step 60800/136300 (epoch 45/100), loss = 0.378419 (0.233 sec/batch), lr: 0.197701
2019-12-27 15:12:19.702519: step 60820/136300 (epoch 45/100), loss = 0.392924 (0.225 sec/batch), lr: 0.197701
2019-12-27 15:12:24.438628: step 60840/136300 (epoch 45/100), loss = 0.180742 (0.237 sec/batch), lr: 0.197701
2019-12-27 15:12:29.233417: step 60860/136300 (epoch 45/100), loss = 0.153763 (0.238 sec/batch), lr: 0.197701
2019-12-27 15:12:34.153751: step 60880/136300 (epoch 45/100), loss = 0.256932 (0.240 sec/batch), lr: 0.197701
2019-12-27 15:12:38.985785: step 60900/136300 (epoch 45/100), loss = 0.351143 (0.235 sec/batch), lr: 0.197701
2019-12-27 15:12:43.870593: step 60920/136300 (epoch 45/100), loss = 0.485557 (0.239 sec/batch), lr: 0.197701
2019-12-27 15:12:48.837195: step 60940/136300 (epoch 45/100), loss = 0.205570 (0.245 sec/batch), lr: 0.197701
2019-12-27 15:12:53.635232: step 60960/136300 (epoch 45/100), loss = 0.265429 (0.175 sec/batch), lr: 0.197701
2019-12-27 15:12:59.836471: step 60980/136300 (epoch 45/100), loss = 0.340793 (0.214 sec/batch), lr: 0.197701
2019-12-27 15:13:04.766297: step 61000/136300 (epoch 45/100), loss = 0.284993 (0.237 sec/batch), lr: 0.197701
2019-12-27 15:13:09.379000: step 61020/136300 (epoch 45/100), loss = 0.256795 (0.234 sec/batch), lr: 0.197701
2019-12-27 15:13:14.098143: step 61040/136300 (epoch 45/100), loss = 0.327460 (0.179 sec/batch), lr: 0.197701
2019-12-27 15:13:18.976300: step 61060/136300 (epoch 45/100), loss = 0.584371 (0.237 sec/batch), lr: 0.197701
2019-12-27 15:13:23.753170: step 61080/136300 (epoch 45/100), loss = 0.321380 (0.201 sec/batch), lr: 0.197701
2019-12-27 15:13:28.665381: step 61100/136300 (epoch 45/100), loss = 0.457395 (0.240 sec/batch), lr: 0.197701
2019-12-27 15:13:33.440575: step 61120/136300 (epoch 45/100), loss = 0.277130 (0.231 sec/batch), lr: 0.197701
2019-12-27 15:13:38.187127: step 61140/136300 (epoch 45/100), loss = 0.207832 (0.212 sec/batch), lr: 0.197701
2019-12-27 15:13:42.962660: step 61160/136300 (epoch 45/100), loss = 0.285547 (0.233 sec/batch), lr: 0.197701
2019-12-27 15:13:49.438259: step 61180/136300 (epoch 45/100), loss = 0.158347 (0.201 sec/batch), lr: 0.197701
2019-12-27 15:13:54.401418: step 61200/136300 (epoch 45/100), loss = 0.133790 (0.232 sec/batch), lr: 0.197701
2019-12-27 15:13:59.289702: step 61220/136300 (epoch 45/100), loss = 0.193089 (0.239 sec/batch), lr: 0.197701
2019-12-27 15:14:04.006063: step 61240/136300 (epoch 45/100), loss = 0.274650 (0.217 sec/batch), lr: 0.197701
2019-12-27 15:14:08.810794: step 61260/136300 (epoch 45/100), loss = 0.561436 (0.251 sec/batch), lr: 0.197701
2019-12-27 15:14:13.541662: step 61280/136300 (epoch 45/100), loss = 0.172669 (0.218 sec/batch), lr: 0.197701
2019-12-27 15:14:18.488842: step 61300/136300 (epoch 45/100), loss = 0.237372 (0.243 sec/batch), lr: 0.197701
2019-12-27 15:14:23.330497: step 61320/136300 (epoch 45/100), loss = 0.191072 (0.196 sec/batch), lr: 0.197701
Evaluating on dev set...
Precision (micro): 73.258%
   Recall (micro): 59.768%
       F1 (micro): 65.829%
epoch 45: train_loss = 0.279950, dev_loss = 0.442010, dev_f1 = 0.6583
model saved to ./saved_models/01/checkpoint_epoch_45.pt

2019-12-27 15:15:01.649063: step 61340/136300 (epoch 46/100), loss = 0.244124 (0.245 sec/batch), lr: 0.197701
2019-12-27 15:15:07.802937: step 61360/136300 (epoch 46/100), loss = 0.263539 (0.228 sec/batch), lr: 0.197701
2019-12-27 15:15:12.605795: step 61380/136300 (epoch 46/100), loss = 0.158561 (0.237 sec/batch), lr: 0.197701
2019-12-27 15:15:17.354365: step 61400/136300 (epoch 46/100), loss = 0.338314 (0.217 sec/batch), lr: 0.197701
2019-12-27 15:15:22.264416: step 61420/136300 (epoch 46/100), loss = 0.157003 (0.214 sec/batch), lr: 0.197701
2019-12-27 15:15:27.059473: step 61440/136300 (epoch 46/100), loss = 0.217891 (0.241 sec/batch), lr: 0.197701
2019-12-27 15:15:31.830593: step 61460/136300 (epoch 46/100), loss = 0.307754 (0.235 sec/batch), lr: 0.197701
2019-12-27 15:15:36.694836: step 61480/136300 (epoch 46/100), loss = 0.405593 (0.234 sec/batch), lr: 0.197701
2019-12-27 15:15:41.490025: step 61500/136300 (epoch 46/100), loss = 0.224457 (0.211 sec/batch), lr: 0.197701
2019-12-27 15:15:46.386568: step 61520/136300 (epoch 46/100), loss = 0.476927 (0.231 sec/batch), lr: 0.197701
2019-12-27 15:15:51.209121: step 61540/136300 (epoch 46/100), loss = 0.211949 (0.246 sec/batch), lr: 0.197701
2019-12-27 15:15:57.195605: step 61560/136300 (epoch 46/100), loss = 0.213043 (0.239 sec/batch), lr: 0.197701
2019-12-27 15:16:02.012795: step 61580/136300 (epoch 46/100), loss = 0.431134 (0.216 sec/batch), lr: 0.197701
2019-12-27 15:16:06.790638: step 61600/136300 (epoch 46/100), loss = 0.171013 (0.226 sec/batch), lr: 0.197701
2019-12-27 15:16:11.593149: step 61620/136300 (epoch 46/100), loss = 0.549363 (0.220 sec/batch), lr: 0.197701
2019-12-27 15:16:16.443954: step 61640/136300 (epoch 46/100), loss = 0.356526 (0.194 sec/batch), lr: 0.197701
2019-12-27 15:16:21.181395: step 61660/136300 (epoch 46/100), loss = 0.235031 (0.216 sec/batch), lr: 0.197701
2019-12-27 15:16:25.991949: step 61680/136300 (epoch 46/100), loss = 0.261685 (0.218 sec/batch), lr: 0.197701
2019-12-27 15:16:30.919114: step 61700/136300 (epoch 46/100), loss = 0.151803 (0.221 sec/batch), lr: 0.197701
2019-12-27 15:16:35.690608: step 61720/136300 (epoch 46/100), loss = 0.086056 (0.243 sec/batch), lr: 0.197701
2019-12-27 15:16:40.322300: step 61740/136300 (epoch 46/100), loss = 0.420439 (0.215 sec/batch), lr: 0.197701
2019-12-27 15:16:46.741356: step 61760/136300 (epoch 46/100), loss = 0.181409 (0.204 sec/batch), lr: 0.197701
2019-12-27 15:16:51.692863: step 61780/136300 (epoch 46/100), loss = 0.455263 (0.234 sec/batch), lr: 0.197701
2019-12-27 15:16:56.596103: step 61800/136300 (epoch 46/100), loss = 0.136152 (0.238 sec/batch), lr: 0.197701
2019-12-27 15:17:01.410188: step 61820/136300 (epoch 46/100), loss = 0.240518 (0.244 sec/batch), lr: 0.197701
2019-12-27 15:17:06.243282: step 61840/136300 (epoch 46/100), loss = 0.334728 (0.243 sec/batch), lr: 0.197701
2019-12-27 15:17:11.064779: step 61860/136300 (epoch 46/100), loss = 0.478081 (0.201 sec/batch), lr: 0.197701
2019-12-27 15:17:16.014546: step 61880/136300 (epoch 46/100), loss = 0.215990 (0.244 sec/batch), lr: 0.197701
2019-12-27 15:17:20.773378: step 61900/136300 (epoch 46/100), loss = 0.208875 (0.250 sec/batch), lr: 0.197701
2019-12-27 15:17:25.552666: step 61920/136300 (epoch 46/100), loss = 0.410157 (0.209 sec/batch), lr: 0.197701
2019-12-27 15:17:30.331076: step 61940/136300 (epoch 46/100), loss = 0.462823 (0.246 sec/batch), lr: 0.197701
2019-12-27 15:17:36.522655: step 61960/136300 (epoch 46/100), loss = 0.259334 (0.234 sec/batch), lr: 0.197701
2019-12-27 15:17:41.264441: step 61980/136300 (epoch 46/100), loss = 0.184973 (0.219 sec/batch), lr: 0.197701
2019-12-27 15:17:46.044265: step 62000/136300 (epoch 46/100), loss = 0.243175 (0.227 sec/batch), lr: 0.197701
2019-12-27 15:17:50.930206: step 62020/136300 (epoch 46/100), loss = 0.409707 (0.217 sec/batch), lr: 0.197701
2019-12-27 15:17:55.882377: step 62040/136300 (epoch 46/100), loss = 0.257030 (0.247 sec/batch), lr: 0.197701
2019-12-27 15:18:00.767881: step 62060/136300 (epoch 46/100), loss = 0.358997 (0.221 sec/batch), lr: 0.197701
2019-12-27 15:18:05.684859: step 62080/136300 (epoch 46/100), loss = 0.363039 (0.249 sec/batch), lr: 0.197701
2019-12-27 15:18:10.440366: step 62100/136300 (epoch 46/100), loss = 0.346203 (0.226 sec/batch), lr: 0.197701
2019-12-27 15:18:15.231090: step 62120/136300 (epoch 46/100), loss = 0.261824 (0.180 sec/batch), lr: 0.197701
2019-12-27 15:18:21.831579: step 62140/136300 (epoch 46/100), loss = 0.286429 (0.220 sec/batch), lr: 0.197701
2019-12-27 15:18:26.605466: step 62160/136300 (epoch 46/100), loss = 0.205761 (0.216 sec/batch), lr: 0.197701
2019-12-27 15:18:31.545342: step 62180/136300 (epoch 46/100), loss = 0.285143 (0.249 sec/batch), lr: 0.197701
2019-12-27 15:18:36.298961: step 62200/136300 (epoch 46/100), loss = 0.355971 (0.205 sec/batch), lr: 0.197701
2019-12-27 15:18:41.058567: step 62220/136300 (epoch 46/100), loss = 0.313335 (0.229 sec/batch), lr: 0.197701
2019-12-27 15:18:45.955494: step 62240/136300 (epoch 46/100), loss = 0.211734 (0.243 sec/batch), lr: 0.197701
2019-12-27 15:18:50.886230: step 62260/136300 (epoch 46/100), loss = 0.116497 (0.219 sec/batch), lr: 0.197701
2019-12-27 15:18:55.673476: step 62280/136300 (epoch 46/100), loss = 0.386442 (0.235 sec/batch), lr: 0.197701
2019-12-27 15:19:00.641565: step 62300/136300 (epoch 46/100), loss = 0.385434 (0.250 sec/batch), lr: 0.197701
2019-12-27 15:19:05.523763: step 62320/136300 (epoch 46/100), loss = 0.423512 (0.218 sec/batch), lr: 0.197701
2019-12-27 15:19:11.755303: step 62340/136300 (epoch 46/100), loss = 0.209042 (0.239 sec/batch), lr: 0.197701
2019-12-27 15:19:16.647090: step 62360/136300 (epoch 46/100), loss = 0.168876 (0.234 sec/batch), lr: 0.197701
2019-12-27 15:19:21.283127: step 62380/136300 (epoch 46/100), loss = 0.348764 (0.229 sec/batch), lr: 0.197701
2019-12-27 15:19:26.054329: step 62400/136300 (epoch 46/100), loss = 0.459974 (0.217 sec/batch), lr: 0.197701
2019-12-27 15:19:30.912689: step 62420/136300 (epoch 46/100), loss = 0.313337 (0.205 sec/batch), lr: 0.197701
2019-12-27 15:19:35.714858: step 62440/136300 (epoch 46/100), loss = 0.323146 (0.236 sec/batch), lr: 0.197701
2019-12-27 15:19:40.639681: step 62460/136300 (epoch 46/100), loss = 0.328890 (0.238 sec/batch), lr: 0.197701
2019-12-27 15:19:45.412273: step 62480/136300 (epoch 46/100), loss = 0.107507 (0.224 sec/batch), lr: 0.197701
2019-12-27 15:19:50.171680: step 62500/136300 (epoch 46/100), loss = 0.356222 (0.218 sec/batch), lr: 0.197701
2019-12-27 15:19:54.959321: step 62520/136300 (epoch 46/100), loss = 0.545761 (0.235 sec/batch), lr: 0.197701
2019-12-27 15:20:01.482257: step 62540/136300 (epoch 46/100), loss = 0.206413 (0.215 sec/batch), lr: 0.197701
2019-12-27 15:20:06.403906: step 62560/136300 (epoch 46/100), loss = 0.439717 (0.247 sec/batch), lr: 0.197701
2019-12-27 15:20:11.294568: step 62580/136300 (epoch 46/100), loss = 0.293301 (0.230 sec/batch), lr: 0.197701
2019-12-27 15:20:16.103695: step 62600/136300 (epoch 46/100), loss = 0.387150 (0.243 sec/batch), lr: 0.197701
2019-12-27 15:20:20.821214: step 62620/136300 (epoch 46/100), loss = 0.228809 (0.232 sec/batch), lr: 0.197701
2019-12-27 15:20:25.574497: step 62640/136300 (epoch 46/100), loss = 0.115028 (0.237 sec/batch), lr: 0.197701
2019-12-27 15:20:30.463078: step 62660/136300 (epoch 46/100), loss = 0.190858 (0.236 sec/batch), lr: 0.197701
2019-12-27 15:20:35.363369: step 62680/136300 (epoch 46/100), loss = 0.366431 (0.231 sec/batch), lr: 0.197701
Evaluating on dev set...
Precision (micro): 73.907%
   Recall (micro): 60.026%
       F1 (micro): 66.247%
epoch 46: train_loss = 0.278768, dev_loss = 0.423039, dev_f1 = 0.6625
model saved to ./saved_models/01/checkpoint_epoch_46.pt
new best model saved.

2019-12-27 15:21:13.740041: step 62700/136300 (epoch 47/100), loss = 0.254344 (0.231 sec/batch), lr: 0.197701
2019-12-27 15:21:20.049134: step 62720/136300 (epoch 47/100), loss = 0.241064 (0.214 sec/batch), lr: 0.197701
2019-12-27 15:21:24.828157: step 62740/136300 (epoch 47/100), loss = 0.461966 (0.217 sec/batch), lr: 0.197701
2019-12-27 15:21:29.658598: step 62760/136300 (epoch 47/100), loss = 0.287295 (0.216 sec/batch), lr: 0.197701
2019-12-27 15:21:34.518897: step 62780/136300 (epoch 47/100), loss = 0.284391 (0.202 sec/batch), lr: 0.197701
2019-12-27 15:21:39.329737: step 62800/136300 (epoch 47/100), loss = 0.255824 (0.226 sec/batch), lr: 0.197701
2019-12-27 15:21:44.100174: step 62820/136300 (epoch 47/100), loss = 0.325947 (0.241 sec/batch), lr: 0.197701
2019-12-27 15:21:48.953264: step 62840/136300 (epoch 47/100), loss = 0.344813 (0.236 sec/batch), lr: 0.197701
2019-12-27 15:21:53.777359: step 62860/136300 (epoch 47/100), loss = 0.123048 (0.224 sec/batch), lr: 0.197701
2019-12-27 15:21:58.670223: step 62880/136300 (epoch 47/100), loss = 0.252663 (0.217 sec/batch), lr: 0.197701
2019-12-27 15:22:03.516514: step 62900/136300 (epoch 47/100), loss = 0.283121 (0.230 sec/batch), lr: 0.197701
2019-12-27 15:22:09.760896: step 62920/136300 (epoch 47/100), loss = 0.435225 (0.237 sec/batch), lr: 0.197701
2019-12-27 15:22:14.546316: step 62940/136300 (epoch 47/100), loss = 0.407813 (0.242 sec/batch), lr: 0.197701
2019-12-27 15:22:19.368918: step 62960/136300 (epoch 47/100), loss = 0.249772 (0.216 sec/batch), lr: 0.197701
2019-12-27 15:22:24.193332: step 62980/136300 (epoch 47/100), loss = 0.260650 (0.217 sec/batch), lr: 0.197701
2019-12-27 15:22:29.062343: step 63000/136300 (epoch 47/100), loss = 0.180893 (0.243 sec/batch), lr: 0.197701
2019-12-27 15:22:33.858571: step 63020/136300 (epoch 47/100), loss = 0.230371 (0.241 sec/batch), lr: 0.197701
2019-12-27 15:22:38.611067: step 63040/136300 (epoch 47/100), loss = 0.161422 (0.230 sec/batch), lr: 0.197701
2019-12-27 15:22:43.521091: step 63060/136300 (epoch 47/100), loss = 0.214777 (0.239 sec/batch), lr: 0.197701
2019-12-27 15:22:48.273013: step 63080/136300 (epoch 47/100), loss = 0.138917 (0.218 sec/batch), lr: 0.197701
2019-12-27 15:22:53.018872: step 63100/136300 (epoch 47/100), loss = 0.235379 (0.227 sec/batch), lr: 0.197701
2019-12-27 15:22:59.345326: step 63120/136300 (epoch 47/100), loss = 0.221421 (0.215 sec/batch), lr: 0.197701
2019-12-27 15:23:04.241127: step 63140/136300 (epoch 47/100), loss = 0.209872 (0.240 sec/batch), lr: 0.197701
2019-12-27 15:23:09.151161: step 63160/136300 (epoch 47/100), loss = 0.239234 (0.233 sec/batch), lr: 0.197701
2019-12-27 15:23:13.954412: step 63180/136300 (epoch 47/100), loss = 0.149910 (0.215 sec/batch), lr: 0.197701
2019-12-27 15:23:18.781702: step 63200/136300 (epoch 47/100), loss = 0.481688 (0.225 sec/batch), lr: 0.197701
2019-12-27 15:23:23.653992: step 63220/136300 (epoch 47/100), loss = 0.184302 (0.248 sec/batch), lr: 0.197701
2019-12-27 15:23:28.642171: step 63240/136300 (epoch 47/100), loss = 0.367157 (0.237 sec/batch), lr: 0.197701
2019-12-27 15:23:33.408428: step 63260/136300 (epoch 47/100), loss = 0.280982 (0.243 sec/batch), lr: 0.197701
2019-12-27 15:23:38.234225: step 63280/136300 (epoch 47/100), loss = 0.266643 (0.218 sec/batch), lr: 0.197701
2019-12-27 15:23:42.936323: step 63300/136300 (epoch 47/100), loss = 0.227541 (0.214 sec/batch), lr: 0.197701
2019-12-27 15:23:49.230363: step 63320/136300 (epoch 47/100), loss = 0.158526 (0.223 sec/batch), lr: 0.197701
2019-12-27 15:23:54.027659: step 63340/136300 (epoch 47/100), loss = 0.314813 (0.218 sec/batch), lr: 0.197701
2019-12-27 15:23:58.819434: step 63360/136300 (epoch 47/100), loss = 0.067310 (0.223 sec/batch), lr: 0.197701
2019-12-27 15:24:03.674451: step 63380/136300 (epoch 47/100), loss = 0.562939 (0.205 sec/batch), lr: 0.197701
2019-12-27 15:24:08.594598: step 63400/136300 (epoch 47/100), loss = 0.400472 (0.209 sec/batch), lr: 0.197701
2019-12-27 15:24:13.533391: step 63420/136300 (epoch 47/100), loss = 0.339394 (0.250 sec/batch), lr: 0.197701
2019-12-27 15:24:18.422295: step 63440/136300 (epoch 47/100), loss = 0.291841 (0.249 sec/batch), lr: 0.197701
2019-12-27 15:24:23.186178: step 63460/136300 (epoch 47/100), loss = 0.230442 (0.212 sec/batch), lr: 0.197701
2019-12-27 15:24:28.024794: step 63480/136300 (epoch 47/100), loss = 0.201651 (0.246 sec/batch), lr: 0.197701
2019-12-27 15:24:34.374111: step 63500/136300 (epoch 47/100), loss = 0.329936 (0.218 sec/batch), lr: 0.197701
2019-12-27 15:24:39.145677: step 63520/136300 (epoch 47/100), loss = 0.428128 (0.202 sec/batch), lr: 0.197701
2019-12-27 15:24:44.067780: step 63540/136300 (epoch 47/100), loss = 0.180222 (0.252 sec/batch), lr: 0.197701
2019-12-27 15:24:48.909266: step 63560/136300 (epoch 47/100), loss = 0.180166 (0.215 sec/batch), lr: 0.197701
2019-12-27 15:24:53.622233: step 63580/136300 (epoch 47/100), loss = 0.322605 (0.210 sec/batch), lr: 0.197701
2019-12-27 15:24:58.495598: step 63600/136300 (epoch 47/100), loss = 0.253489 (0.230 sec/batch), lr: 0.197701
2019-12-27 15:25:03.440977: step 63620/136300 (epoch 47/100), loss = 0.186641 (0.223 sec/batch), lr: 0.197701
2019-12-27 15:25:08.252313: step 63640/136300 (epoch 47/100), loss = 0.322491 (0.249 sec/batch), lr: 0.197701
2019-12-27 15:25:13.256675: step 63660/136300 (epoch 47/100), loss = 0.256308 (0.248 sec/batch), lr: 0.197701
2019-12-27 15:25:18.131996: step 63680/136300 (epoch 47/100), loss = 0.446882 (0.242 sec/batch), lr: 0.197701
2019-12-27 15:25:24.480999: step 63700/136300 (epoch 47/100), loss = 0.364684 (1.760 sec/batch), lr: 0.197701
2019-12-27 15:25:29.310458: step 63720/136300 (epoch 47/100), loss = 0.150047 (0.223 sec/batch), lr: 0.197701
2019-12-27 15:25:33.988014: step 63740/136300 (epoch 47/100), loss = 0.321161 (0.198 sec/batch), lr: 0.197701
2019-12-27 15:25:38.744836: step 63760/136300 (epoch 47/100), loss = 0.366785 (0.234 sec/batch), lr: 0.197701
2019-12-27 15:25:43.602593: step 63780/136300 (epoch 47/100), loss = 0.100357 (0.229 sec/batch), lr: 0.197701
2019-12-27 15:25:48.381548: step 63800/136300 (epoch 47/100), loss = 0.166544 (0.233 sec/batch), lr: 0.197701
2019-12-27 15:25:53.302702: step 63820/136300 (epoch 47/100), loss = 0.462139 (0.245 sec/batch), lr: 0.197701
2019-12-27 15:25:58.165701: step 63840/136300 (epoch 47/100), loss = 0.312418 (0.236 sec/batch), lr: 0.197701
2019-12-27 15:26:02.922389: step 63860/136300 (epoch 47/100), loss = 0.161574 (0.248 sec/batch), lr: 0.197701
2019-12-27 15:26:07.679131: step 63880/136300 (epoch 47/100), loss = 0.277557 (0.245 sec/batch), lr: 0.197701
2019-12-27 15:26:13.987160: step 63900/136300 (epoch 47/100), loss = 0.298830 (0.239 sec/batch), lr: 0.197701
2019-12-27 15:26:18.884537: step 63920/136300 (epoch 47/100), loss = 0.335008 (0.236 sec/batch), lr: 0.197701
2019-12-27 15:26:23.804856: step 63940/136300 (epoch 47/100), loss = 0.113320 (0.240 sec/batch), lr: 0.197701
2019-12-27 15:26:28.614519: step 63960/136300 (epoch 47/100), loss = 0.333342 (0.242 sec/batch), lr: 0.197701
2019-12-27 15:26:33.325974: step 63980/136300 (epoch 47/100), loss = 0.261117 (0.230 sec/batch), lr: 0.197701
2019-12-27 15:26:38.101743: step 64000/136300 (epoch 47/100), loss = 0.135016 (0.212 sec/batch), lr: 0.197701
2019-12-27 15:26:42.949548: step 64020/136300 (epoch 47/100), loss = 0.283464 (0.244 sec/batch), lr: 0.197701
2019-12-27 15:26:47.873707: step 64040/136300 (epoch 47/100), loss = 0.232147 (0.244 sec/batch), lr: 0.197701
2019-12-27 15:26:52.777265: step 64060/136300 (epoch 47/100), loss = 0.325964 (0.202 sec/batch), lr: 0.197701
Evaluating on dev set...
Precision (micro): 70.237%
   Recall (micro): 61.516%
       F1 (micro): 65.588%
epoch 47: train_loss = 0.277073, dev_loss = 0.451937, dev_f1 = 0.6559
model saved to ./saved_models/01/checkpoint_epoch_47.pt

2019-12-27 15:27:32.552840: step 64080/136300 (epoch 48/100), loss = 0.174859 (0.229 sec/batch), lr: 0.177931
2019-12-27 15:27:37.267268: step 64100/136300 (epoch 48/100), loss = 0.184527 (0.215 sec/batch), lr: 0.177931
2019-12-27 15:27:42.109665: step 64120/136300 (epoch 48/100), loss = 0.467174 (0.215 sec/batch), lr: 0.177931
2019-12-27 15:27:46.977766: step 64140/136300 (epoch 48/100), loss = 0.211282 (0.239 sec/batch), lr: 0.177931
2019-12-27 15:27:51.790250: step 64160/136300 (epoch 48/100), loss = 0.130802 (0.241 sec/batch), lr: 0.177931
2019-12-27 15:27:56.561232: step 64180/136300 (epoch 48/100), loss = 0.264765 (0.226 sec/batch), lr: 0.177931
2019-12-27 15:28:01.365608: step 64200/136300 (epoch 48/100), loss = 0.315903 (0.218 sec/batch), lr: 0.177931
2019-12-27 15:28:06.192382: step 64220/136300 (epoch 48/100), loss = 0.207255 (0.232 sec/batch), lr: 0.177931
2019-12-27 15:28:11.110341: step 64240/136300 (epoch 48/100), loss = 0.172202 (0.240 sec/batch), lr: 0.177931
2019-12-27 15:28:15.980570: step 64260/136300 (epoch 48/100), loss = 0.613403 (0.222 sec/batch), lr: 0.177931
2019-12-27 15:28:22.363849: step 64280/136300 (epoch 48/100), loss = 0.357433 (0.227 sec/batch), lr: 0.177931
2019-12-27 15:28:27.113706: step 64300/136300 (epoch 48/100), loss = 0.352442 (0.244 sec/batch), lr: 0.177931
2019-12-27 15:28:31.922087: step 64320/136300 (epoch 48/100), loss = 0.293336 (0.238 sec/batch), lr: 0.177931
2019-12-27 15:28:36.787513: step 64340/136300 (epoch 48/100), loss = 0.217810 (0.232 sec/batch), lr: 0.177931
2019-12-27 15:28:41.579725: step 64360/136300 (epoch 48/100), loss = 0.380329 (0.212 sec/batch), lr: 0.177931
2019-12-27 15:28:46.357983: step 64380/136300 (epoch 48/100), loss = 0.302258 (0.242 sec/batch), lr: 0.177931
2019-12-27 15:28:51.167861: step 64400/136300 (epoch 48/100), loss = 0.331623 (0.198 sec/batch), lr: 0.177931
2019-12-27 15:28:56.035382: step 64420/136300 (epoch 48/100), loss = 0.192703 (0.242 sec/batch), lr: 0.177931
2019-12-27 15:29:00.783986: step 64440/136300 (epoch 48/100), loss = 0.198968 (0.245 sec/batch), lr: 0.177931
2019-12-27 15:29:05.539144: step 64460/136300 (epoch 48/100), loss = 0.348586 (0.234 sec/batch), lr: 0.177931
2019-12-27 15:29:11.795325: step 64480/136300 (epoch 48/100), loss = 0.247990 (0.241 sec/batch), lr: 0.177931
2019-12-27 15:29:16.648682: step 64500/136300 (epoch 48/100), loss = 0.244820 (0.237 sec/batch), lr: 0.177931
2019-12-27 15:29:21.509403: step 64520/136300 (epoch 48/100), loss = 0.151625 (0.205 sec/batch), lr: 0.177931
2019-12-27 15:29:26.364441: step 64540/136300 (epoch 48/100), loss = 0.382692 (0.238 sec/batch), lr: 0.177931
2019-12-27 15:29:31.174751: step 64560/136300 (epoch 48/100), loss = 0.167829 (0.228 sec/batch), lr: 0.177931
2019-12-27 15:29:36.071822: step 64580/136300 (epoch 48/100), loss = 0.232299 (0.231 sec/batch), lr: 0.177931
2019-12-27 15:29:40.998572: step 64600/136300 (epoch 48/100), loss = 0.368609 (0.212 sec/batch), lr: 0.177931
2019-12-27 15:29:45.823181: step 64620/136300 (epoch 48/100), loss = 0.218405 (0.243 sec/batch), lr: 0.177931
2019-12-27 15:29:50.656894: step 64640/136300 (epoch 48/100), loss = 0.270711 (0.236 sec/batch), lr: 0.177931
2019-12-27 15:29:55.377616: step 64660/136300 (epoch 48/100), loss = 0.391031 (0.224 sec/batch), lr: 0.177931
2019-12-27 15:30:01.680950: step 64680/136300 (epoch 48/100), loss = 0.234823 (0.238 sec/batch), lr: 0.177931
2019-12-27 15:30:06.518366: step 64700/136300 (epoch 48/100), loss = 0.407708 (0.227 sec/batch), lr: 0.177931
2019-12-27 15:30:11.266441: step 64720/136300 (epoch 48/100), loss = 0.169992 (0.225 sec/batch), lr: 0.177931
2019-12-27 15:30:16.126574: step 64740/136300 (epoch 48/100), loss = 0.255300 (0.242 sec/batch), lr: 0.177931
2019-12-27 15:30:21.046283: step 64760/136300 (epoch 48/100), loss = 0.126154 (0.231 sec/batch), lr: 0.177931
2019-12-27 15:30:25.951785: step 64780/136300 (epoch 48/100), loss = 0.227153 (0.240 sec/batch), lr: 0.177931
2019-12-27 15:30:30.882160: step 64800/136300 (epoch 48/100), loss = 0.418067 (0.248 sec/batch), lr: 0.177931
2019-12-27 15:30:35.623528: step 64820/136300 (epoch 48/100), loss = 0.189135 (0.213 sec/batch), lr: 0.177931
2019-12-27 15:30:40.418033: step 64840/136300 (epoch 48/100), loss = 0.207554 (0.174 sec/batch), lr: 0.177931
2019-12-27 15:30:46.721421: step 64860/136300 (epoch 48/100), loss = 0.316167 (0.246 sec/batch), lr: 0.177931
2019-12-27 15:30:51.484144: step 64880/136300 (epoch 48/100), loss = 0.383890 (0.220 sec/batch), lr: 0.177931
2019-12-27 15:30:56.354317: step 64900/136300 (epoch 48/100), loss = 0.342369 (0.236 sec/batch), lr: 0.177931
2019-12-27 15:31:01.243179: step 64920/136300 (epoch 48/100), loss = 0.415553 (0.210 sec/batch), lr: 0.177931
2019-12-27 15:31:05.970824: step 64940/136300 (epoch 48/100), loss = 0.400147 (0.236 sec/batch), lr: 0.177931
2019-12-27 15:31:10.833484: step 64960/136300 (epoch 48/100), loss = 0.234472 (0.211 sec/batch), lr: 0.177931
2019-12-27 15:31:15.752371: step 64980/136300 (epoch 48/100), loss = 0.244435 (0.218 sec/batch), lr: 0.177931
2019-12-27 15:31:20.589455: step 65000/136300 (epoch 48/100), loss = 0.383183 (0.244 sec/batch), lr: 0.177931
2019-12-27 15:31:25.557446: step 65020/136300 (epoch 48/100), loss = 0.341327 (0.232 sec/batch), lr: 0.177931
2019-12-27 15:31:30.454256: step 65040/136300 (epoch 48/100), loss = 0.130584 (0.245 sec/batch), lr: 0.177931
2019-12-27 15:31:35.251087: step 65060/136300 (epoch 48/100), loss = 0.351275 (0.235 sec/batch), lr: 0.177931
2019-12-27 15:31:41.614819: step 65080/136300 (epoch 48/100), loss = 0.109175 (0.224 sec/batch), lr: 0.177931
2019-12-27 15:31:46.388946: step 65100/136300 (epoch 48/100), loss = 0.343078 (0.203 sec/batch), lr: 0.177931
2019-12-27 15:31:51.102385: step 65120/136300 (epoch 48/100), loss = 0.408864 (0.216 sec/batch), lr: 0.177931
2019-12-27 15:31:55.901713: step 65140/136300 (epoch 48/100), loss = 0.344373 (0.237 sec/batch), lr: 0.177931
2019-12-27 15:32:00.683828: step 65160/136300 (epoch 48/100), loss = 0.290602 (0.240 sec/batch), lr: 0.177931
2019-12-27 15:32:05.615637: step 65180/136300 (epoch 48/100), loss = 0.214548 (0.240 sec/batch), lr: 0.177931
2019-12-27 15:32:10.455104: step 65200/136300 (epoch 48/100), loss = 0.139768 (0.240 sec/batch), lr: 0.177931
2019-12-27 15:32:15.211802: step 65220/136300 (epoch 48/100), loss = 0.311125 (0.235 sec/batch), lr: 0.177931
2019-12-27 15:32:19.937976: step 65240/136300 (epoch 48/100), loss = 0.193772 (0.234 sec/batch), lr: 0.177931
2019-12-27 15:32:24.764127: step 65260/136300 (epoch 48/100), loss = 0.251564 (0.231 sec/batch), lr: 0.177931
2019-12-27 15:32:30.958423: step 65280/136300 (epoch 48/100), loss = 0.263245 (0.240 sec/batch), lr: 0.177931
2019-12-27 15:32:35.865227: step 65300/136300 (epoch 48/100), loss = 0.211522 (0.244 sec/batch), lr: 0.177931
2019-12-27 15:32:40.711515: step 65320/136300 (epoch 48/100), loss = 0.415449 (0.220 sec/batch), lr: 0.177931
2019-12-27 15:32:45.415948: step 65340/136300 (epoch 48/100), loss = 0.181937 (0.213 sec/batch), lr: 0.177931
2019-12-27 15:32:50.281415: step 65360/136300 (epoch 48/100), loss = 0.171971 (0.237 sec/batch), lr: 0.177931
2019-12-27 15:32:55.055965: step 65380/136300 (epoch 48/100), loss = 0.320782 (0.240 sec/batch), lr: 0.177931
2019-12-27 15:32:59.954081: step 65400/136300 (epoch 48/100), loss = 0.289023 (0.199 sec/batch), lr: 0.177931
2019-12-27 15:33:04.920146: step 65420/136300 (epoch 48/100), loss = 0.280420 (0.240 sec/batch), lr: 0.177931
Evaluating on dev set...
Precision (micro): 71.169%
   Recall (micro): 61.939%
       F1 (micro): 66.234%
epoch 48: train_loss = 0.272957, dev_loss = 0.441779, dev_f1 = 0.6623
model saved to ./saved_models/01/checkpoint_epoch_48.pt

2019-12-27 15:33:43.408325: step 65440/136300 (epoch 49/100), loss = 0.270247 (0.224 sec/batch), lr: 0.177931
2019-12-27 15:33:49.541503: step 65460/136300 (epoch 49/100), loss = 0.371056 (0.206 sec/batch), lr: 0.177931
2019-12-27 15:33:54.416145: step 65480/136300 (epoch 49/100), loss = 0.155729 (0.220 sec/batch), lr: 0.177931
2019-12-27 15:33:59.237403: step 65500/136300 (epoch 49/100), loss = 0.310264 (0.244 sec/batch), lr: 0.177931
2019-12-27 15:34:04.101979: step 65520/136300 (epoch 49/100), loss = 0.244673 (0.205 sec/batch), lr: 0.177931
2019-12-27 15:34:08.862646: step 65540/136300 (epoch 49/100), loss = 0.249415 (0.216 sec/batch), lr: 0.177931
2019-12-27 15:34:13.690378: step 65560/136300 (epoch 49/100), loss = 0.325609 (0.235 sec/batch), lr: 0.177931
2019-12-27 15:34:18.516176: step 65580/136300 (epoch 49/100), loss = 0.255637 (0.225 sec/batch), lr: 0.177931
2019-12-27 15:34:23.408915: step 65600/136300 (epoch 49/100), loss = 0.291656 (0.236 sec/batch), lr: 0.177931
2019-12-27 15:34:28.295107: step 65620/136300 (epoch 49/100), loss = 0.248682 (0.247 sec/batch), lr: 0.177931
2019-12-27 15:34:34.548440: step 65640/136300 (epoch 49/100), loss = 0.157160 (0.226 sec/batch), lr: 0.177931
2019-12-27 15:34:39.293996: step 65660/136300 (epoch 49/100), loss = 0.357322 (0.227 sec/batch), lr: 0.177931
2019-12-27 15:34:44.098136: step 65680/136300 (epoch 49/100), loss = 0.473001 (0.201 sec/batch), lr: 0.177931
2019-12-27 15:34:48.981273: step 65700/136300 (epoch 49/100), loss = 0.274651 (0.206 sec/batch), lr: 0.177931
2019-12-27 15:34:53.845296: step 65720/136300 (epoch 49/100), loss = 0.245707 (0.233 sec/batch), lr: 0.177931
2019-12-27 15:34:58.560799: step 65740/136300 (epoch 49/100), loss = 0.352041 (0.217 sec/batch), lr: 0.177931
2019-12-27 15:35:03.402501: step 65760/136300 (epoch 49/100), loss = 0.232349 (0.202 sec/batch), lr: 0.177931
2019-12-27 15:35:08.260864: step 65780/136300 (epoch 49/100), loss = 0.323534 (0.232 sec/batch), lr: 0.177931
2019-12-27 15:35:13.006555: step 65800/136300 (epoch 49/100), loss = 0.127321 (0.235 sec/batch), lr: 0.177931
2019-12-27 15:35:17.797330: step 65820/136300 (epoch 49/100), loss = 0.329673 (0.235 sec/batch), lr: 0.177931
2019-12-27 15:35:23.858542: step 65840/136300 (epoch 49/100), loss = 0.182036 (0.230 sec/batch), lr: 0.177931
2019-12-27 15:35:28.701994: step 65860/136300 (epoch 49/100), loss = 0.337413 (0.248 sec/batch), lr: 0.177931
2019-12-27 15:35:33.584637: step 65880/136300 (epoch 49/100), loss = 0.161069 (0.205 sec/batch), lr: 0.177931
2019-12-27 15:35:38.425139: step 65900/136300 (epoch 49/100), loss = 0.188637 (0.233 sec/batch), lr: 0.177931
2019-12-27 15:35:43.270553: step 65920/136300 (epoch 49/100), loss = 0.100017 (0.216 sec/batch), lr: 0.177931
2019-12-27 15:35:48.166223: step 65940/136300 (epoch 49/100), loss = 0.245684 (0.211 sec/batch), lr: 0.177931
2019-12-27 15:35:53.090862: step 65960/136300 (epoch 49/100), loss = 0.309300 (0.243 sec/batch), lr: 0.177931
2019-12-27 15:35:57.908839: step 65980/136300 (epoch 49/100), loss = 0.474710 (0.232 sec/batch), lr: 0.177931
2019-12-27 15:36:02.746750: step 66000/136300 (epoch 49/100), loss = 0.222038 (0.205 sec/batch), lr: 0.177931
2019-12-27 15:36:07.486396: step 66020/136300 (epoch 49/100), loss = 0.292396 (0.205 sec/batch), lr: 0.177931
2019-12-27 15:36:13.859927: step 66040/136300 (epoch 49/100), loss = 0.179085 (0.228 sec/batch), lr: 0.177931
2019-12-27 15:36:18.665271: step 66060/136300 (epoch 49/100), loss = 0.094827 (0.213 sec/batch), lr: 0.177931
2019-12-27 15:36:23.421996: step 66080/136300 (epoch 49/100), loss = 0.180174 (0.210 sec/batch), lr: 0.177931
2019-12-27 15:36:28.253058: step 66100/136300 (epoch 49/100), loss = 0.258784 (0.211 sec/batch), lr: 0.177931
2019-12-27 15:36:33.235184: step 66120/136300 (epoch 49/100), loss = 0.277408 (0.238 sec/batch), lr: 0.177931
2019-12-27 15:36:38.111909: step 66140/136300 (epoch 49/100), loss = 0.342329 (0.244 sec/batch), lr: 0.177931
2019-12-27 15:36:43.013943: step 66160/136300 (epoch 49/100), loss = 0.248919 (0.205 sec/batch), lr: 0.177931
2019-12-27 15:36:47.809750: step 66180/136300 (epoch 49/100), loss = 0.450039 (0.182 sec/batch), lr: 0.177931
2019-12-27 15:36:52.650705: step 66200/136300 (epoch 49/100), loss = 0.243494 (0.226 sec/batch), lr: 0.177931
2019-12-27 15:36:57.562864: step 66220/136300 (epoch 49/100), loss = 0.310342 (0.228 sec/batch), lr: 0.177931
2019-12-27 15:37:03.918690: step 66240/136300 (epoch 49/100), loss = 0.269187 (0.244 sec/batch), lr: 0.177931
2019-12-27 15:37:08.685202: step 66260/136300 (epoch 49/100), loss = 0.288185 (0.216 sec/batch), lr: 0.177931
2019-12-27 15:37:13.650293: step 66280/136300 (epoch 49/100), loss = 0.257579 (0.244 sec/batch), lr: 0.177931
2019-12-27 15:37:18.357330: step 66300/136300 (epoch 49/100), loss = 0.309773 (0.241 sec/batch), lr: 0.177931
2019-12-27 15:37:23.223406: step 66320/136300 (epoch 49/100), loss = 0.189937 (0.230 sec/batch), lr: 0.177931
2019-12-27 15:37:28.153962: step 66340/136300 (epoch 49/100), loss = 0.217589 (0.203 sec/batch), lr: 0.177931
2019-12-27 15:37:32.956483: step 66360/136300 (epoch 49/100), loss = 0.229346 (0.235 sec/batch), lr: 0.177931
2019-12-27 15:37:37.932299: step 66380/136300 (epoch 49/100), loss = 0.225380 (0.223 sec/batch), lr: 0.177931
2019-12-27 15:37:42.836004: step 66400/136300 (epoch 49/100), loss = 0.306450 (0.240 sec/batch), lr: 0.177931
2019-12-27 15:37:47.662513: step 66420/136300 (epoch 49/100), loss = 0.341192 (0.229 sec/batch), lr: 0.177931
2019-12-27 15:37:53.957114: step 66440/136300 (epoch 49/100), loss = 0.152304 (0.231 sec/batch), lr: 0.177931
2019-12-27 15:37:58.798432: step 66460/136300 (epoch 49/100), loss = 0.350458 (0.201 sec/batch), lr: 0.177931
2019-12-27 15:38:03.465595: step 66480/136300 (epoch 49/100), loss = 0.186488 (0.216 sec/batch), lr: 0.177931
2019-12-27 15:38:08.244966: step 66500/136300 (epoch 49/100), loss = 0.302241 (0.234 sec/batch), lr: 0.177931
2019-12-27 15:38:13.079337: step 66520/136300 (epoch 49/100), loss = 0.308918 (0.233 sec/batch), lr: 0.177931
2019-12-27 15:38:17.908783: step 66540/136300 (epoch 49/100), loss = 0.141488 (0.237 sec/batch), lr: 0.177931
2019-12-27 15:38:22.808022: step 66560/136300 (epoch 49/100), loss = 0.169759 (0.229 sec/batch), lr: 0.177931
2019-12-27 15:38:27.567487: step 66580/136300 (epoch 49/100), loss = 0.287350 (0.231 sec/batch), lr: 0.177931
2019-12-27 15:38:32.242630: step 66600/136300 (epoch 49/100), loss = 0.415631 (0.214 sec/batch), lr: 0.177931
2019-12-27 15:38:37.119935: step 66620/136300 (epoch 49/100), loss = 0.224400 (0.232 sec/batch), lr: 0.177931
2019-12-27 15:38:43.444030: step 66640/136300 (epoch 49/100), loss = 0.305571 (0.227 sec/batch), lr: 0.177931
2019-12-27 15:38:48.353112: step 66660/136300 (epoch 49/100), loss = 0.348901 (0.213 sec/batch), lr: 0.177931
2019-12-27 15:38:53.238536: step 66680/136300 (epoch 49/100), loss = 0.213050 (0.205 sec/batch), lr: 0.177931
2019-12-27 15:38:57.919540: step 66700/136300 (epoch 49/100), loss = 0.248163 (0.245 sec/batch), lr: 0.177931
2019-12-27 15:39:02.773824: step 66720/136300 (epoch 49/100), loss = 0.374297 (0.244 sec/batch), lr: 0.177931
2019-12-27 15:39:07.563521: step 66740/136300 (epoch 49/100), loss = 0.209305 (0.201 sec/batch), lr: 0.177931
2019-12-27 15:39:12.515028: step 66760/136300 (epoch 49/100), loss = 0.322478 (0.230 sec/batch), lr: 0.177931
2019-12-27 15:39:17.403195: step 66780/136300 (epoch 49/100), loss = 0.230076 (0.244 sec/batch), lr: 0.177931
Evaluating on dev set...
Precision (micro): 74.496%
   Recall (micro): 58.462%
       F1 (micro): 65.512%
epoch 49: train_loss = 0.270026, dev_loss = 0.441771, dev_f1 = 0.6551
model saved to ./saved_models/01/checkpoint_epoch_49.pt

2019-12-27 15:39:55.734947: step 66800/136300 (epoch 50/100), loss = 0.397317 (0.237 sec/batch), lr: 0.160138
2019-12-27 15:40:01.800646: step 66820/136300 (epoch 50/100), loss = 0.307508 (0.250 sec/batch), lr: 0.160138
2019-12-27 15:40:06.631259: step 66840/136300 (epoch 50/100), loss = 0.135634 (0.244 sec/batch), lr: 0.160138
2019-12-27 15:40:11.359363: step 66860/136300 (epoch 50/100), loss = 0.161882 (0.222 sec/batch), lr: 0.160138
2019-12-27 15:40:16.292042: step 66880/136300 (epoch 50/100), loss = 0.264195 (0.237 sec/batch), lr: 0.160138
2019-12-27 15:40:21.023019: step 66900/136300 (epoch 50/100), loss = 0.213977 (0.220 sec/batch), lr: 0.160138
2019-12-27 15:40:25.848290: step 66920/136300 (epoch 50/100), loss = 0.267843 (0.237 sec/batch), lr: 0.160138
2019-12-27 15:40:30.631571: step 66940/136300 (epoch 50/100), loss = 0.331202 (0.183 sec/batch), lr: 0.160138
2019-12-27 15:40:35.525365: step 66960/136300 (epoch 50/100), loss = 0.123308 (0.233 sec/batch), lr: 0.160138
2019-12-27 15:40:40.420547: step 66980/136300 (epoch 50/100), loss = 0.251215 (0.206 sec/batch), lr: 0.160138
2019-12-27 15:40:46.937594: step 67000/136300 (epoch 50/100), loss = 0.153265 (1.900 sec/batch), lr: 0.160138
2019-12-27 15:40:51.646230: step 67020/136300 (epoch 50/100), loss = 0.328153 (0.181 sec/batch), lr: 0.160138
2019-12-27 15:40:56.446575: step 67040/136300 (epoch 50/100), loss = 0.214614 (0.242 sec/batch), lr: 0.160138
2019-12-27 15:41:01.350504: step 67060/136300 (epoch 50/100), loss = 0.332956 (0.231 sec/batch), lr: 0.160138
2019-12-27 15:41:06.173001: step 67080/136300 (epoch 50/100), loss = 0.176174 (0.241 sec/batch), lr: 0.160138
2019-12-27 15:41:10.885235: step 67100/136300 (epoch 50/100), loss = 0.580038 (0.201 sec/batch), lr: 0.160138
2019-12-27 15:41:15.715391: step 67120/136300 (epoch 50/100), loss = 0.235018 (0.235 sec/batch), lr: 0.160138
2019-12-27 15:41:20.568442: step 67140/136300 (epoch 50/100), loss = 0.150416 (0.246 sec/batch), lr: 0.160138
2019-12-27 15:41:25.322332: step 67160/136300 (epoch 50/100), loss = 0.271789 (0.181 sec/batch), lr: 0.160138
2019-12-27 15:41:30.084027: step 67180/136300 (epoch 50/100), loss = 0.288541 (0.211 sec/batch), lr: 0.160138
2019-12-27 15:41:36.383799: step 67200/136300 (epoch 50/100), loss = 0.159328 (0.233 sec/batch), lr: 0.160138
2019-12-27 15:41:41.214682: step 67220/136300 (epoch 50/100), loss = 0.427362 (0.237 sec/batch), lr: 0.160138
2019-12-27 15:41:46.106472: step 67240/136300 (epoch 50/100), loss = 0.273007 (0.215 sec/batch), lr: 0.160138
2019-12-27 15:41:50.943673: step 67260/136300 (epoch 50/100), loss = 0.187078 (0.202 sec/batch), lr: 0.160138
2019-12-27 15:41:55.789685: step 67280/136300 (epoch 50/100), loss = 0.231607 (0.228 sec/batch), lr: 0.160138
2019-12-27 15:42:00.685857: step 67300/136300 (epoch 50/100), loss = 0.419329 (0.246 sec/batch), lr: 0.160138
2019-12-27 15:42:05.545297: step 67320/136300 (epoch 50/100), loss = 0.366286 (0.225 sec/batch), lr: 0.160138
2019-12-27 15:42:10.401664: step 67340/136300 (epoch 50/100), loss = 0.206064 (0.217 sec/batch), lr: 0.160138
2019-12-27 15:42:15.240800: step 67360/136300 (epoch 50/100), loss = 0.275204 (0.239 sec/batch), lr: 0.160138
2019-12-27 15:42:20.041695: step 67380/136300 (epoch 50/100), loss = 0.480848 (0.231 sec/batch), lr: 0.160138
2019-12-27 15:42:26.269738: step 67400/136300 (epoch 50/100), loss = 0.316800 (0.219 sec/batch), lr: 0.160138
2019-12-27 15:42:31.067754: step 67420/136300 (epoch 50/100), loss = 0.199767 (0.219 sec/batch), lr: 0.160138
2019-12-27 15:42:35.829148: step 67440/136300 (epoch 50/100), loss = 0.315525 (0.238 sec/batch), lr: 0.160138
2019-12-27 15:42:40.664934: step 67460/136300 (epoch 50/100), loss = 0.236237 (0.241 sec/batch), lr: 0.160138
2019-12-27 15:42:45.578500: step 67480/136300 (epoch 50/100), loss = 0.223435 (0.222 sec/batch), lr: 0.160138
2019-12-27 15:42:50.466394: step 67500/136300 (epoch 50/100), loss = 0.206624 (0.232 sec/batch), lr: 0.160138
2019-12-27 15:42:55.398814: step 67520/136300 (epoch 50/100), loss = 0.279877 (0.238 sec/batch), lr: 0.160138
2019-12-27 15:43:00.247404: step 67540/136300 (epoch 50/100), loss = 0.412639 (0.228 sec/batch), lr: 0.160138
2019-12-27 15:43:04.998976: step 67560/136300 (epoch 50/100), loss = 0.343447 (0.215 sec/batch), lr: 0.160138
2019-12-27 15:43:09.946630: step 67580/136300 (epoch 50/100), loss = 0.283465 (0.248 sec/batch), lr: 0.160138
2019-12-27 15:43:16.346011: step 67600/136300 (epoch 50/100), loss = 0.346971 (0.220 sec/batch), lr: 0.160138
2019-12-27 15:43:21.138715: step 67620/136300 (epoch 50/100), loss = 0.464672 (0.223 sec/batch), lr: 0.160138
2019-12-27 15:43:26.058626: step 67640/136300 (epoch 50/100), loss = 0.394363 (0.214 sec/batch), lr: 0.160138
2019-12-27 15:43:30.738257: step 67660/136300 (epoch 50/100), loss = 0.266512 (0.229 sec/batch), lr: 0.160138
2019-12-27 15:43:35.637026: step 67680/136300 (epoch 50/100), loss = 0.192430 (0.219 sec/batch), lr: 0.160138
2019-12-27 15:43:40.577192: step 67700/136300 (epoch 50/100), loss = 0.198758 (0.232 sec/batch), lr: 0.160138
2019-12-27 15:43:45.376876: step 67720/136300 (epoch 50/100), loss = 0.251377 (0.227 sec/batch), lr: 0.160138
2019-12-27 15:43:50.328179: step 67740/136300 (epoch 50/100), loss = 0.425991 (0.233 sec/batch), lr: 0.160138
2019-12-27 15:43:55.195118: step 67760/136300 (epoch 50/100), loss = 0.418548 (0.206 sec/batch), lr: 0.160138
2019-12-27 15:44:00.043829: step 67780/136300 (epoch 50/100), loss = 0.287250 (0.229 sec/batch), lr: 0.160138
2019-12-27 15:44:06.263906: step 67800/136300 (epoch 50/100), loss = 0.139966 (0.240 sec/batch), lr: 0.160138
2019-12-27 15:44:11.091499: step 67820/136300 (epoch 50/100), loss = 0.337657 (0.180 sec/batch), lr: 0.160138
2019-12-27 15:44:15.782145: step 67840/136300 (epoch 50/100), loss = 0.102153 (0.235 sec/batch), lr: 0.160138
2019-12-27 15:44:20.513494: step 67860/136300 (epoch 50/100), loss = 0.320776 (0.220 sec/batch), lr: 0.160138
2019-12-27 15:44:25.370604: step 67880/136300 (epoch 50/100), loss = 0.279577 (0.242 sec/batch), lr: 0.160138
2019-12-27 15:44:30.183878: step 67900/136300 (epoch 50/100), loss = 0.279803 (0.214 sec/batch), lr: 0.160138
2019-12-27 15:44:35.088832: step 67920/136300 (epoch 50/100), loss = 0.384235 (0.205 sec/batch), lr: 0.160138
2019-12-27 15:44:39.842576: step 67940/136300 (epoch 50/100), loss = 0.244440 (0.215 sec/batch), lr: 0.160138
2019-12-27 15:44:44.574857: step 67960/136300 (epoch 50/100), loss = 0.270036 (0.247 sec/batch), lr: 0.160138
2019-12-27 15:44:49.412376: step 67980/136300 (epoch 50/100), loss = 0.381209 (0.233 sec/batch), lr: 0.160138
2019-12-27 15:44:55.681346: step 68000/136300 (epoch 50/100), loss = 0.268337 (0.242 sec/batch), lr: 0.160138
2019-12-27 15:45:00.591221: step 68020/136300 (epoch 50/100), loss = 0.356035 (0.247 sec/batch), lr: 0.160138
2019-12-27 15:45:05.477431: step 68040/136300 (epoch 50/100), loss = 0.128537 (0.206 sec/batch), lr: 0.160138
2019-12-27 15:45:10.114521: step 68060/136300 (epoch 50/100), loss = 0.405677 (0.244 sec/batch), lr: 0.160138
2019-12-27 15:45:14.922869: step 68080/136300 (epoch 50/100), loss = 0.291818 (0.207 sec/batch), lr: 0.160138
2019-12-27 15:45:19.720619: step 68100/136300 (epoch 50/100), loss = 0.114133 (0.241 sec/batch), lr: 0.160138
2019-12-27 15:45:24.629024: step 68120/136300 (epoch 50/100), loss = 0.179649 (0.238 sec/batch), lr: 0.160138
2019-12-27 15:45:29.486405: step 68140/136300 (epoch 50/100), loss = 0.343121 (0.245 sec/batch), lr: 0.160138
Evaluating on dev set...
Precision (micro): 70.869%
   Recall (micro): 62.252%
       F1 (micro): 66.281%
epoch 50: train_loss = 0.268885, dev_loss = 0.445807, dev_f1 = 0.6628
model saved to ./saved_models/01/checkpoint_epoch_50.pt
new best model saved.

2019-12-27 15:46:07.673906: step 68160/136300 (epoch 51/100), loss = 0.213487 (0.176 sec/batch), lr: 0.160138
2019-12-27 15:46:14.162416: step 68180/136300 (epoch 51/100), loss = 0.224318 (0.244 sec/batch), lr: 0.160138
2019-12-27 15:46:18.959538: step 68200/136300 (epoch 51/100), loss = 0.324943 (0.216 sec/batch), lr: 0.160138
2019-12-27 15:46:23.685459: step 68220/136300 (epoch 51/100), loss = 0.320306 (0.232 sec/batch), lr: 0.160138
2019-12-27 15:46:28.635441: step 68240/136300 (epoch 51/100), loss = 0.164990 (0.238 sec/batch), lr: 0.160138
2019-12-27 15:46:33.416207: step 68260/136300 (epoch 51/100), loss = 0.125256 (0.198 sec/batch), lr: 0.160138
2019-12-27 15:46:38.173082: step 68280/136300 (epoch 51/100), loss = 0.289252 (0.249 sec/batch), lr: 0.160138
2019-12-27 15:46:42.978213: step 68300/136300 (epoch 51/100), loss = 0.304349 (0.164 sec/batch), lr: 0.160138
2019-12-27 15:46:47.841554: step 68320/136300 (epoch 51/100), loss = 0.433248 (0.242 sec/batch), lr: 0.160138
2019-12-27 15:46:52.724686: step 68340/136300 (epoch 51/100), loss = 0.353753 (0.224 sec/batch), lr: 0.160138
2019-12-27 15:46:57.505347: step 68360/136300 (epoch 51/100), loss = 0.390933 (0.237 sec/batch), lr: 0.160138
2019-12-27 15:47:03.700822: step 68380/136300 (epoch 51/100), loss = 0.336611 (0.240 sec/batch), lr: 0.160138
2019-12-27 15:47:08.477563: step 68400/136300 (epoch 51/100), loss = 0.247482 (0.175 sec/batch), lr: 0.160138
2019-12-27 15:47:13.366569: step 68420/136300 (epoch 51/100), loss = 0.416062 (0.210 sec/batch), lr: 0.160138
2019-12-27 15:47:18.200747: step 68440/136300 (epoch 51/100), loss = 0.281155 (0.246 sec/batch), lr: 0.160138
2019-12-27 15:47:22.932511: step 68460/136300 (epoch 51/100), loss = 0.308666 (0.238 sec/batch), lr: 0.160138
2019-12-27 15:47:27.723138: step 68480/136300 (epoch 51/100), loss = 0.329828 (0.206 sec/batch), lr: 0.160138
2019-12-27 15:47:32.567202: step 68500/136300 (epoch 51/100), loss = 0.267944 (0.229 sec/batch), lr: 0.160138
2019-12-27 15:47:37.425997: step 68520/136300 (epoch 51/100), loss = 0.182071 (0.182 sec/batch), lr: 0.160138
2019-12-27 15:47:42.147762: step 68540/136300 (epoch 51/100), loss = 0.392988 (0.204 sec/batch), lr: 0.160138
2019-12-27 15:47:48.514528: step 68560/136300 (epoch 51/100), loss = 0.085936 (0.210 sec/batch), lr: 0.160138
2019-12-27 15:47:53.293105: step 68580/136300 (epoch 51/100), loss = 0.181959 (0.234 sec/batch), lr: 0.160138
2019-12-27 15:47:58.231637: step 68600/136300 (epoch 51/100), loss = 0.174918 (0.229 sec/batch), lr: 0.160138
2019-12-27 15:48:03.106668: step 68620/136300 (epoch 51/100), loss = 0.157133 (0.244 sec/batch), lr: 0.160138
2019-12-27 15:48:07.870682: step 68640/136300 (epoch 51/100), loss = 0.363000 (0.226 sec/batch), lr: 0.160138
2019-12-27 15:48:12.751988: step 68660/136300 (epoch 51/100), loss = 0.163584 (0.239 sec/batch), lr: 0.160138
2019-12-27 15:48:17.615173: step 68680/136300 (epoch 51/100), loss = 0.274896 (0.241 sec/batch), lr: 0.160138
2019-12-27 15:48:22.471163: step 68700/136300 (epoch 51/100), loss = 0.373662 (0.239 sec/batch), lr: 0.160138
2019-12-27 15:48:27.280208: step 68720/136300 (epoch 51/100), loss = 0.267305 (0.207 sec/batch), lr: 0.160138
2019-12-27 15:48:32.120933: step 68740/136300 (epoch 51/100), loss = 0.248182 (0.243 sec/batch), lr: 0.160138
2019-12-27 15:48:38.143551: step 68760/136300 (epoch 51/100), loss = 0.274369 (0.248 sec/batch), lr: 0.160138
2019-12-27 15:48:42.950152: step 68780/136300 (epoch 51/100), loss = 0.201781 (0.240 sec/batch), lr: 0.160138
2019-12-27 15:48:47.687784: step 68800/136300 (epoch 51/100), loss = 0.186532 (0.221 sec/batch), lr: 0.160138
2019-12-27 15:48:52.543487: step 68820/136300 (epoch 51/100), loss = 0.216914 (0.230 sec/batch), lr: 0.160138
2019-12-27 15:48:57.449207: step 68840/136300 (epoch 51/100), loss = 0.188243 (0.231 sec/batch), lr: 0.160138
2019-12-27 15:49:02.340890: step 68860/136300 (epoch 51/100), loss = 0.283632 (0.214 sec/batch), lr: 0.160138
2019-12-27 15:49:07.251580: step 68880/136300 (epoch 51/100), loss = 0.204111 (0.210 sec/batch), lr: 0.160138
2019-12-27 15:49:12.147724: step 68900/136300 (epoch 51/100), loss = 0.222981 (0.201 sec/batch), lr: 0.160138
2019-12-27 15:49:16.872562: step 68920/136300 (epoch 51/100), loss = 0.063482 (0.221 sec/batch), lr: 0.160138
2019-12-27 15:49:21.783745: step 68940/136300 (epoch 51/100), loss = 0.274199 (0.227 sec/batch), lr: 0.160138
2019-12-27 15:49:28.332031: step 68960/136300 (epoch 51/100), loss = 0.172034 (0.237 sec/batch), lr: 0.160138
2019-12-27 15:49:33.147466: step 68980/136300 (epoch 51/100), loss = 0.154296 (0.236 sec/batch), lr: 0.160138
2019-12-27 15:49:38.090952: step 69000/136300 (epoch 51/100), loss = 0.233636 (0.231 sec/batch), lr: 0.160138
2019-12-27 15:49:42.749374: step 69020/136300 (epoch 51/100), loss = 0.221336 (0.222 sec/batch), lr: 0.160138
2019-12-27 15:49:47.610216: step 69040/136300 (epoch 51/100), loss = 0.256179 (0.238 sec/batch), lr: 0.160138
2019-12-27 15:49:52.521109: step 69060/136300 (epoch 51/100), loss = 0.227597 (0.227 sec/batch), lr: 0.160138
2019-12-27 15:49:57.336244: step 69080/136300 (epoch 51/100), loss = 0.259252 (0.239 sec/batch), lr: 0.160138
2019-12-27 15:50:02.238196: step 69100/136300 (epoch 51/100), loss = 0.121847 (0.241 sec/batch), lr: 0.160138
2019-12-27 15:50:07.170045: step 69120/136300 (epoch 51/100), loss = 0.521991 (0.221 sec/batch), lr: 0.160138
2019-12-27 15:50:11.976801: step 69140/136300 (epoch 51/100), loss = 0.160160 (0.240 sec/batch), lr: 0.160138
2019-12-27 15:50:18.361148: step 69160/136300 (epoch 51/100), loss = 0.230878 (0.243 sec/batch), lr: 0.160138
2019-12-27 15:50:23.235549: step 69180/136300 (epoch 51/100), loss = 0.156779 (0.193 sec/batch), lr: 0.160138
2019-12-27 15:50:27.868255: step 69200/136300 (epoch 51/100), loss = 0.210218 (0.220 sec/batch), lr: 0.160138
2019-12-27 15:50:32.581330: step 69220/136300 (epoch 51/100), loss = 0.245225 (0.210 sec/batch), lr: 0.160138
2019-12-27 15:50:37.449009: step 69240/136300 (epoch 51/100), loss = 0.190423 (0.217 sec/batch), lr: 0.160138
2019-12-27 15:50:42.280179: step 69260/136300 (epoch 51/100), loss = 0.193551 (0.251 sec/batch), lr: 0.160138
2019-12-27 15:50:47.176736: step 69280/136300 (epoch 51/100), loss = 0.339952 (0.232 sec/batch), lr: 0.160138
2019-12-27 15:50:51.942159: step 69300/136300 (epoch 51/100), loss = 0.290383 (0.226 sec/batch), lr: 0.160138
2019-12-27 15:50:56.647109: step 69320/136300 (epoch 51/100), loss = 0.161490 (0.233 sec/batch), lr: 0.160138
2019-12-27 15:51:01.422212: step 69340/136300 (epoch 51/100), loss = 0.218019 (0.232 sec/batch), lr: 0.160138
2019-12-27 15:51:07.857748: step 69360/136300 (epoch 51/100), loss = 0.139005 (0.238 sec/batch), lr: 0.160138
2019-12-27 15:51:12.781268: step 69380/136300 (epoch 51/100), loss = 0.311872 (0.233 sec/batch), lr: 0.160138
2019-12-27 15:51:17.681986: step 69400/136300 (epoch 51/100), loss = 0.247325 (0.240 sec/batch), lr: 0.160138
2019-12-27 15:51:22.323332: step 69420/136300 (epoch 51/100), loss = 0.225376 (0.214 sec/batch), lr: 0.160138
2019-12-27 15:51:27.090443: step 69440/136300 (epoch 51/100), loss = 0.129088 (0.175 sec/batch), lr: 0.160138
2019-12-27 15:51:31.858231: step 69460/136300 (epoch 51/100), loss = 0.213661 (0.217 sec/batch), lr: 0.160138
2019-12-27 15:51:36.792885: step 69480/136300 (epoch 51/100), loss = 0.287210 (0.240 sec/batch), lr: 0.160138
2019-12-27 15:51:41.613016: step 69500/136300 (epoch 51/100), loss = 0.133774 (0.208 sec/batch), lr: 0.160138
Evaluating on dev set...
Precision (micro): 70.603%
   Recall (micro): 60.927%
       F1 (micro): 65.409%
epoch 51: train_loss = 0.266456, dev_loss = 0.450384, dev_f1 = 0.6541
model saved to ./saved_models/01/checkpoint_epoch_51.pt

2019-12-27 15:52:19.929938: step 69520/136300 (epoch 52/100), loss = 0.461976 (0.230 sec/batch), lr: 0.144124
2019-12-27 15:52:26.077825: step 69540/136300 (epoch 52/100), loss = 0.207712 (0.228 sec/batch), lr: 0.144124
2019-12-27 15:52:30.918428: step 69560/136300 (epoch 52/100), loss = 0.349464 (0.242 sec/batch), lr: 0.144124
2019-12-27 15:52:35.646483: step 69580/136300 (epoch 52/100), loss = 0.133590 (0.243 sec/batch), lr: 0.144124
2019-12-27 15:52:40.527232: step 69600/136300 (epoch 52/100), loss = 0.245393 (0.215 sec/batch), lr: 0.144124
2019-12-27 15:52:45.357017: step 69620/136300 (epoch 52/100), loss = 0.200048 (0.231 sec/batch), lr: 0.144124
2019-12-27 15:52:50.079773: step 69640/136300 (epoch 52/100), loss = 0.178722 (0.232 sec/batch), lr: 0.144124
2019-12-27 15:52:54.957621: step 69660/136300 (epoch 52/100), loss = 0.166376 (0.245 sec/batch), lr: 0.144124
2019-12-27 15:52:59.759860: step 69680/136300 (epoch 52/100), loss = 0.302938 (0.222 sec/batch), lr: 0.144124
2019-12-27 15:53:04.644704: step 69700/136300 (epoch 52/100), loss = 0.167468 (0.232 sec/batch), lr: 0.144124
2019-12-27 15:53:09.454420: step 69720/136300 (epoch 52/100), loss = 0.164808 (0.238 sec/batch), lr: 0.144124
2019-12-27 15:53:15.710229: step 69740/136300 (epoch 52/100), loss = 0.413236 (0.215 sec/batch), lr: 0.144124
2019-12-27 15:53:20.558207: step 69760/136300 (epoch 52/100), loss = 0.124415 (0.211 sec/batch), lr: 0.144124
2019-12-27 15:53:25.370902: step 69780/136300 (epoch 52/100), loss = 0.108234 (0.240 sec/batch), lr: 0.144124
2019-12-27 15:53:30.166271: step 69800/136300 (epoch 52/100), loss = 0.374463 (0.241 sec/batch), lr: 0.144124
2019-12-27 15:53:34.981256: step 69820/136300 (epoch 52/100), loss = 0.223420 (0.215 sec/batch), lr: 0.144124
2019-12-27 15:53:39.743842: step 69840/136300 (epoch 52/100), loss = 0.252978 (0.245 sec/batch), lr: 0.144124
2019-12-27 15:53:44.547029: step 69860/136300 (epoch 52/100), loss = 0.212050 (0.239 sec/batch), lr: 0.144124
2019-12-27 15:53:49.405590: step 69880/136300 (epoch 52/100), loss = 0.236691 (0.177 sec/batch), lr: 0.144124
2019-12-27 15:53:54.173377: step 69900/136300 (epoch 52/100), loss = 0.225734 (0.241 sec/batch), lr: 0.144124
2019-12-27 15:53:58.789625: step 69920/136300 (epoch 52/100), loss = 0.205929 (0.233 sec/batch), lr: 0.144124
2019-12-27 15:54:05.048101: step 69940/136300 (epoch 52/100), loss = 0.137293 (0.244 sec/batch), lr: 0.144124
2019-12-27 15:54:09.978072: step 69960/136300 (epoch 52/100), loss = 0.147544 (0.237 sec/batch), lr: 0.144124
2019-12-27 15:54:14.870589: step 69980/136300 (epoch 52/100), loss = 0.202271 (0.232 sec/batch), lr: 0.144124
2019-12-27 15:54:19.627249: step 70000/136300 (epoch 52/100), loss = 0.183958 (0.230 sec/batch), lr: 0.144124
2019-12-27 15:54:24.471439: step 70020/136300 (epoch 52/100), loss = 0.093778 (0.227 sec/batch), lr: 0.144124
2019-12-27 15:54:29.325688: step 70040/136300 (epoch 52/100), loss = 0.227758 (0.231 sec/batch), lr: 0.144124
2019-12-27 15:54:34.279336: step 70060/136300 (epoch 52/100), loss = 0.278812 (0.242 sec/batch), lr: 0.144124
2019-12-27 15:54:39.075186: step 70080/136300 (epoch 52/100), loss = 0.340789 (0.242 sec/batch), lr: 0.144124
2019-12-27 15:54:43.832365: step 70100/136300 (epoch 52/100), loss = 0.122551 (0.236 sec/batch), lr: 0.144124
2019-12-27 15:54:49.980626: step 70120/136300 (epoch 52/100), loss = 0.424291 (1.633 sec/batch), lr: 0.144124
2019-12-27 15:54:54.709893: step 70140/136300 (epoch 52/100), loss = 0.227925 (0.218 sec/batch), lr: 0.144124
2019-12-27 15:54:59.473298: step 70160/136300 (epoch 52/100), loss = 0.240131 (0.207 sec/batch), lr: 0.144124
2019-12-27 15:55:04.282701: step 70180/136300 (epoch 52/100), loss = 0.253918 (0.238 sec/batch), lr: 0.144124
2019-12-27 15:55:09.167086: step 70200/136300 (epoch 52/100), loss = 0.178946 (0.238 sec/batch), lr: 0.144124
2019-12-27 15:55:14.087224: step 70220/136300 (epoch 52/100), loss = 0.093316 (0.235 sec/batch), lr: 0.144124
2019-12-27 15:55:18.983948: step 70240/136300 (epoch 52/100), loss = 0.335623 (0.240 sec/batch), lr: 0.144124
2019-12-27 15:55:23.875245: step 70260/136300 (epoch 52/100), loss = 0.336478 (0.200 sec/batch), lr: 0.144124
2019-12-27 15:55:28.621447: step 70280/136300 (epoch 52/100), loss = 0.313769 (0.201 sec/batch), lr: 0.144124
2019-12-27 15:55:33.469853: step 70300/136300 (epoch 52/100), loss = 0.377545 (0.248 sec/batch), lr: 0.144124
2019-12-27 15:55:39.793973: step 70320/136300 (epoch 52/100), loss = 0.198718 (0.243 sec/batch), lr: 0.144124
2019-12-27 15:55:44.516689: step 70340/136300 (epoch 52/100), loss = 0.191962 (0.239 sec/batch), lr: 0.144124
2019-12-27 15:55:49.480340: step 70360/136300 (epoch 52/100), loss = 0.320026 (0.235 sec/batch), lr: 0.144124
2019-12-27 15:55:54.196189: step 70380/136300 (epoch 52/100), loss = 0.377152 (0.235 sec/batch), lr: 0.144124
2019-12-27 15:55:58.977422: step 70400/136300 (epoch 52/100), loss = 0.229456 (0.241 sec/batch), lr: 0.144124
2019-12-27 15:56:03.878957: step 70420/136300 (epoch 52/100), loss = 0.240747 (0.243 sec/batch), lr: 0.144124
2019-12-27 15:56:08.696495: step 70440/136300 (epoch 52/100), loss = 0.311188 (0.161 sec/batch), lr: 0.144124
2019-12-27 15:56:13.563961: step 70460/136300 (epoch 52/100), loss = 0.437904 (0.239 sec/batch), lr: 0.144124
2019-12-27 15:56:18.524485: step 70480/136300 (epoch 52/100), loss = 0.263097 (0.223 sec/batch), lr: 0.144124
2019-12-27 15:56:23.380141: step 70500/136300 (epoch 52/100), loss = 0.454531 (0.230 sec/batch), lr: 0.144124
2019-12-27 15:56:29.786272: step 70520/136300 (epoch 52/100), loss = 0.463583 (0.221 sec/batch), lr: 0.144124
2019-12-27 15:56:34.684861: step 70540/136300 (epoch 52/100), loss = 0.254286 (0.238 sec/batch), lr: 0.144124
2019-12-27 15:56:39.293501: step 70560/136300 (epoch 52/100), loss = 0.305001 (0.229 sec/batch), lr: 0.144124
2019-12-27 15:56:44.050108: step 70580/136300 (epoch 52/100), loss = 0.176316 (0.234 sec/batch), lr: 0.144124
2019-12-27 15:56:48.859688: step 70600/136300 (epoch 52/100), loss = 0.334203 (0.238 sec/batch), lr: 0.144124
2019-12-27 15:56:53.655202: step 70620/136300 (epoch 52/100), loss = 0.346480 (0.243 sec/batch), lr: 0.144124
2019-12-27 15:56:58.518203: step 70640/136300 (epoch 52/100), loss = 0.311305 (0.198 sec/batch), lr: 0.144124
2019-12-27 15:57:03.285661: step 70660/136300 (epoch 52/100), loss = 0.404165 (0.234 sec/batch), lr: 0.144124
2019-12-27 15:57:08.029485: step 70680/136300 (epoch 52/100), loss = 0.294165 (0.217 sec/batch), lr: 0.144124
2019-12-27 15:57:12.765448: step 70700/136300 (epoch 52/100), loss = 0.238955 (0.163 sec/batch), lr: 0.144124
2019-12-27 15:57:19.081715: step 70720/136300 (epoch 52/100), loss = 0.274785 (0.234 sec/batch), lr: 0.144124
2019-12-27 15:57:23.998612: step 70740/136300 (epoch 52/100), loss = 0.215210 (0.230 sec/batch), lr: 0.144124
2019-12-27 15:57:28.867388: step 70760/136300 (epoch 52/100), loss = 0.329158 (0.229 sec/batch), lr: 0.144124
2019-12-27 15:57:33.594849: step 70780/136300 (epoch 52/100), loss = 0.127374 (0.172 sec/batch), lr: 0.144124
2019-12-27 15:57:38.351075: step 70800/136300 (epoch 52/100), loss = 0.177437 (0.197 sec/batch), lr: 0.144124
2019-12-27 15:57:43.109794: step 70820/136300 (epoch 52/100), loss = 0.102565 (0.199 sec/batch), lr: 0.144124
2019-12-27 15:57:48.028548: step 70840/136300 (epoch 52/100), loss = 0.313732 (0.234 sec/batch), lr: 0.144124
2019-12-27 15:57:52.904219: step 70860/136300 (epoch 52/100), loss = 0.203075 (0.224 sec/batch), lr: 0.144124
Evaluating on dev set...
Precision (micro): 72.185%
   Recall (micro): 62.160%
       F1 (micro): 66.798%
epoch 52: train_loss = 0.262899, dev_loss = 0.443912, dev_f1 = 0.6680
model saved to ./saved_models/01/checkpoint_epoch_52.pt
new best model saved.

2019-12-27 15:58:31.190017: step 70880/136300 (epoch 53/100), loss = 0.211721 (0.205 sec/batch), lr: 0.144124
2019-12-27 15:58:37.355439: step 70900/136300 (epoch 53/100), loss = 0.451880 (0.223 sec/batch), lr: 0.144124
2019-12-27 15:58:42.158329: step 70920/136300 (epoch 53/100), loss = 0.379877 (0.221 sec/batch), lr: 0.144124
2019-12-27 15:58:46.920684: step 70940/136300 (epoch 53/100), loss = 0.332872 (0.198 sec/batch), lr: 0.144124
2019-12-27 15:58:51.841149: step 70960/136300 (epoch 53/100), loss = 0.260536 (0.232 sec/batch), lr: 0.144124
2019-12-27 15:58:56.605939: step 70980/136300 (epoch 53/100), loss = 0.206104 (0.218 sec/batch), lr: 0.144124
2019-12-27 15:59:01.386141: step 71000/136300 (epoch 53/100), loss = 0.290852 (0.206 sec/batch), lr: 0.144124
2019-12-27 15:59:06.258640: step 71020/136300 (epoch 53/100), loss = 0.719292 (0.216 sec/batch), lr: 0.144124
2019-12-27 15:59:11.085270: step 71040/136300 (epoch 53/100), loss = 0.169176 (0.243 sec/batch), lr: 0.144124
2019-12-27 15:59:15.962440: step 71060/136300 (epoch 53/100), loss = 0.320801 (0.233 sec/batch), lr: 0.144124
2019-12-27 15:59:20.762217: step 71080/136300 (epoch 53/100), loss = 0.186702 (0.175 sec/batch), lr: 0.144124
2019-12-27 15:59:27.115921: step 71100/136300 (epoch 53/100), loss = 0.368589 (0.185 sec/batch), lr: 0.144124
2019-12-27 15:59:31.956069: step 71120/136300 (epoch 53/100), loss = 0.301344 (0.235 sec/batch), lr: 0.144124
2019-12-27 15:59:36.718439: step 71140/136300 (epoch 53/100), loss = 0.155273 (0.233 sec/batch), lr: 0.144124
2019-12-27 15:59:41.531376: step 71160/136300 (epoch 53/100), loss = 0.150958 (0.202 sec/batch), lr: 0.144124
2019-12-27 15:59:46.393593: step 71180/136300 (epoch 53/100), loss = 0.493380 (0.221 sec/batch), lr: 0.144124
2019-12-27 15:59:51.091137: step 71200/136300 (epoch 53/100), loss = 0.168574 (0.204 sec/batch), lr: 0.144124
2019-12-27 15:59:55.888607: step 71220/136300 (epoch 53/100), loss = 0.249739 (0.233 sec/batch), lr: 0.144124
2019-12-27 16:00:00.796998: step 71240/136300 (epoch 53/100), loss = 0.252609 (0.248 sec/batch), lr: 0.144124
2019-12-27 16:00:05.525465: step 71260/136300 (epoch 53/100), loss = 0.232772 (0.230 sec/batch), lr: 0.144124
2019-12-27 16:00:10.167122: step 71280/136300 (epoch 53/100), loss = 0.257012 (0.168 sec/batch), lr: 0.144124
2019-12-27 16:00:16.392411: step 71300/136300 (epoch 53/100), loss = 0.138078 (0.202 sec/batch), lr: 0.144124
2019-12-27 16:00:21.314155: step 71320/136300 (epoch 53/100), loss = 0.417533 (0.212 sec/batch), lr: 0.144124
2019-12-27 16:00:26.212155: step 71340/136300 (epoch 53/100), loss = 0.332323 (0.226 sec/batch), lr: 0.144124
2019-12-27 16:00:31.014742: step 71360/136300 (epoch 53/100), loss = 0.258914 (0.218 sec/batch), lr: 0.144124
2019-12-27 16:00:35.842029: step 71380/136300 (epoch 53/100), loss = 0.175212 (0.230 sec/batch), lr: 0.144124
2019-12-27 16:00:40.711623: step 71400/136300 (epoch 53/100), loss = 0.198852 (0.229 sec/batch), lr: 0.144124
2019-12-27 16:00:45.626441: step 71420/136300 (epoch 53/100), loss = 0.272590 (0.176 sec/batch), lr: 0.144124
2019-12-27 16:00:50.390745: step 71440/136300 (epoch 53/100), loss = 0.169174 (0.180 sec/batch), lr: 0.144124
2019-12-27 16:00:55.214684: step 71460/136300 (epoch 53/100), loss = 0.182032 (0.233 sec/batch), lr: 0.144124
2019-12-27 16:00:59.968117: step 71480/136300 (epoch 53/100), loss = 0.297404 (0.220 sec/batch), lr: 0.144124
2019-12-27 16:01:06.146910: step 71500/136300 (epoch 53/100), loss = 0.203041 (0.227 sec/batch), lr: 0.144124
2019-12-27 16:01:10.901047: step 71520/136300 (epoch 53/100), loss = 0.257197 (0.203 sec/batch), lr: 0.144124
2019-12-27 16:01:15.675107: step 71540/136300 (epoch 53/100), loss = 0.386132 (0.201 sec/batch), lr: 0.144124
2019-12-27 16:01:20.570807: step 71560/136300 (epoch 53/100), loss = 0.250861 (0.235 sec/batch), lr: 0.144124
2019-12-27 16:01:25.450965: step 71580/136300 (epoch 53/100), loss = 0.141419 (0.216 sec/batch), lr: 0.144124
2019-12-27 16:01:30.361623: step 71600/136300 (epoch 53/100), loss = 0.245538 (0.204 sec/batch), lr: 0.144124
2019-12-27 16:01:35.257941: step 71620/136300 (epoch 53/100), loss = 0.177336 (0.238 sec/batch), lr: 0.144124
2019-12-27 16:01:40.022316: step 71640/136300 (epoch 53/100), loss = 0.125116 (0.241 sec/batch), lr: 0.144124
2019-12-27 16:01:44.849915: step 71660/136300 (epoch 53/100), loss = 0.236297 (0.246 sec/batch), lr: 0.144124
2019-12-27 16:01:51.127663: step 71680/136300 (epoch 53/100), loss = 0.276587 (0.231 sec/batch), lr: 0.144124
2019-12-27 16:01:55.894155: step 71700/136300 (epoch 53/100), loss = 0.276412 (0.238 sec/batch), lr: 0.144124
2019-12-27 16:02:00.802356: step 71720/136300 (epoch 53/100), loss = 0.173453 (0.233 sec/batch), lr: 0.144124
2019-12-27 16:02:05.584869: step 71740/136300 (epoch 53/100), loss = 0.216535 (0.210 sec/batch), lr: 0.144124
2019-12-27 16:02:10.323294: step 71760/136300 (epoch 53/100), loss = 0.386594 (0.234 sec/batch), lr: 0.144124
2019-12-27 16:02:15.217015: step 71780/136300 (epoch 53/100), loss = 0.189160 (0.231 sec/batch), lr: 0.144124
2019-12-27 16:02:20.169871: step 71800/136300 (epoch 53/100), loss = 0.228113 (0.241 sec/batch), lr: 0.144124
2019-12-27 16:02:24.949108: step 71820/136300 (epoch 53/100), loss = 0.266392 (0.219 sec/batch), lr: 0.144124
2019-12-27 16:02:29.900205: step 71840/136300 (epoch 53/100), loss = 0.264944 (0.233 sec/batch), lr: 0.144124
2019-12-27 16:02:34.813286: step 71860/136300 (epoch 53/100), loss = 0.201819 (0.216 sec/batch), lr: 0.144124
2019-12-27 16:02:41.213494: step 71880/136300 (epoch 53/100), loss = 0.190508 (0.216 sec/batch), lr: 0.144124
2019-12-27 16:02:46.101374: step 71900/136300 (epoch 53/100), loss = 0.163619 (0.243 sec/batch), lr: 0.144124
2019-12-27 16:02:50.736479: step 71920/136300 (epoch 53/100), loss = 0.272925 (0.229 sec/batch), lr: 0.144124
2019-12-27 16:02:55.505681: step 71940/136300 (epoch 53/100), loss = 0.211304 (0.228 sec/batch), lr: 0.144124
2019-12-27 16:03:00.361920: step 71960/136300 (epoch 53/100), loss = 0.366691 (0.227 sec/batch), lr: 0.144124
2019-12-27 16:03:05.115519: step 71980/136300 (epoch 53/100), loss = 0.163782 (0.244 sec/batch), lr: 0.144124
2019-12-27 16:03:10.027406: step 72000/136300 (epoch 53/100), loss = 0.371819 (0.243 sec/batch), lr: 0.144124
2019-12-27 16:03:14.795034: step 72020/136300 (epoch 53/100), loss = 0.531008 (0.199 sec/batch), lr: 0.144124
2019-12-27 16:03:19.542621: step 72040/136300 (epoch 53/100), loss = 0.266003 (0.169 sec/batch), lr: 0.144124
2019-12-27 16:03:24.294535: step 72060/136300 (epoch 53/100), loss = 0.328980 (0.233 sec/batch), lr: 0.144124
2019-12-27 16:03:30.513059: step 72080/136300 (epoch 53/100), loss = 0.350077 (0.231 sec/batch), lr: 0.144124
2019-12-27 16:03:35.395126: step 72100/136300 (epoch 53/100), loss = 0.267414 (0.233 sec/batch), lr: 0.144124
2019-12-27 16:03:40.293286: step 72120/136300 (epoch 53/100), loss = 0.338349 (0.245 sec/batch), lr: 0.144124
2019-12-27 16:03:45.088309: step 72140/136300 (epoch 53/100), loss = 0.142533 (0.217 sec/batch), lr: 0.144124
2019-12-27 16:03:49.819030: step 72160/136300 (epoch 53/100), loss = 0.190245 (0.239 sec/batch), lr: 0.144124
2019-12-27 16:03:54.569206: step 72180/136300 (epoch 53/100), loss = 0.123471 (0.226 sec/batch), lr: 0.144124
2019-12-27 16:03:59.454887: step 72200/136300 (epoch 53/100), loss = 0.406236 (0.242 sec/batch), lr: 0.144124
2019-12-27 16:04:04.368234: step 72220/136300 (epoch 53/100), loss = 0.272827 (0.225 sec/batch), lr: 0.144124
Evaluating on dev set...
Precision (micro): 72.234%
   Recall (micro): 61.019%
       F1 (micro): 66.155%
epoch 53: train_loss = 0.260991, dev_loss = 0.442300, dev_f1 = 0.6615
model saved to ./saved_models/01/checkpoint_epoch_53.pt

2019-12-27 16:04:42.637847: step 72240/136300 (epoch 54/100), loss = 0.322693 (0.176 sec/batch), lr: 0.129711
2019-12-27 16:04:48.829208: step 72260/136300 (epoch 54/100), loss = 0.392939 (0.196 sec/batch), lr: 0.129711
2019-12-27 16:04:53.584842: step 72280/136300 (epoch 54/100), loss = 0.162121 (0.241 sec/batch), lr: 0.129711
2019-12-27 16:04:58.409242: step 72300/136300 (epoch 54/100), loss = 0.250255 (0.239 sec/batch), lr: 0.129711
2019-12-27 16:05:03.272779: step 72320/136300 (epoch 54/100), loss = 0.094928 (0.234 sec/batch), lr: 0.129711
2019-12-27 16:05:08.028095: step 72340/136300 (epoch 54/100), loss = 0.251263 (0.201 sec/batch), lr: 0.129711
2019-12-27 16:05:12.751754: step 72360/136300 (epoch 54/100), loss = 0.268733 (0.175 sec/batch), lr: 0.129711
2019-12-27 16:05:17.573868: step 72380/136300 (epoch 54/100), loss = 0.248721 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:05:22.365653: step 72400/136300 (epoch 54/100), loss = 0.139074 (0.245 sec/batch), lr: 0.129711
2019-12-27 16:05:27.235018: step 72420/136300 (epoch 54/100), loss = 0.231770 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:05:32.026960: step 72440/136300 (epoch 54/100), loss = 0.215652 (0.204 sec/batch), lr: 0.129711
2019-12-27 16:05:38.277918: step 72460/136300 (epoch 54/100), loss = 0.208724 (0.201 sec/batch), lr: 0.129711
2019-12-27 16:05:43.000251: step 72480/136300 (epoch 54/100), loss = 0.202498 (0.221 sec/batch), lr: 0.129711
2019-12-27 16:05:47.800103: step 72500/136300 (epoch 54/100), loss = 0.372848 (0.245 sec/batch), lr: 0.129711
2019-12-27 16:05:52.561043: step 72520/136300 (epoch 54/100), loss = 0.187090 (0.220 sec/batch), lr: 0.129711
2019-12-27 16:05:57.357274: step 72540/136300 (epoch 54/100), loss = 0.251643 (0.216 sec/batch), lr: 0.129711
2019-12-27 16:06:02.096812: step 72560/136300 (epoch 54/100), loss = 0.140264 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:06:06.810344: step 72580/136300 (epoch 54/100), loss = 0.231224 (0.202 sec/batch), lr: 0.129711
2019-12-27 16:06:11.657051: step 72600/136300 (epoch 54/100), loss = 0.512461 (0.219 sec/batch), lr: 0.129711
2019-12-27 16:06:16.381878: step 72620/136300 (epoch 54/100), loss = 0.182704 (0.223 sec/batch), lr: 0.129711
2019-12-27 16:06:21.065544: step 72640/136300 (epoch 54/100), loss = 0.364359 (0.176 sec/batch), lr: 0.129711
2019-12-27 16:06:27.113847: step 72660/136300 (epoch 54/100), loss = 0.211037 (0.183 sec/batch), lr: 0.129711
2019-12-27 16:06:31.944486: step 72680/136300 (epoch 54/100), loss = 0.168545 (0.204 sec/batch), lr: 0.129711
2019-12-27 16:06:36.813679: step 72700/136300 (epoch 54/100), loss = 0.256578 (0.237 sec/batch), lr: 0.129711
2019-12-27 16:06:41.598359: step 72720/136300 (epoch 54/100), loss = 0.234007 (0.204 sec/batch), lr: 0.129711
2019-12-27 16:06:46.381009: step 72740/136300 (epoch 54/100), loss = 0.201829 (0.232 sec/batch), lr: 0.129711
2019-12-27 16:06:51.190873: step 72760/136300 (epoch 54/100), loss = 0.203265 (0.171 sec/batch), lr: 0.129711
2019-12-27 16:06:56.145812: step 72780/136300 (epoch 54/100), loss = 0.248480 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:07:00.870428: step 72800/136300 (epoch 54/100), loss = 0.248092 (0.199 sec/batch), lr: 0.129711
2019-12-27 16:07:05.682687: step 72820/136300 (epoch 54/100), loss = 0.362666 (0.219 sec/batch), lr: 0.129711
2019-12-27 16:07:10.359899: step 72840/136300 (epoch 54/100), loss = 0.206927 (0.222 sec/batch), lr: 0.129711
2019-12-27 16:07:16.589703: step 72860/136300 (epoch 54/100), loss = 0.305901 (0.227 sec/batch), lr: 0.129711
2019-12-27 16:07:21.357086: step 72880/136300 (epoch 54/100), loss = 0.220669 (0.232 sec/batch), lr: 0.129711
2019-12-27 16:07:26.097785: step 72900/136300 (epoch 54/100), loss = 0.195240 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:07:30.933684: step 72920/136300 (epoch 54/100), loss = 0.366479 (0.241 sec/batch), lr: 0.129711
2019-12-27 16:07:35.810279: step 72940/136300 (epoch 54/100), loss = 0.362769 (0.227 sec/batch), lr: 0.129711
2019-12-27 16:07:40.661313: step 72960/136300 (epoch 54/100), loss = 0.071136 (0.228 sec/batch), lr: 0.129711
2019-12-27 16:07:45.495529: step 72980/136300 (epoch 54/100), loss = 0.213806 (0.239 sec/batch), lr: 0.129711
2019-12-27 16:07:50.253813: step 73000/136300 (epoch 54/100), loss = 0.229012 (0.229 sec/batch), lr: 0.129711
2019-12-27 16:07:55.008697: step 73020/136300 (epoch 54/100), loss = 0.439158 (0.229 sec/batch), lr: 0.129711
2019-12-27 16:08:01.252074: step 73040/136300 (epoch 54/100), loss = 0.267725 (0.236 sec/batch), lr: 0.129711
2019-12-27 16:08:06.005795: step 73060/136300 (epoch 54/100), loss = 0.376346 (0.225 sec/batch), lr: 0.129711
2019-12-27 16:08:10.842847: step 73080/136300 (epoch 54/100), loss = 0.410562 (0.237 sec/batch), lr: 0.129711
2019-12-27 16:08:15.686179: step 73100/136300 (epoch 54/100), loss = 0.268318 (0.227 sec/batch), lr: 0.129711
2019-12-27 16:08:20.367416: step 73120/136300 (epoch 54/100), loss = 0.227588 (0.205 sec/batch), lr: 0.129711
2019-12-27 16:08:25.178039: step 73140/136300 (epoch 54/100), loss = 0.304346 (0.228 sec/batch), lr: 0.129711
2019-12-27 16:08:30.089781: step 73160/136300 (epoch 54/100), loss = 0.318940 (0.243 sec/batch), lr: 0.129711
2019-12-27 16:08:34.840818: step 73180/136300 (epoch 54/100), loss = 0.142202 (0.182 sec/batch), lr: 0.129711
2019-12-27 16:08:39.809544: step 73200/136300 (epoch 54/100), loss = 0.224792 (0.246 sec/batch), lr: 0.129711
2019-12-27 16:08:44.654823: step 73220/136300 (epoch 54/100), loss = 0.190074 (0.230 sec/batch), lr: 0.129711
2019-12-27 16:08:49.442287: step 73240/136300 (epoch 54/100), loss = 0.211872 (0.233 sec/batch), lr: 0.129711
2019-12-27 16:08:55.788696: step 73260/136300 (epoch 54/100), loss = 0.251527 (0.230 sec/batch), lr: 0.129711
2019-12-27 16:09:00.442936: step 73280/136300 (epoch 54/100), loss = 0.291450 (0.206 sec/batch), lr: 0.129711
2019-12-27 16:09:05.126208: step 73300/136300 (epoch 54/100), loss = 0.286244 (0.229 sec/batch), lr: 0.129711
2019-12-27 16:09:09.973161: step 73320/136300 (epoch 54/100), loss = 0.198792 (0.233 sec/batch), lr: 0.129711
2019-12-27 16:09:14.730047: step 73340/136300 (epoch 54/100), loss = 0.350213 (0.225 sec/batch), lr: 0.129711
2019-12-27 16:09:19.618082: step 73360/136300 (epoch 54/100), loss = 0.261774 (0.222 sec/batch), lr: 0.129711
2019-12-27 16:09:24.472945: step 73380/136300 (epoch 54/100), loss = 0.191214 (0.241 sec/batch), lr: 0.129711
2019-12-27 16:09:29.203398: step 73400/136300 (epoch 54/100), loss = 0.301240 (0.235 sec/batch), lr: 0.129711
2019-12-27 16:09:33.948976: step 73420/136300 (epoch 54/100), loss = 0.259244 (0.245 sec/batch), lr: 0.129711
2019-12-27 16:09:40.170745: step 73440/136300 (epoch 54/100), loss = 0.106696 (0.232 sec/batch), lr: 0.129711
2019-12-27 16:09:45.061898: step 73460/136300 (epoch 54/100), loss = 0.306052 (0.241 sec/batch), lr: 0.129711
2019-12-27 16:09:49.966390: step 73480/136300 (epoch 54/100), loss = 0.200332 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:09:54.769637: step 73500/136300 (epoch 54/100), loss = 0.513599 (0.186 sec/batch), lr: 0.129711
2019-12-27 16:09:59.473936: step 73520/136300 (epoch 54/100), loss = 0.228221 (0.219 sec/batch), lr: 0.129711
2019-12-27 16:10:04.229816: step 73540/136300 (epoch 54/100), loss = 0.209358 (0.214 sec/batch), lr: 0.129711
2019-12-27 16:10:08.997600: step 73560/136300 (epoch 54/100), loss = 0.246039 (0.237 sec/batch), lr: 0.129711
2019-12-27 16:10:13.882254: step 73580/136300 (epoch 54/100), loss = 0.206227 (0.240 sec/batch), lr: 0.129711
2019-12-27 16:10:18.791714: step 73600/136300 (epoch 54/100), loss = 0.261084 (0.217 sec/batch), lr: 0.129711
Evaluating on dev set...
Precision (micro): 71.155%
   Recall (micro): 62.215%
       F1 (micro): 66.385%
epoch 54: train_loss = 0.257806, dev_loss = 0.444475, dev_f1 = 0.6639
model saved to ./saved_models/01/checkpoint_epoch_54.pt

2019-12-27 16:10:58.121857: step 73620/136300 (epoch 55/100), loss = 0.146762 (1.466 sec/batch), lr: 0.129711
2019-12-27 16:11:02.848134: step 73640/136300 (epoch 55/100), loss = 0.343529 (0.221 sec/batch), lr: 0.129711
2019-12-27 16:11:07.686292: step 73660/136300 (epoch 55/100), loss = 0.296533 (0.234 sec/batch), lr: 0.129711
2019-12-27 16:11:12.536677: step 73680/136300 (epoch 55/100), loss = 0.327752 (0.242 sec/batch), lr: 0.129711
2019-12-27 16:11:17.345459: step 73700/136300 (epoch 55/100), loss = 0.260781 (0.221 sec/batch), lr: 0.129711
2019-12-27 16:11:22.109726: step 73720/136300 (epoch 55/100), loss = 0.356738 (0.237 sec/batch), lr: 0.129711
2019-12-27 16:11:26.877459: step 73740/136300 (epoch 55/100), loss = 0.275138 (0.229 sec/batch), lr: 0.129711
2019-12-27 16:11:31.653314: step 73760/136300 (epoch 55/100), loss = 0.280512 (0.217 sec/batch), lr: 0.129711
2019-12-27 16:11:36.526276: step 73780/136300 (epoch 55/100), loss = 0.174898 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:11:41.377455: step 73800/136300 (epoch 55/100), loss = 0.284833 (0.240 sec/batch), lr: 0.129711
2019-12-27 16:11:47.577110: step 73820/136300 (epoch 55/100), loss = 0.194864 (0.225 sec/batch), lr: 0.129711
2019-12-27 16:11:52.309317: step 73840/136300 (epoch 55/100), loss = 0.225274 (0.228 sec/batch), lr: 0.129711
2019-12-27 16:11:57.108501: step 73860/136300 (epoch 55/100), loss = 0.297633 (0.242 sec/batch), lr: 0.129711
2019-12-27 16:12:01.962069: step 73880/136300 (epoch 55/100), loss = 0.126476 (0.240 sec/batch), lr: 0.129711
2019-12-27 16:12:06.761939: step 73900/136300 (epoch 55/100), loss = 0.136216 (0.241 sec/batch), lr: 0.129711
2019-12-27 16:12:11.496520: step 73920/136300 (epoch 55/100), loss = 0.217197 (0.236 sec/batch), lr: 0.129711
2019-12-27 16:12:16.342095: step 73940/136300 (epoch 55/100), loss = 0.575065 (0.239 sec/batch), lr: 0.129711
2019-12-27 16:12:21.153770: step 73960/136300 (epoch 55/100), loss = 0.202848 (0.215 sec/batch), lr: 0.129711
2019-12-27 16:12:25.895822: step 73980/136300 (epoch 55/100), loss = 0.278059 (0.223 sec/batch), lr: 0.129711
2019-12-27 16:12:30.655062: step 74000/136300 (epoch 55/100), loss = 0.426442 (0.207 sec/batch), lr: 0.129711
2019-12-27 16:12:36.652943: step 74020/136300 (epoch 55/100), loss = 0.112058 (0.212 sec/batch), lr: 0.129711
2019-12-27 16:12:41.474204: step 74040/136300 (epoch 55/100), loss = 0.274724 (0.232 sec/batch), lr: 0.129711
2019-12-27 16:12:46.366233: step 74060/136300 (epoch 55/100), loss = 0.201763 (0.243 sec/batch), lr: 0.129711
2019-12-27 16:12:51.190209: step 74080/136300 (epoch 55/100), loss = 0.093879 (0.235 sec/batch), lr: 0.129711
2019-12-27 16:12:56.002308: step 74100/136300 (epoch 55/100), loss = 0.247504 (0.236 sec/batch), lr: 0.129711
2019-12-27 16:13:00.889026: step 74120/136300 (epoch 55/100), loss = 0.405991 (0.216 sec/batch), lr: 0.129711
2019-12-27 16:13:05.809661: step 74140/136300 (epoch 55/100), loss = 0.148428 (0.241 sec/batch), lr: 0.129711
2019-12-27 16:13:10.568319: step 74160/136300 (epoch 55/100), loss = 0.310670 (0.236 sec/batch), lr: 0.129711
2019-12-27 16:13:15.367425: step 74180/136300 (epoch 55/100), loss = 0.129526 (0.241 sec/batch), lr: 0.129711
2019-12-27 16:13:20.061461: step 74200/136300 (epoch 55/100), loss = 0.221326 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:13:26.263688: step 74220/136300 (epoch 55/100), loss = 0.210443 (0.214 sec/batch), lr: 0.129711
2019-12-27 16:13:31.072470: step 74240/136300 (epoch 55/100), loss = 0.198168 (0.231 sec/batch), lr: 0.129711
2019-12-27 16:13:35.771795: step 74260/136300 (epoch 55/100), loss = 0.103974 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:13:40.563364: step 74280/136300 (epoch 55/100), loss = 0.213949 (0.244 sec/batch), lr: 0.129711
2019-12-27 16:13:45.448797: step 74300/136300 (epoch 55/100), loss = 0.329770 (0.201 sec/batch), lr: 0.129711
2019-12-27 16:13:50.305261: step 74320/136300 (epoch 55/100), loss = 0.155157 (0.226 sec/batch), lr: 0.129711
2019-12-27 16:13:55.181012: step 74340/136300 (epoch 55/100), loss = 0.270999 (0.234 sec/batch), lr: 0.129711
2019-12-27 16:13:59.909678: step 74360/136300 (epoch 55/100), loss = 0.315913 (0.221 sec/batch), lr: 0.129711
2019-12-27 16:14:04.699724: step 74380/136300 (epoch 55/100), loss = 0.246784 (0.231 sec/batch), lr: 0.129711
2019-12-27 16:14:11.000427: step 74400/136300 (epoch 55/100), loss = 0.224553 (1.645 sec/batch), lr: 0.129711
2019-12-27 16:14:15.740734: step 74420/136300 (epoch 55/100), loss = 0.274156 (0.173 sec/batch), lr: 0.129711
2019-12-27 16:14:20.550143: step 74440/136300 (epoch 55/100), loss = 0.205567 (0.245 sec/batch), lr: 0.129711
2019-12-27 16:14:25.432028: step 74460/136300 (epoch 55/100), loss = 0.243342 (0.215 sec/batch), lr: 0.129711
2019-12-27 16:14:30.081672: step 74480/136300 (epoch 55/100), loss = 0.310246 (0.232 sec/batch), lr: 0.129711
2019-12-27 16:14:34.933940: step 74500/136300 (epoch 55/100), loss = 0.434971 (0.234 sec/batch), lr: 0.129711
2019-12-27 16:14:39.800079: step 74520/136300 (epoch 55/100), loss = 0.265447 (0.240 sec/batch), lr: 0.129711
2019-12-27 16:14:44.565546: step 74540/136300 (epoch 55/100), loss = 0.151741 (0.237 sec/batch), lr: 0.129711
2019-12-27 16:14:49.497227: step 74560/136300 (epoch 55/100), loss = 0.191774 (0.245 sec/batch), lr: 0.129711
2019-12-27 16:14:54.324234: step 74580/136300 (epoch 55/100), loss = 0.291046 (0.233 sec/batch), lr: 0.129711
2019-12-27 16:14:59.087923: step 74600/136300 (epoch 55/100), loss = 0.297034 (0.216 sec/batch), lr: 0.129711
2019-12-27 16:15:05.302839: step 74620/136300 (epoch 55/100), loss = 0.161707 (0.244 sec/batch), lr: 0.129711
2019-12-27 16:15:10.053895: step 74640/136300 (epoch 55/100), loss = 0.206233 (0.201 sec/batch), lr: 0.129711
2019-12-27 16:15:14.701849: step 74660/136300 (epoch 55/100), loss = 0.414541 (0.230 sec/batch), lr: 0.129711
2019-12-27 16:15:19.431391: step 74680/136300 (epoch 55/100), loss = 0.289824 (0.213 sec/batch), lr: 0.129711
2019-12-27 16:15:24.159722: step 74700/136300 (epoch 55/100), loss = 0.215918 (0.182 sec/batch), lr: 0.129711
2019-12-27 16:15:29.082258: step 74720/136300 (epoch 55/100), loss = 0.273620 (0.243 sec/batch), lr: 0.129711
2019-12-27 16:15:33.915001: step 74740/136300 (epoch 55/100), loss = 0.332351 (0.225 sec/batch), lr: 0.129711
2019-12-27 16:15:38.673566: step 74760/136300 (epoch 55/100), loss = 0.267704 (0.219 sec/batch), lr: 0.129711
2019-12-27 16:15:43.359421: step 74780/136300 (epoch 55/100), loss = 0.125196 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:15:48.168333: step 74800/136300 (epoch 55/100), loss = 0.307643 (0.226 sec/batch), lr: 0.129711
2019-12-27 16:15:54.316877: step 74820/136300 (epoch 55/100), loss = 0.200494 (0.246 sec/batch), lr: 0.129711
2019-12-27 16:15:59.186141: step 74840/136300 (epoch 55/100), loss = 0.248822 (0.236 sec/batch), lr: 0.129711
2019-12-27 16:16:04.018000: step 74860/136300 (epoch 55/100), loss = 0.518294 (0.236 sec/batch), lr: 0.129711
2019-12-27 16:16:08.688876: step 74880/136300 (epoch 55/100), loss = 0.224991 (0.227 sec/batch), lr: 0.129711
2019-12-27 16:16:13.491343: step 74900/136300 (epoch 55/100), loss = 0.249552 (0.212 sec/batch), lr: 0.129711
2019-12-27 16:16:18.258101: step 74920/136300 (epoch 55/100), loss = 0.318459 (0.233 sec/batch), lr: 0.129711
2019-12-27 16:16:23.186367: step 74940/136300 (epoch 55/100), loss = 0.146344 (0.244 sec/batch), lr: 0.129711
2019-12-27 16:16:28.087071: step 74960/136300 (epoch 55/100), loss = 0.258216 (0.240 sec/batch), lr: 0.129711
Evaluating on dev set...
Precision (micro): 71.354%
   Recall (micro): 62.546%
       F1 (micro): 66.660%
epoch 55: train_loss = 0.256623, dev_loss = 0.433277, dev_f1 = 0.6666
model saved to ./saved_models/01/checkpoint_epoch_55.pt

2019-12-27 16:17:06.179784: step 74980/136300 (epoch 56/100), loss = 0.205816 (0.231 sec/batch), lr: 0.129711
2019-12-27 16:17:12.242140: step 75000/136300 (epoch 56/100), loss = 0.185103 (0.200 sec/batch), lr: 0.129711
2019-12-27 16:17:17.053758: step 75020/136300 (epoch 56/100), loss = 0.155552 (0.236 sec/batch), lr: 0.129711
2019-12-27 16:17:21.799294: step 75040/136300 (epoch 56/100), loss = 0.350121 (0.249 sec/batch), lr: 0.129711
2019-12-27 16:17:26.659329: step 75060/136300 (epoch 56/100), loss = 0.210677 (0.214 sec/batch), lr: 0.129711
2019-12-27 16:17:31.358626: step 75080/136300 (epoch 56/100), loss = 0.228223 (0.204 sec/batch), lr: 0.129711
2019-12-27 16:17:36.116113: step 75100/136300 (epoch 56/100), loss = 0.294284 (0.212 sec/batch), lr: 0.129711
2019-12-27 16:17:40.898257: step 75120/136300 (epoch 56/100), loss = 0.261755 (0.243 sec/batch), lr: 0.129711
2019-12-27 16:17:45.716445: step 75140/136300 (epoch 56/100), loss = 0.211206 (0.227 sec/batch), lr: 0.129711
2019-12-27 16:17:50.539486: step 75160/136300 (epoch 56/100), loss = 0.277878 (0.237 sec/batch), lr: 0.129711
2019-12-27 16:17:56.488820: step 75180/136300 (epoch 56/100), loss = 0.223020 (0.181 sec/batch), lr: 0.129711
2019-12-27 16:18:01.189713: step 75200/136300 (epoch 56/100), loss = 0.291313 (0.226 sec/batch), lr: 0.129711
2019-12-27 16:18:05.975003: step 75220/136300 (epoch 56/100), loss = 0.264740 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:18:10.809966: step 75240/136300 (epoch 56/100), loss = 0.199686 (0.214 sec/batch), lr: 0.129711
2019-12-27 16:18:15.600007: step 75260/136300 (epoch 56/100), loss = 0.359828 (0.200 sec/batch), lr: 0.129711
2019-12-27 16:18:20.280188: step 75280/136300 (epoch 56/100), loss = 0.289670 (0.202 sec/batch), lr: 0.129711
2019-12-27 16:18:25.097247: step 75300/136300 (epoch 56/100), loss = 0.188653 (0.240 sec/batch), lr: 0.129711
2019-12-27 16:18:29.885331: step 75320/136300 (epoch 56/100), loss = 0.166777 (0.245 sec/batch), lr: 0.129711
2019-12-27 16:18:34.581803: step 75340/136300 (epoch 56/100), loss = 0.135058 (0.207 sec/batch), lr: 0.129711
2019-12-27 16:18:39.323877: step 75360/136300 (epoch 56/100), loss = 0.181072 (0.212 sec/batch), lr: 0.129711
2019-12-27 16:18:45.434879: step 75380/136300 (epoch 56/100), loss = 0.153379 (0.221 sec/batch), lr: 0.129711
2019-12-27 16:18:50.211827: step 75400/136300 (epoch 56/100), loss = 0.238157 (0.216 sec/batch), lr: 0.129711
2019-12-27 16:18:55.092015: step 75420/136300 (epoch 56/100), loss = 0.105857 (0.234 sec/batch), lr: 0.129711
2019-12-27 16:18:59.854262: step 75440/136300 (epoch 56/100), loss = 0.305946 (0.222 sec/batch), lr: 0.129711
2019-12-27 16:19:04.661787: step 75460/136300 (epoch 56/100), loss = 0.318671 (0.230 sec/batch), lr: 0.129711
2019-12-27 16:19:09.511624: step 75480/136300 (epoch 56/100), loss = 0.225454 (0.232 sec/batch), lr: 0.129711
2019-12-27 16:19:14.346256: step 75500/136300 (epoch 56/100), loss = 0.274906 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:19:19.113500: step 75520/136300 (epoch 56/100), loss = 0.167790 (0.242 sec/batch), lr: 0.129711
2019-12-27 16:19:23.919746: step 75540/136300 (epoch 56/100), loss = 0.109263 (0.227 sec/batch), lr: 0.129711
2019-12-27 16:19:28.618182: step 75560/136300 (epoch 56/100), loss = 0.523584 (0.174 sec/batch), lr: 0.129711
2019-12-27 16:19:34.436807: step 75580/136300 (epoch 56/100), loss = 0.228471 (0.225 sec/batch), lr: 0.129711
2019-12-27 16:19:39.213762: step 75600/136300 (epoch 56/100), loss = 0.165719 (0.241 sec/batch), lr: 0.129711
2019-12-27 16:19:43.921007: step 75620/136300 (epoch 56/100), loss = 0.195746 (0.231 sec/batch), lr: 0.129711
2019-12-27 16:19:48.689128: step 75640/136300 (epoch 56/100), loss = 0.298505 (0.223 sec/batch), lr: 0.129711
2019-12-27 16:19:53.563389: step 75660/136300 (epoch 56/100), loss = 0.128263 (0.250 sec/batch), lr: 0.129711
2019-12-27 16:19:58.391391: step 75680/136300 (epoch 56/100), loss = 0.145760 (0.239 sec/batch), lr: 0.129711
2019-12-27 16:20:03.289528: step 75700/136300 (epoch 56/100), loss = 0.105925 (0.239 sec/batch), lr: 0.129711
2019-12-27 16:20:08.069567: step 75720/136300 (epoch 56/100), loss = 0.134628 (0.204 sec/batch), lr: 0.129711
2019-12-27 16:20:12.822347: step 75740/136300 (epoch 56/100), loss = 0.426417 (0.246 sec/batch), lr: 0.129711
2019-12-27 16:20:17.696887: step 75760/136300 (epoch 56/100), loss = 0.189757 (0.233 sec/batch), lr: 0.129711
2019-12-27 16:20:23.970239: step 75780/136300 (epoch 56/100), loss = 0.199113 (0.229 sec/batch), lr: 0.129711
2019-12-27 16:20:28.724280: step 75800/136300 (epoch 56/100), loss = 0.283553 (0.236 sec/batch), lr: 0.129711
2019-12-27 16:20:33.632892: step 75820/136300 (epoch 56/100), loss = 0.292367 (0.234 sec/batch), lr: 0.129711
2019-12-27 16:20:38.325468: step 75840/136300 (epoch 56/100), loss = 0.322344 (0.249 sec/batch), lr: 0.129711
2019-12-27 16:20:43.189221: step 75860/136300 (epoch 56/100), loss = 0.279103 (0.241 sec/batch), lr: 0.129711
2019-12-27 16:20:48.108060: step 75880/136300 (epoch 56/100), loss = 0.223582 (0.246 sec/batch), lr: 0.129711
2019-12-27 16:20:52.824099: step 75900/136300 (epoch 56/100), loss = 0.191233 (0.216 sec/batch), lr: 0.129711
2019-12-27 16:20:57.746882: step 75920/136300 (epoch 56/100), loss = 0.392297 (0.229 sec/batch), lr: 0.129711
2019-12-27 16:21:02.580844: step 75940/136300 (epoch 56/100), loss = 0.192588 (0.234 sec/batch), lr: 0.129711
2019-12-27 16:21:07.357940: step 75960/136300 (epoch 56/100), loss = 0.219225 (0.205 sec/batch), lr: 0.129711
2019-12-27 16:21:13.290199: step 75980/136300 (epoch 56/100), loss = 0.283484 (0.234 sec/batch), lr: 0.129711
2019-12-27 16:21:18.121757: step 76000/136300 (epoch 56/100), loss = 0.299287 (0.240 sec/batch), lr: 0.129711
2019-12-27 16:21:22.658745: step 76020/136300 (epoch 56/100), loss = 0.201125 (0.227 sec/batch), lr: 0.129711
2019-12-27 16:21:27.372180: step 76040/136300 (epoch 56/100), loss = 0.280742 (0.221 sec/batch), lr: 0.129711
2019-12-27 16:21:32.175945: step 76060/136300 (epoch 56/100), loss = 0.302292 (0.235 sec/batch), lr: 0.129711
2019-12-27 16:21:36.966430: step 76080/136300 (epoch 56/100), loss = 0.221790 (0.216 sec/batch), lr: 0.129711
2019-12-27 16:21:41.838751: step 76100/136300 (epoch 56/100), loss = 0.172065 (0.224 sec/batch), lr: 0.129711
2019-12-27 16:21:46.570978: step 76120/136300 (epoch 56/100), loss = 0.347075 (0.226 sec/batch), lr: 0.129711
2019-12-27 16:21:51.229532: step 76140/136300 (epoch 56/100), loss = 0.349395 (0.225 sec/batch), lr: 0.129711
2019-12-27 16:21:56.062862: step 76160/136300 (epoch 56/100), loss = 0.218003 (0.247 sec/batch), lr: 0.129711
2019-12-27 16:22:02.249629: step 76180/136300 (epoch 56/100), loss = 0.189024 (0.227 sec/batch), lr: 0.129711
2019-12-27 16:22:07.138284: step 76200/136300 (epoch 56/100), loss = 0.273147 (0.227 sec/batch), lr: 0.129711
2019-12-27 16:22:11.988347: step 76220/136300 (epoch 56/100), loss = 0.373569 (0.214 sec/batch), lr: 0.129711
2019-12-27 16:22:16.582215: step 76240/136300 (epoch 56/100), loss = 0.234648 (0.176 sec/batch), lr: 0.129711
2019-12-27 16:22:21.387642: step 76260/136300 (epoch 56/100), loss = 0.331442 (0.235 sec/batch), lr: 0.129711
2019-12-27 16:22:26.149657: step 76280/136300 (epoch 56/100), loss = 0.311266 (0.243 sec/batch), lr: 0.129711
2019-12-27 16:22:31.013527: step 76300/136300 (epoch 56/100), loss = 0.502909 (0.239 sec/batch), lr: 0.129711
2019-12-27 16:22:35.762598: step 76320/136300 (epoch 56/100), loss = 0.240734 (0.235 sec/batch), lr: 0.129711
Evaluating on dev set...
Precision (micro): 70.583%
   Recall (micro): 63.650%
       F1 (micro): 66.938%
epoch 56: train_loss = 0.254378, dev_loss = 0.439761, dev_f1 = 0.6694
model saved to ./saved_models/01/checkpoint_epoch_56.pt
new best model saved.

2019-12-27 16:23:13.977795: step 76340/136300 (epoch 57/100), loss = 0.177279 (0.231 sec/batch), lr: 0.129711
2019-12-27 16:23:20.208774: step 76360/136300 (epoch 57/100), loss = 0.198954 (0.217 sec/batch), lr: 0.129711
2019-12-27 16:23:25.013442: step 76380/136300 (epoch 57/100), loss = 0.221556 (0.217 sec/batch), lr: 0.129711
2019-12-27 16:23:29.704289: step 76400/136300 (epoch 57/100), loss = 0.202062 (0.245 sec/batch), lr: 0.129711
2019-12-27 16:23:34.577607: step 76420/136300 (epoch 57/100), loss = 0.316217 (0.202 sec/batch), lr: 0.129711
2019-12-27 16:23:39.263649: step 76440/136300 (epoch 57/100), loss = 0.173059 (0.219 sec/batch), lr: 0.129711
2019-12-27 16:23:44.020522: step 76460/136300 (epoch 57/100), loss = 0.254321 (0.229 sec/batch), lr: 0.129711
2019-12-27 16:23:48.809947: step 76480/136300 (epoch 57/100), loss = 0.203557 (0.206 sec/batch), lr: 0.129711
2019-12-27 16:23:53.602984: step 76500/136300 (epoch 57/100), loss = 0.410228 (0.205 sec/batch), lr: 0.129711
2019-12-27 16:23:58.484305: step 76520/136300 (epoch 57/100), loss = 0.213691 (0.220 sec/batch), lr: 0.129711
2019-12-27 16:24:03.261207: step 76540/136300 (epoch 57/100), loss = 0.122456 (0.231 sec/batch), lr: 0.129711
2019-12-27 16:24:09.308463: step 76560/136300 (epoch 57/100), loss = 0.367936 (0.239 sec/batch), lr: 0.129711
2019-12-27 16:24:14.024720: step 76580/136300 (epoch 57/100), loss = 0.496182 (0.204 sec/batch), lr: 0.129711
2019-12-27 16:24:18.949497: step 76600/136300 (epoch 57/100), loss = 0.300251 (0.237 sec/batch), lr: 0.129711
2019-12-27 16:24:23.771520: step 76620/136300 (epoch 57/100), loss = 0.234025 (0.217 sec/batch), lr: 0.129711
2019-12-27 16:24:28.502487: step 76640/136300 (epoch 57/100), loss = 0.322519 (0.234 sec/batch), lr: 0.129711
2019-12-27 16:24:33.225994: step 76660/136300 (epoch 57/100), loss = 0.207981 (0.243 sec/batch), lr: 0.129711
2019-12-27 16:24:38.034170: step 76680/136300 (epoch 57/100), loss = 0.174225 (0.232 sec/batch), lr: 0.129711
2019-12-27 16:24:42.819663: step 76700/136300 (epoch 57/100), loss = 0.256740 (0.196 sec/batch), lr: 0.129711
2019-12-27 16:24:47.517498: step 76720/136300 (epoch 57/100), loss = 0.131402 (0.222 sec/batch), lr: 0.129711
2019-12-27 16:24:53.771946: step 76740/136300 (epoch 57/100), loss = 0.128482 (0.224 sec/batch), lr: 0.129711
2019-12-27 16:24:58.563515: step 76760/136300 (epoch 57/100), loss = 0.313162 (0.245 sec/batch), lr: 0.129711
2019-12-27 16:25:03.441130: step 76780/136300 (epoch 57/100), loss = 0.174110 (0.226 sec/batch), lr: 0.129711
2019-12-27 16:25:08.251452: step 76800/136300 (epoch 57/100), loss = 0.374389 (0.227 sec/batch), lr: 0.129711
2019-12-27 16:25:13.029960: step 76820/136300 (epoch 57/100), loss = 0.190231 (0.224 sec/batch), lr: 0.129711
2019-12-27 16:25:17.869111: step 76840/136300 (epoch 57/100), loss = 0.211724 (0.239 sec/batch), lr: 0.129711
2019-12-27 16:25:22.712932: step 76860/136300 (epoch 57/100), loss = 0.243840 (0.236 sec/batch), lr: 0.129711
2019-12-27 16:25:27.535241: step 76880/136300 (epoch 57/100), loss = 0.200397 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:25:32.307294: step 76900/136300 (epoch 57/100), loss = 0.184985 (0.232 sec/batch), lr: 0.129711
2019-12-27 16:25:37.073774: step 76920/136300 (epoch 57/100), loss = 0.295040 (0.178 sec/batch), lr: 0.129711
2019-12-27 16:25:42.930537: step 76940/136300 (epoch 57/100), loss = 0.207593 (0.220 sec/batch), lr: 0.129711
2019-12-27 16:25:47.698327: step 76960/136300 (epoch 57/100), loss = 0.285910 (0.235 sec/batch), lr: 0.129711
2019-12-27 16:25:52.413740: step 76980/136300 (epoch 57/100), loss = 0.154376 (0.221 sec/batch), lr: 0.129711
2019-12-27 16:25:57.213914: step 77000/136300 (epoch 57/100), loss = 0.080639 (0.230 sec/batch), lr: 0.129711
2019-12-27 16:26:02.105929: step 77020/136300 (epoch 57/100), loss = 0.277041 (0.235 sec/batch), lr: 0.129711
2019-12-27 16:26:06.953233: step 77040/136300 (epoch 57/100), loss = 0.321563 (0.219 sec/batch), lr: 0.129711
2019-12-27 16:26:11.851209: step 77060/136300 (epoch 57/100), loss = 0.326551 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:26:16.680219: step 77080/136300 (epoch 57/100), loss = 0.193627 (0.233 sec/batch), lr: 0.129711
2019-12-27 16:26:21.416922: step 77100/136300 (epoch 57/100), loss = 0.314779 (0.228 sec/batch), lr: 0.129711
2019-12-27 16:26:26.290824: step 77120/136300 (epoch 57/100), loss = 0.282778 (0.232 sec/batch), lr: 0.129711
2019-12-27 16:26:32.482998: step 77140/136300 (epoch 57/100), loss = 0.285711 (0.217 sec/batch), lr: 0.129711
2019-12-27 16:26:37.231030: step 77160/136300 (epoch 57/100), loss = 0.369275 (0.243 sec/batch), lr: 0.129711
2019-12-27 16:26:42.124464: step 77180/136300 (epoch 57/100), loss = 0.233588 (0.177 sec/batch), lr: 0.129711
2019-12-27 16:26:46.754524: step 77200/136300 (epoch 57/100), loss = 0.190991 (0.229 sec/batch), lr: 0.129711
2019-12-27 16:26:51.659283: step 77220/136300 (epoch 57/100), loss = 0.276842 (0.235 sec/batch), lr: 0.129711
2019-12-27 16:26:56.575489: step 77240/136300 (epoch 57/100), loss = 0.158461 (0.218 sec/batch), lr: 0.129711
2019-12-27 16:27:01.370309: step 77260/136300 (epoch 57/100), loss = 0.277799 (0.238 sec/batch), lr: 0.129711
2019-12-27 16:27:06.304385: step 77280/136300 (epoch 57/100), loss = 0.173378 (0.240 sec/batch), lr: 0.129711
2019-12-27 16:27:11.181550: step 77300/136300 (epoch 57/100), loss = 0.357496 (0.225 sec/batch), lr: 0.129711
2019-12-27 16:27:15.985896: step 77320/136300 (epoch 57/100), loss = 0.140902 (0.225 sec/batch), lr: 0.129711
2019-12-27 16:27:22.095867: step 77340/136300 (epoch 57/100), loss = 0.178746 (0.215 sec/batch), lr: 0.129711
2019-12-27 16:27:26.949630: step 77360/136300 (epoch 57/100), loss = 0.140539 (0.199 sec/batch), lr: 0.129711
2019-12-27 16:27:31.557025: step 77380/136300 (epoch 57/100), loss = 0.227036 (0.202 sec/batch), lr: 0.129711
2019-12-27 16:27:36.269807: step 77400/136300 (epoch 57/100), loss = 0.269976 (0.228 sec/batch), lr: 0.129711
2019-12-27 16:27:41.058347: step 77420/136300 (epoch 57/100), loss = 0.209945 (0.237 sec/batch), lr: 0.129711
2019-12-27 16:27:45.856156: step 77440/136300 (epoch 57/100), loss = 0.344692 (0.203 sec/batch), lr: 0.129711
2019-12-27 16:27:50.770754: step 77460/136300 (epoch 57/100), loss = 0.204803 (0.236 sec/batch), lr: 0.129711
2019-12-27 16:27:55.517226: step 77480/136300 (epoch 57/100), loss = 0.172578 (0.237 sec/batch), lr: 0.129711
2019-12-27 16:28:00.178585: step 77500/136300 (epoch 57/100), loss = 0.176109 (0.177 sec/batch), lr: 0.129711
2019-12-27 16:28:04.994048: step 77520/136300 (epoch 57/100), loss = 0.151949 (0.237 sec/batch), lr: 0.129711
2019-12-27 16:28:11.254794: step 77540/136300 (epoch 57/100), loss = 0.254190 (0.245 sec/batch), lr: 0.129711
2019-12-27 16:28:16.162822: step 77560/136300 (epoch 57/100), loss = 0.247589 (0.233 sec/batch), lr: 0.129711
2019-12-27 16:28:21.088281: step 77580/136300 (epoch 57/100), loss = 0.148053 (0.225 sec/batch), lr: 0.129711
2019-12-27 16:28:25.656135: step 77600/136300 (epoch 57/100), loss = 0.295429 (0.220 sec/batch), lr: 0.129711
2019-12-27 16:28:30.462164: step 77620/136300 (epoch 57/100), loss = 0.205994 (0.231 sec/batch), lr: 0.129711
2019-12-27 16:28:35.195631: step 77640/136300 (epoch 57/100), loss = 0.217543 (0.233 sec/batch), lr: 0.129711
2019-12-27 16:28:40.077939: step 77660/136300 (epoch 57/100), loss = 0.286703 (0.202 sec/batch), lr: 0.129711
2019-12-27 16:28:44.898036: step 77680/136300 (epoch 57/100), loss = 0.167865 (0.234 sec/batch), lr: 0.129711
Evaluating on dev set...
Precision (micro): 70.871%
   Recall (micro): 62.436%
       F1 (micro): 66.386%
epoch 57: train_loss = 0.254699, dev_loss = 0.447652, dev_f1 = 0.6639
model saved to ./saved_models/01/checkpoint_epoch_57.pt

2019-12-27 16:29:23.245992: step 77700/136300 (epoch 58/100), loss = 0.218877 (0.217 sec/batch), lr: 0.116740
2019-12-27 16:29:29.083165: step 77720/136300 (epoch 58/100), loss = 0.151002 (0.238 sec/batch), lr: 0.116740
2019-12-27 16:29:33.879551: step 77740/136300 (epoch 58/100), loss = 0.402683 (0.214 sec/batch), lr: 0.116740
2019-12-27 16:29:38.549838: step 77760/136300 (epoch 58/100), loss = 0.188182 (0.232 sec/batch), lr: 0.116740
2019-12-27 16:29:43.457762: step 77780/136300 (epoch 58/100), loss = 0.244558 (0.242 sec/batch), lr: 0.116740
2019-12-27 16:29:48.247508: step 77800/136300 (epoch 58/100), loss = 0.445747 (0.243 sec/batch), lr: 0.116740
2019-12-27 16:29:52.915769: step 77820/136300 (epoch 58/100), loss = 0.438173 (0.218 sec/batch), lr: 0.116740
2019-12-27 16:29:57.769892: step 77840/136300 (epoch 58/100), loss = 0.269228 (0.234 sec/batch), lr: 0.116740
2019-12-27 16:30:02.516713: step 77860/136300 (epoch 58/100), loss = 0.289044 (0.232 sec/batch), lr: 0.116740
2019-12-27 16:30:07.385375: step 77880/136300 (epoch 58/100), loss = 0.164681 (0.245 sec/batch), lr: 0.116740
2019-12-27 16:30:12.123044: step 77900/136300 (epoch 58/100), loss = 0.180974 (0.175 sec/batch), lr: 0.116740
2019-12-27 16:30:18.010027: step 77920/136300 (epoch 58/100), loss = 0.122349 (0.212 sec/batch), lr: 0.116740
2019-12-27 16:30:22.819467: step 77940/136300 (epoch 58/100), loss = 0.162823 (0.200 sec/batch), lr: 0.116740
2019-12-27 16:30:27.643642: step 77960/136300 (epoch 58/100), loss = 0.185775 (0.236 sec/batch), lr: 0.116740
2019-12-27 16:30:32.402072: step 77980/136300 (epoch 58/100), loss = 0.291321 (0.234 sec/batch), lr: 0.116740
2019-12-27 16:30:37.114861: step 78000/136300 (epoch 58/100), loss = 0.323176 (0.169 sec/batch), lr: 0.116740
2019-12-27 16:30:41.898087: step 78020/136300 (epoch 58/100), loss = 0.210344 (0.234 sec/batch), lr: 0.116740
2019-12-27 16:30:46.678094: step 78040/136300 (epoch 58/100), loss = 0.279706 (0.247 sec/batch), lr: 0.116740
2019-12-27 16:30:51.545594: step 78060/136300 (epoch 58/100), loss = 0.274077 (0.240 sec/batch), lr: 0.116740
2019-12-27 16:30:56.207407: step 78080/136300 (epoch 58/100), loss = 0.351295 (0.217 sec/batch), lr: 0.116740
2019-12-27 16:31:02.212482: step 78100/136300 (epoch 58/100), loss = 0.232075 (1.562 sec/batch), lr: 0.116740
2019-12-27 16:31:06.933756: step 78120/136300 (epoch 58/100), loss = 0.158828 (0.232 sec/batch), lr: 0.116740
2019-12-27 16:31:11.842007: step 78140/136300 (epoch 58/100), loss = 0.240619 (0.227 sec/batch), lr: 0.116740
2019-12-27 16:31:16.668782: step 78160/136300 (epoch 58/100), loss = 0.372580 (0.236 sec/batch), lr: 0.116740
2019-12-27 16:31:21.411508: step 78180/136300 (epoch 58/100), loss = 0.355589 (0.199 sec/batch), lr: 0.116740
2019-12-27 16:31:26.244496: step 78200/136300 (epoch 58/100), loss = 0.410487 (0.211 sec/batch), lr: 0.116740
2019-12-27 16:31:31.076462: step 78220/136300 (epoch 58/100), loss = 0.272202 (0.238 sec/batch), lr: 0.116740
2019-12-27 16:31:35.908393: step 78240/136300 (epoch 58/100), loss = 0.378072 (0.181 sec/batch), lr: 0.116740
2019-12-27 16:31:40.722759: step 78260/136300 (epoch 58/100), loss = 0.168331 (0.240 sec/batch), lr: 0.116740
2019-12-27 16:31:45.486425: step 78280/136300 (epoch 58/100), loss = 0.254456 (0.231 sec/batch), lr: 0.116740
2019-12-27 16:31:51.313817: step 78300/136300 (epoch 58/100), loss = 0.279069 (0.173 sec/batch), lr: 0.116740
2019-12-27 16:31:56.096615: step 78320/136300 (epoch 58/100), loss = 0.225065 (0.215 sec/batch), lr: 0.116740
2019-12-27 16:32:00.823368: step 78340/136300 (epoch 58/100), loss = 0.204533 (0.220 sec/batch), lr: 0.116740
2019-12-27 16:32:05.627758: step 78360/136300 (epoch 58/100), loss = 0.154400 (0.236 sec/batch), lr: 0.116740
2019-12-27 16:32:10.495263: step 78380/136300 (epoch 58/100), loss = 0.217064 (0.212 sec/batch), lr: 0.116740
2019-12-27 16:32:15.365856: step 78400/136300 (epoch 58/100), loss = 0.322336 (0.220 sec/batch), lr: 0.116740
2019-12-27 16:32:20.241605: step 78420/136300 (epoch 58/100), loss = 0.229152 (0.219 sec/batch), lr: 0.116740
2019-12-27 16:32:25.087487: step 78440/136300 (epoch 58/100), loss = 0.153995 (0.233 sec/batch), lr: 0.116740
2019-12-27 16:32:29.750010: step 78460/136300 (epoch 58/100), loss = 0.124019 (0.230 sec/batch), lr: 0.116740
2019-12-27 16:32:34.617103: step 78480/136300 (epoch 58/100), loss = 0.310512 (0.204 sec/batch), lr: 0.116740
2019-12-27 16:32:40.781671: step 78500/136300 (epoch 58/100), loss = 0.255261 (0.181 sec/batch), lr: 0.116740
2019-12-27 16:32:45.561913: step 78520/136300 (epoch 58/100), loss = 0.255434 (0.247 sec/batch), lr: 0.116740
2019-12-27 16:32:50.478968: step 78540/136300 (epoch 58/100), loss = 0.334233 (0.236 sec/batch), lr: 0.116740
2019-12-27 16:32:55.118012: step 78560/136300 (epoch 58/100), loss = 0.226858 (0.179 sec/batch), lr: 0.116740
2019-12-27 16:32:59.930046: step 78580/136300 (epoch 58/100), loss = 0.596249 (0.229 sec/batch), lr: 0.116740
2019-12-27 16:33:04.817885: step 78600/136300 (epoch 58/100), loss = 0.380499 (0.241 sec/batch), lr: 0.116740
2019-12-27 16:33:09.582088: step 78620/136300 (epoch 58/100), loss = 0.248081 (0.223 sec/batch), lr: 0.116740
2019-12-27 16:33:14.451609: step 78640/136300 (epoch 58/100), loss = 0.224865 (0.246 sec/batch), lr: 0.116740
2019-12-27 16:33:19.370897: step 78660/136300 (epoch 58/100), loss = 0.185961 (0.230 sec/batch), lr: 0.116740
2019-12-27 16:33:24.128257: step 78680/136300 (epoch 58/100), loss = 0.236413 (0.225 sec/batch), lr: 0.116740
2019-12-27 16:33:30.242314: step 78700/136300 (epoch 58/100), loss = 0.270036 (0.239 sec/batch), lr: 0.116740
2019-12-27 16:33:35.128605: step 78720/136300 (epoch 58/100), loss = 0.438272 (0.235 sec/batch), lr: 0.116740
2019-12-27 16:33:39.710326: step 78740/136300 (epoch 58/100), loss = 0.265933 (0.232 sec/batch), lr: 0.116740
2019-12-27 16:33:44.403700: step 78760/136300 (epoch 58/100), loss = 0.121750 (0.238 sec/batch), lr: 0.116740
2019-12-27 16:33:49.227064: step 78780/136300 (epoch 58/100), loss = 0.212752 (0.212 sec/batch), lr: 0.116740
2019-12-27 16:33:53.987256: step 78800/136300 (epoch 58/100), loss = 0.366383 (0.230 sec/batch), lr: 0.116740
2019-12-27 16:33:58.862633: step 78820/136300 (epoch 58/100), loss = 0.198093 (0.231 sec/batch), lr: 0.116740
2019-12-27 16:34:03.594255: step 78840/136300 (epoch 58/100), loss = 0.469748 (0.237 sec/batch), lr: 0.116740
2019-12-27 16:34:08.249955: step 78860/136300 (epoch 58/100), loss = 0.101870 (0.200 sec/batch), lr: 0.116740
2019-12-27 16:34:12.979278: step 78880/136300 (epoch 58/100), loss = 0.500712 (0.208 sec/batch), lr: 0.116740
2019-12-27 16:34:19.064232: step 78900/136300 (epoch 58/100), loss = 0.330134 (0.240 sec/batch), lr: 0.116740
2019-12-27 16:34:23.968061: step 78920/136300 (epoch 58/100), loss = 0.240247 (0.219 sec/batch), lr: 0.116740
2019-12-27 16:34:28.817695: step 78940/136300 (epoch 58/100), loss = 0.310858 (0.236 sec/batch), lr: 0.116740
2019-12-27 16:34:33.469878: step 78960/136300 (epoch 58/100), loss = 0.081228 (0.203 sec/batch), lr: 0.116740
2019-12-27 16:34:38.257330: step 78980/136300 (epoch 58/100), loss = 0.200064 (0.230 sec/batch), lr: 0.116740
2019-12-27 16:34:42.951209: step 79000/136300 (epoch 58/100), loss = 0.112590 (0.242 sec/batch), lr: 0.116740
2019-12-27 16:34:47.838769: step 79020/136300 (epoch 58/100), loss = 0.143867 (0.208 sec/batch), lr: 0.116740
2019-12-27 16:34:52.662853: step 79040/136300 (epoch 58/100), loss = 0.187757 (0.229 sec/batch), lr: 0.116740
Evaluating on dev set...
Precision (micro): 72.272%
   Recall (micro): 61.902%
       F1 (micro): 66.686%
epoch 58: train_loss = 0.249197, dev_loss = 0.440471, dev_f1 = 0.6669
model saved to ./saved_models/01/checkpoint_epoch_58.pt

2019-12-27 16:35:30.989349: step 79060/136300 (epoch 59/100), loss = 0.184798 (0.244 sec/batch), lr: 0.116740
2019-12-27 16:35:36.853344: step 79080/136300 (epoch 59/100), loss = 0.327270 (0.210 sec/batch), lr: 0.116740
2019-12-27 16:35:41.651486: step 79100/136300 (epoch 59/100), loss = 0.183124 (0.234 sec/batch), lr: 0.116740
2019-12-27 16:35:46.350280: step 79120/136300 (epoch 59/100), loss = 0.194807 (0.226 sec/batch), lr: 0.116740
2019-12-27 16:35:51.249022: step 79140/136300 (epoch 59/100), loss = 0.240503 (0.228 sec/batch), lr: 0.116740
2019-12-27 16:35:56.030397: step 79160/136300 (epoch 59/100), loss = 0.178536 (0.246 sec/batch), lr: 0.116740
2019-12-27 16:36:00.729482: step 79180/136300 (epoch 59/100), loss = 0.299872 (0.205 sec/batch), lr: 0.116740
2019-12-27 16:36:05.566464: step 79200/136300 (epoch 59/100), loss = 0.352703 (0.204 sec/batch), lr: 0.116740
2019-12-27 16:36:10.361483: step 79220/136300 (epoch 59/100), loss = 0.263273 (0.233 sec/batch), lr: 0.116740
2019-12-27 16:36:15.217906: step 79240/136300 (epoch 59/100), loss = 0.312139 (0.223 sec/batch), lr: 0.116740
2019-12-27 16:36:19.999891: step 79260/136300 (epoch 59/100), loss = 0.130374 (0.210 sec/batch), lr: 0.116740
2019-12-27 16:36:26.178464: step 79280/136300 (epoch 59/100), loss = 0.152192 (0.204 sec/batch), lr: 0.116740
2019-12-27 16:36:30.998413: step 79300/136300 (epoch 59/100), loss = 0.153872 (0.237 sec/batch), lr: 0.116740
2019-12-27 16:36:35.758031: step 79320/136300 (epoch 59/100), loss = 0.367703 (0.243 sec/batch), lr: 0.116740
2019-12-27 16:36:40.512545: step 79340/136300 (epoch 59/100), loss = 0.196128 (0.235 sec/batch), lr: 0.116740
2019-12-27 16:36:45.324084: step 79360/136300 (epoch 59/100), loss = 0.256341 (0.234 sec/batch), lr: 0.116740
2019-12-27 16:36:50.026677: step 79380/136300 (epoch 59/100), loss = 0.300386 (0.231 sec/batch), lr: 0.116740
2019-12-27 16:36:54.809643: step 79400/136300 (epoch 59/100), loss = 0.088725 (0.239 sec/batch), lr: 0.116740
2019-12-27 16:36:59.706128: step 79420/136300 (epoch 59/100), loss = 0.318025 (0.245 sec/batch), lr: 0.116740
2019-12-27 16:37:04.398771: step 79440/136300 (epoch 59/100), loss = 0.126901 (0.212 sec/batch), lr: 0.116740
2019-12-27 16:37:08.994771: step 79460/136300 (epoch 59/100), loss = 0.151546 (0.218 sec/batch), lr: 0.116740
2019-12-27 16:37:15.116414: step 79480/136300 (epoch 59/100), loss = 0.465080 (0.229 sec/batch), lr: 0.116740
2019-12-27 16:37:20.033688: step 79500/136300 (epoch 59/100), loss = 0.303062 (0.225 sec/batch), lr: 0.116740
2019-12-27 16:37:24.904599: step 79520/136300 (epoch 59/100), loss = 0.284110 (0.230 sec/batch), lr: 0.116740
2019-12-27 16:37:29.632912: step 79540/136300 (epoch 59/100), loss = 0.189337 (0.178 sec/batch), lr: 0.116740
2019-12-27 16:37:34.446425: step 79560/136300 (epoch 59/100), loss = 0.356632 (0.200 sec/batch), lr: 0.116740
2019-12-27 16:37:39.269270: step 79580/136300 (epoch 59/100), loss = 0.190992 (0.236 sec/batch), lr: 0.116740
2019-12-27 16:37:44.186691: step 79600/136300 (epoch 59/100), loss = 0.339908 (0.228 sec/batch), lr: 0.116740
2019-12-27 16:37:48.950185: step 79620/136300 (epoch 59/100), loss = 0.129899 (0.249 sec/batch), lr: 0.116740
2019-12-27 16:37:53.675174: step 79640/136300 (epoch 59/100), loss = 0.250636 (0.228 sec/batch), lr: 0.116740
2019-12-27 16:37:58.400512: step 79660/136300 (epoch 59/100), loss = 0.420053 (0.200 sec/batch), lr: 0.116740
2019-12-27 16:38:04.600063: step 79680/136300 (epoch 59/100), loss = 0.126519 (0.203 sec/batch), lr: 0.116740
2019-12-27 16:38:09.336123: step 79700/136300 (epoch 59/100), loss = 0.216490 (0.235 sec/batch), lr: 0.116740
2019-12-27 16:38:14.077966: step 79720/136300 (epoch 59/100), loss = 0.330777 (0.228 sec/batch), lr: 0.116740
2019-12-27 16:38:18.935020: step 79740/136300 (epoch 59/100), loss = 0.198957 (0.233 sec/batch), lr: 0.116740
2019-12-27 16:38:23.826353: step 79760/136300 (epoch 59/100), loss = 0.196141 (0.212 sec/batch), lr: 0.116740
2019-12-27 16:38:28.679976: step 79780/136300 (epoch 59/100), loss = 0.230977 (0.227 sec/batch), lr: 0.116740
2019-12-27 16:38:33.580632: step 79800/136300 (epoch 59/100), loss = 0.437515 (0.235 sec/batch), lr: 0.116740
2019-12-27 16:38:38.292025: step 79820/136300 (epoch 59/100), loss = 0.319677 (0.232 sec/batch), lr: 0.116740
2019-12-27 16:38:43.060532: step 79840/136300 (epoch 59/100), loss = 0.246976 (0.250 sec/batch), lr: 0.116740
2019-12-27 16:38:49.376996: step 79860/136300 (epoch 59/100), loss = 0.216310 (0.242 sec/batch), lr: 0.116740
2019-12-27 16:38:54.072928: step 79880/136300 (epoch 59/100), loss = 0.520122 (0.205 sec/batch), lr: 0.116740
2019-12-27 16:38:59.008427: step 79900/136300 (epoch 59/100), loss = 0.265497 (0.242 sec/batch), lr: 0.116740
2019-12-27 16:39:03.685650: step 79920/136300 (epoch 59/100), loss = 0.243419 (0.210 sec/batch), lr: 0.116740
2019-12-27 16:39:08.434064: step 79940/136300 (epoch 59/100), loss = 0.089796 (0.231 sec/batch), lr: 0.116740
2019-12-27 16:39:13.303560: step 79960/136300 (epoch 59/100), loss = 0.362714 (0.242 sec/batch), lr: 0.116740
2019-12-27 16:39:18.174054: step 79980/136300 (epoch 59/100), loss = 0.150752 (0.219 sec/batch), lr: 0.116740
2019-12-27 16:39:22.931796: step 80000/136300 (epoch 59/100), loss = 0.437917 (0.219 sec/batch), lr: 0.116740
2019-12-27 16:39:27.878501: step 80020/136300 (epoch 59/100), loss = 0.372995 (0.230 sec/batch), lr: 0.116740
2019-12-27 16:39:32.693051: step 80040/136300 (epoch 59/100), loss = 0.343035 (0.205 sec/batch), lr: 0.116740
2019-12-27 16:39:38.880977: step 80060/136300 (epoch 59/100), loss = 0.334693 (0.235 sec/batch), lr: 0.116740
2019-12-27 16:39:43.723395: step 80080/136300 (epoch 59/100), loss = 0.135867 (0.235 sec/batch), lr: 0.116740
2019-12-27 16:39:48.309539: step 80100/136300 (epoch 59/100), loss = 0.264467 (0.227 sec/batch), lr: 0.116740
2019-12-27 16:39:53.031618: step 80120/136300 (epoch 59/100), loss = 0.254586 (0.236 sec/batch), lr: 0.116740
2019-12-27 16:39:57.797691: step 80140/136300 (epoch 59/100), loss = 0.198426 (0.205 sec/batch), lr: 0.116740
2019-12-27 16:40:02.563775: step 80160/136300 (epoch 59/100), loss = 0.113999 (0.226 sec/batch), lr: 0.116740
2019-12-27 16:40:07.440642: step 80180/136300 (epoch 59/100), loss = 0.293726 (0.224 sec/batch), lr: 0.116740
2019-12-27 16:40:12.137027: step 80200/136300 (epoch 59/100), loss = 0.220131 (0.203 sec/batch), lr: 0.116740
2019-12-27 16:40:16.865751: step 80220/136300 (epoch 59/100), loss = 0.163949 (0.223 sec/batch), lr: 0.116740
2019-12-27 16:40:21.627506: step 80240/136300 (epoch 59/100), loss = 0.187622 (0.245 sec/batch), lr: 0.116740
2019-12-27 16:40:27.595386: step 80260/136300 (epoch 59/100), loss = 0.299580 (0.243 sec/batch), lr: 0.116740
2019-12-27 16:40:32.487998: step 80280/136300 (epoch 59/100), loss = 0.142010 (0.248 sec/batch), lr: 0.116740
2019-12-27 16:40:37.324701: step 80300/136300 (epoch 59/100), loss = 0.235676 (0.237 sec/batch), lr: 0.116740
2019-12-27 16:40:42.073934: step 80320/136300 (epoch 59/100), loss = 0.215005 (0.216 sec/batch), lr: 0.116740
2019-12-27 16:40:46.767298: step 80340/136300 (epoch 59/100), loss = 0.287385 (0.231 sec/batch), lr: 0.116740
2019-12-27 16:40:51.497215: step 80360/136300 (epoch 59/100), loss = 0.234898 (0.242 sec/batch), lr: 0.116740
2019-12-27 16:40:56.352974: step 80380/136300 (epoch 59/100), loss = 0.288720 (0.244 sec/batch), lr: 0.116740
2019-12-27 16:41:01.204645: step 80400/136300 (epoch 59/100), loss = 0.207586 (0.237 sec/batch), lr: 0.116740
Evaluating on dev set...
Precision (micro): 70.299%
   Recall (micro): 63.484%
       F1 (micro): 66.718%
epoch 59: train_loss = 0.249123, dev_loss = 0.444214, dev_f1 = 0.6672
model saved to ./saved_models/01/checkpoint_epoch_59.pt

2019-12-27 16:41:39.906858: step 80420/136300 (epoch 60/100), loss = 0.294085 (0.213 sec/batch), lr: 0.116740
2019-12-27 16:41:45.940766: step 80440/136300 (epoch 60/100), loss = 0.232004 (0.212 sec/batch), lr: 0.116740
2019-12-27 16:41:50.722001: step 80460/136300 (epoch 60/100), loss = 0.353155 (0.241 sec/batch), lr: 0.116740
2019-12-27 16:41:55.481048: step 80480/136300 (epoch 60/100), loss = 0.248042 (0.202 sec/batch), lr: 0.116740
2019-12-27 16:42:00.337555: step 80500/136300 (epoch 60/100), loss = 0.354865 (0.229 sec/batch), lr: 0.116740
2019-12-27 16:42:05.100221: step 80520/136300 (epoch 60/100), loss = 0.225269 (0.210 sec/batch), lr: 0.116740
2019-12-27 16:42:09.857998: step 80540/136300 (epoch 60/100), loss = 0.149151 (0.230 sec/batch), lr: 0.116740
2019-12-27 16:42:14.685848: step 80560/136300 (epoch 60/100), loss = 0.298987 (0.237 sec/batch), lr: 0.116740
2019-12-27 16:42:19.460142: step 80580/136300 (epoch 60/100), loss = 0.211208 (0.223 sec/batch), lr: 0.116740
2019-12-27 16:42:24.312222: step 80600/136300 (epoch 60/100), loss = 0.192055 (0.218 sec/batch), lr: 0.116740
2019-12-27 16:42:29.149227: step 80620/136300 (epoch 60/100), loss = 0.416411 (0.247 sec/batch), lr: 0.116740
2019-12-27 16:42:35.441731: step 80640/136300 (epoch 60/100), loss = 0.260104 (0.213 sec/batch), lr: 0.116740
2019-12-27 16:42:40.193109: step 80660/136300 (epoch 60/100), loss = 0.403828 (0.237 sec/batch), lr: 0.116740
2019-12-27 16:42:44.929645: step 80680/136300 (epoch 60/100), loss = 0.100276 (0.201 sec/batch), lr: 0.116740
2019-12-27 16:42:49.727602: step 80700/136300 (epoch 60/100), loss = 0.197306 (0.238 sec/batch), lr: 0.116740
2019-12-27 16:42:54.548309: step 80720/136300 (epoch 60/100), loss = 0.420626 (0.231 sec/batch), lr: 0.116740
2019-12-27 16:42:59.238904: step 80740/136300 (epoch 60/100), loss = 0.203430 (0.182 sec/batch), lr: 0.116740
2019-12-27 16:43:03.977732: step 80760/136300 (epoch 60/100), loss = 0.175114 (0.215 sec/batch), lr: 0.116740
2019-12-27 16:43:08.846050: step 80780/136300 (epoch 60/100), loss = 0.302632 (0.217 sec/batch), lr: 0.116740
2019-12-27 16:43:13.556485: step 80800/136300 (epoch 60/100), loss = 0.305691 (0.229 sec/batch), lr: 0.116740
2019-12-27 16:43:18.230502: step 80820/136300 (epoch 60/100), loss = 0.115472 (0.213 sec/batch), lr: 0.116740
2019-12-27 16:43:24.319725: step 80840/136300 (epoch 60/100), loss = 0.405990 (0.214 sec/batch), lr: 0.116740
2019-12-27 16:43:29.208230: step 80860/136300 (epoch 60/100), loss = 0.230538 (0.243 sec/batch), lr: 0.116740
2019-12-27 16:43:34.060764: step 80880/136300 (epoch 60/100), loss = 0.303507 (0.222 sec/batch), lr: 0.116740
2019-12-27 16:43:38.841319: step 80900/136300 (epoch 60/100), loss = 0.149873 (0.238 sec/batch), lr: 0.116740
2019-12-27 16:43:43.632168: step 80920/136300 (epoch 60/100), loss = 0.220182 (0.234 sec/batch), lr: 0.116740
2019-12-27 16:43:48.474130: step 80940/136300 (epoch 60/100), loss = 0.364715 (0.235 sec/batch), lr: 0.116740
2019-12-27 16:43:53.417557: step 80960/136300 (epoch 60/100), loss = 0.182742 (0.222 sec/batch), lr: 0.116740
2019-12-27 16:43:58.144157: step 80980/136300 (epoch 60/100), loss = 0.218438 (0.226 sec/batch), lr: 0.116740
2019-12-27 16:44:02.877346: step 81000/136300 (epoch 60/100), loss = 0.158349 (0.176 sec/batch), lr: 0.116740
2019-12-27 16:44:07.605567: step 81020/136300 (epoch 60/100), loss = 0.312760 (0.234 sec/batch), lr: 0.116740
2019-12-27 16:44:13.662665: step 81040/136300 (epoch 60/100), loss = 0.131800 (0.231 sec/batch), lr: 0.116740
2019-12-27 16:44:18.391391: step 81060/136300 (epoch 60/100), loss = 0.266094 (0.218 sec/batch), lr: 0.116740
2019-12-27 16:44:23.130345: step 81080/136300 (epoch 60/100), loss = 0.368749 (0.219 sec/batch), lr: 0.116740
2019-12-27 16:44:27.957927: step 81100/136300 (epoch 60/100), loss = 0.208732 (0.227 sec/batch), lr: 0.116740
2019-12-27 16:44:32.856049: step 81120/136300 (epoch 60/100), loss = 0.242863 (0.239 sec/batch), lr: 0.116740
2019-12-27 16:44:37.745839: step 81140/136300 (epoch 60/100), loss = 0.188426 (0.240 sec/batch), lr: 0.116740
2019-12-27 16:44:42.582017: step 81160/136300 (epoch 60/100), loss = 0.287400 (0.229 sec/batch), lr: 0.116740
2019-12-27 16:44:47.305492: step 81180/136300 (epoch 60/100), loss = 0.233831 (0.237 sec/batch), lr: 0.116740
2019-12-27 16:44:52.106810: step 81200/136300 (epoch 60/100), loss = 0.243104 (0.240 sec/batch), lr: 0.116740
2019-12-27 16:44:58.491087: step 81220/136300 (epoch 60/100), loss = 0.262388 (0.248 sec/batch), lr: 0.116740
2019-12-27 16:45:03.224932: step 81240/136300 (epoch 60/100), loss = 0.451997 (0.249 sec/batch), lr: 0.116740
2019-12-27 16:45:08.100222: step 81260/136300 (epoch 60/100), loss = 0.580259 (0.237 sec/batch), lr: 0.116740
2019-12-27 16:45:12.872895: step 81280/136300 (epoch 60/100), loss = 0.313957 (0.210 sec/batch), lr: 0.116740
2019-12-27 16:45:17.548631: step 81300/136300 (epoch 60/100), loss = 0.190681 (0.215 sec/batch), lr: 0.116740
2019-12-27 16:45:22.401567: step 81320/136300 (epoch 60/100), loss = 0.153946 (0.234 sec/batch), lr: 0.116740
2019-12-27 16:45:27.307656: step 81340/136300 (epoch 60/100), loss = 0.108232 (0.247 sec/batch), lr: 0.116740
2019-12-27 16:45:32.060713: step 81360/136300 (epoch 60/100), loss = 0.377567 (0.241 sec/batch), lr: 0.116740
2019-12-27 16:45:36.952762: step 81380/136300 (epoch 60/100), loss = 0.185872 (0.177 sec/batch), lr: 0.116740
2019-12-27 16:45:41.838858: step 81400/136300 (epoch 60/100), loss = 0.187319 (0.237 sec/batch), lr: 0.116740
2019-12-27 16:45:47.959218: step 81420/136300 (epoch 60/100), loss = 0.335593 (0.197 sec/batch), lr: 0.116740
2019-12-27 16:45:52.784950: step 81440/136300 (epoch 60/100), loss = 0.183501 (0.231 sec/batch), lr: 0.116740
2019-12-27 16:45:57.398067: step 81460/136300 (epoch 60/100), loss = 0.426879 (0.210 sec/batch), lr: 0.116740
2019-12-27 16:46:02.141997: step 81480/136300 (epoch 60/100), loss = 0.260157 (0.237 sec/batch), lr: 0.116740
2019-12-27 16:46:06.966737: step 81500/136300 (epoch 60/100), loss = 0.221691 (0.244 sec/batch), lr: 0.116740
2019-12-27 16:46:11.671762: step 81520/136300 (epoch 60/100), loss = 0.274886 (0.217 sec/batch), lr: 0.116740
2019-12-27 16:46:16.555959: step 81540/136300 (epoch 60/100), loss = 0.192673 (0.218 sec/batch), lr: 0.116740
2019-12-27 16:46:21.343796: step 81560/136300 (epoch 60/100), loss = 0.274542 (0.180 sec/batch), lr: 0.116740
2019-12-27 16:46:26.092275: step 81580/136300 (epoch 60/100), loss = 0.055855 (0.213 sec/batch), lr: 0.116740
2019-12-27 16:46:30.748093: step 81600/136300 (epoch 60/100), loss = 0.395188 (0.164 sec/batch), lr: 0.116740
2019-12-27 16:46:37.161512: step 81620/136300 (epoch 60/100), loss = 0.261524 (0.235 sec/batch), lr: 0.116740
2019-12-27 16:46:42.013240: step 81640/136300 (epoch 60/100), loss = 0.267910 (0.229 sec/batch), lr: 0.116740
2019-12-27 16:46:46.871974: step 81660/136300 (epoch 60/100), loss = 0.370766 (0.204 sec/batch), lr: 0.116740
2019-12-27 16:46:51.662793: step 81680/136300 (epoch 60/100), loss = 0.312065 (0.221 sec/batch), lr: 0.116740
2019-12-27 16:46:56.331258: step 81700/136300 (epoch 60/100), loss = 0.283557 (0.217 sec/batch), lr: 0.116740
2019-12-27 16:47:01.046065: step 81720/136300 (epoch 60/100), loss = 0.283552 (0.203 sec/batch), lr: 0.116740
2019-12-27 16:47:05.875937: step 81740/136300 (epoch 60/100), loss = 0.150160 (0.232 sec/batch), lr: 0.116740
2019-12-27 16:47:10.751111: step 81760/136300 (epoch 60/100), loss = 0.160452 (0.240 sec/batch), lr: 0.116740
2019-12-27 16:47:15.492266: step 81780/136300 (epoch 60/100), loss = 0.047587 (0.130 sec/batch), lr: 0.116740
Evaluating on dev set...
Precision (micro): 71.224%
   Recall (micro): 62.196%
       F1 (micro): 66.405%
epoch 60: train_loss = 0.247514, dev_loss = 0.448978, dev_f1 = 0.6640
model saved to ./saved_models/01/checkpoint_epoch_60.pt

2019-12-27 16:47:55.199698: step 81800/136300 (epoch 61/100), loss = 0.296095 (0.217 sec/batch), lr: 0.105066
2019-12-27 16:47:59.885420: step 81820/136300 (epoch 61/100), loss = 0.107599 (0.234 sec/batch), lr: 0.105066
2019-12-27 16:48:04.670143: step 81840/136300 (epoch 61/100), loss = 0.128993 (0.218 sec/batch), lr: 0.105066
2019-12-27 16:48:09.502845: step 81860/136300 (epoch 61/100), loss = 0.140901 (0.222 sec/batch), lr: 0.105066
2019-12-27 16:48:14.280042: step 81880/136300 (epoch 61/100), loss = 0.163267 (0.232 sec/batch), lr: 0.105066
2019-12-27 16:48:19.019546: step 81900/136300 (epoch 61/100), loss = 0.307201 (0.245 sec/batch), lr: 0.105066
2019-12-27 16:48:23.773419: step 81920/136300 (epoch 61/100), loss = 0.335924 (0.245 sec/batch), lr: 0.105066
2019-12-27 16:48:28.556246: step 81940/136300 (epoch 61/100), loss = 0.111897 (0.241 sec/batch), lr: 0.105066
2019-12-27 16:48:33.414397: step 81960/136300 (epoch 61/100), loss = 0.110649 (0.235 sec/batch), lr: 0.105066
2019-12-27 16:48:38.231930: step 81980/136300 (epoch 61/100), loss = 0.252521 (0.237 sec/batch), lr: 0.105066
2019-12-27 16:48:44.109182: step 82000/136300 (epoch 61/100), loss = 0.306552 (0.201 sec/batch), lr: 0.105066
2019-12-27 16:48:48.765940: step 82020/136300 (epoch 61/100), loss = 0.222867 (0.200 sec/batch), lr: 0.105066
2019-12-27 16:48:53.541676: step 82040/136300 (epoch 61/100), loss = 0.143933 (0.209 sec/batch), lr: 0.105066
2019-12-27 16:48:58.339298: step 82060/136300 (epoch 61/100), loss = 0.116170 (0.182 sec/batch), lr: 0.105066
2019-12-27 16:49:03.149879: step 82080/136300 (epoch 61/100), loss = 0.225114 (0.239 sec/batch), lr: 0.105066
2019-12-27 16:49:07.877180: step 82100/136300 (epoch 61/100), loss = 0.183412 (0.224 sec/batch), lr: 0.105066
2019-12-27 16:49:12.635656: step 82120/136300 (epoch 61/100), loss = 0.221746 (0.208 sec/batch), lr: 0.105066
2019-12-27 16:49:17.479331: step 82140/136300 (epoch 61/100), loss = 0.521388 (0.217 sec/batch), lr: 0.105066
2019-12-27 16:49:22.209780: step 82160/136300 (epoch 61/100), loss = 0.295040 (0.233 sec/batch), lr: 0.105066
2019-12-27 16:49:26.945627: step 82180/136300 (epoch 61/100), loss = 0.290924 (0.246 sec/batch), lr: 0.105066
2019-12-27 16:49:32.987826: step 82200/136300 (epoch 61/100), loss = 0.092240 (0.232 sec/batch), lr: 0.105066
2019-12-27 16:49:37.801320: step 82220/136300 (epoch 61/100), loss = 0.372721 (0.228 sec/batch), lr: 0.105066
2019-12-27 16:49:42.641996: step 82240/136300 (epoch 61/100), loss = 0.388604 (0.246 sec/batch), lr: 0.105066
2019-12-27 16:49:47.450545: step 82260/136300 (epoch 61/100), loss = 0.261612 (0.245 sec/batch), lr: 0.105066
2019-12-27 16:49:52.202549: step 82280/136300 (epoch 61/100), loss = 0.376970 (0.231 sec/batch), lr: 0.105066
2019-12-27 16:49:57.073893: step 82300/136300 (epoch 61/100), loss = 0.311058 (0.242 sec/batch), lr: 0.105066
2019-12-27 16:50:01.963556: step 82320/136300 (epoch 61/100), loss = 0.255377 (0.240 sec/batch), lr: 0.105066
2019-12-27 16:50:06.729662: step 82340/136300 (epoch 61/100), loss = 0.295710 (0.214 sec/batch), lr: 0.105066
2019-12-27 16:50:11.514586: step 82360/136300 (epoch 61/100), loss = 0.185998 (0.212 sec/batch), lr: 0.105066
2019-12-27 16:50:16.187228: step 82380/136300 (epoch 61/100), loss = 0.279038 (0.204 sec/batch), lr: 0.105066
2019-12-27 16:50:22.144084: step 82400/136300 (epoch 61/100), loss = 0.283458 (0.225 sec/batch), lr: 0.105066
2019-12-27 16:50:26.901657: step 82420/136300 (epoch 61/100), loss = 0.183566 (0.179 sec/batch), lr: 0.105066
2019-12-27 16:50:31.636053: step 82440/136300 (epoch 61/100), loss = 0.199321 (0.206 sec/batch), lr: 0.105066
2019-12-27 16:50:36.465401: step 82460/136300 (epoch 61/100), loss = 0.377882 (0.223 sec/batch), lr: 0.105066
2019-12-27 16:50:41.363527: step 82480/136300 (epoch 61/100), loss = 0.348858 (0.240 sec/batch), lr: 0.105066
2019-12-27 16:50:46.222698: step 82500/136300 (epoch 61/100), loss = 0.230302 (0.227 sec/batch), lr: 0.105066
2019-12-27 16:50:51.051832: step 82520/136300 (epoch 61/100), loss = 0.126687 (0.172 sec/batch), lr: 0.105066
2019-12-27 16:50:55.825785: step 82540/136300 (epoch 61/100), loss = 0.313619 (0.243 sec/batch), lr: 0.105066
2019-12-27 16:51:00.591176: step 82560/136300 (epoch 61/100), loss = 0.256041 (0.248 sec/batch), lr: 0.105066
2019-12-27 16:51:06.766749: step 82580/136300 (epoch 61/100), loss = 0.520845 (0.205 sec/batch), lr: 0.105066
2019-12-27 16:51:11.530879: step 82600/136300 (epoch 61/100), loss = 0.301645 (0.245 sec/batch), lr: 0.105066
2019-12-27 16:51:16.357206: step 82620/136300 (epoch 61/100), loss = 0.228668 (0.231 sec/batch), lr: 0.105066
2019-12-27 16:51:21.208019: step 82640/136300 (epoch 61/100), loss = 0.269694 (0.225 sec/batch), lr: 0.105066
2019-12-27 16:51:25.907545: step 82660/136300 (epoch 61/100), loss = 0.153465 (0.246 sec/batch), lr: 0.105066
2019-12-27 16:51:30.692995: step 82680/136300 (epoch 61/100), loss = 0.273831 (0.209 sec/batch), lr: 0.105066
2019-12-27 16:51:35.581435: step 82700/136300 (epoch 61/100), loss = 0.245290 (0.221 sec/batch), lr: 0.105066
2019-12-27 16:51:40.389672: step 82720/136300 (epoch 61/100), loss = 0.074955 (0.233 sec/batch), lr: 0.105066
2019-12-27 16:51:45.293691: step 82740/136300 (epoch 61/100), loss = 0.237609 (0.216 sec/batch), lr: 0.105066
2019-12-27 16:51:50.152593: step 82760/136300 (epoch 61/100), loss = 0.311638 (0.216 sec/batch), lr: 0.105066
2019-12-27 16:51:54.936273: step 82780/136300 (epoch 61/100), loss = 0.243817 (0.240 sec/batch), lr: 0.105066
2019-12-27 16:52:01.229487: step 82800/136300 (epoch 61/100), loss = 0.320985 (0.232 sec/batch), lr: 0.105066
2019-12-27 16:52:05.918196: step 82820/136300 (epoch 61/100), loss = 0.224900 (0.183 sec/batch), lr: 0.105066
2019-12-27 16:52:10.582220: step 82840/136300 (epoch 61/100), loss = 0.265713 (0.168 sec/batch), lr: 0.105066
2019-12-27 16:52:15.403267: step 82860/136300 (epoch 61/100), loss = 0.272035 (0.229 sec/batch), lr: 0.105066
2019-12-27 16:52:20.146216: step 82880/136300 (epoch 61/100), loss = 0.371604 (0.235 sec/batch), lr: 0.105066
2019-12-27 16:52:25.022102: step 82900/136300 (epoch 61/100), loss = 0.229349 (0.214 sec/batch), lr: 0.105066
2019-12-27 16:52:29.831320: step 82920/136300 (epoch 61/100), loss = 0.136563 (0.224 sec/batch), lr: 0.105066
2019-12-27 16:52:34.538053: step 82940/136300 (epoch 61/100), loss = 0.146987 (0.212 sec/batch), lr: 0.105066
2019-12-27 16:52:39.246934: step 82960/136300 (epoch 61/100), loss = 0.312196 (0.232 sec/batch), lr: 0.105066
2019-12-27 16:52:45.234283: step 82980/136300 (epoch 61/100), loss = 0.176185 (1.437 sec/batch), lr: 0.105066
2019-12-27 16:52:50.072046: step 83000/136300 (epoch 61/100), loss = 0.378338 (0.219 sec/batch), lr: 0.105066
2019-12-27 16:52:54.956854: step 83020/136300 (epoch 61/100), loss = 0.216039 (0.230 sec/batch), lr: 0.105066
2019-12-27 16:52:59.776939: step 83040/136300 (epoch 61/100), loss = 0.320306 (0.247 sec/batch), lr: 0.105066
2019-12-27 16:53:04.419067: step 83060/136300 (epoch 61/100), loss = 0.072714 (0.235 sec/batch), lr: 0.105066
2019-12-27 16:53:09.172027: step 83080/136300 (epoch 61/100), loss = 0.372475 (0.178 sec/batch), lr: 0.105066
2019-12-27 16:53:13.926790: step 83100/136300 (epoch 61/100), loss = 0.167737 (0.197 sec/batch), lr: 0.105066
2019-12-27 16:53:18.805935: step 83120/136300 (epoch 61/100), loss = 0.119560 (0.218 sec/batch), lr: 0.105066
2019-12-27 16:53:23.744839: step 83140/136300 (epoch 61/100), loss = 0.149605 (0.229 sec/batch), lr: 0.105066
Evaluating on dev set...
Precision (micro): 71.294%
   Recall (micro): 62.822%
       F1 (micro): 66.791%
epoch 61: train_loss = 0.245595, dev_loss = 0.449320, dev_f1 = 0.6679
model saved to ./saved_models/01/checkpoint_epoch_61.pt

2019-12-27 16:54:01.966946: step 83160/136300 (epoch 62/100), loss = 0.175772 (0.246 sec/batch), lr: 0.105066
2019-12-27 16:54:07.948176: step 83180/136300 (epoch 62/100), loss = 0.133247 (0.248 sec/batch), lr: 0.105066
2019-12-27 16:54:12.747181: step 83200/136300 (epoch 62/100), loss = 0.263048 (0.216 sec/batch), lr: 0.105066
2019-12-27 16:54:17.560033: step 83220/136300 (epoch 62/100), loss = 0.281137 (0.241 sec/batch), lr: 0.105066
2019-12-27 16:54:22.363684: step 83240/136300 (epoch 62/100), loss = 0.216442 (0.216 sec/batch), lr: 0.105066
2019-12-27 16:54:27.101915: step 83260/136300 (epoch 62/100), loss = 0.206507 (0.229 sec/batch), lr: 0.105066
2019-12-27 16:54:31.881306: step 83280/136300 (epoch 62/100), loss = 0.341059 (0.222 sec/batch), lr: 0.105066
2019-12-27 16:54:36.668757: step 83300/136300 (epoch 62/100), loss = 0.386847 (0.228 sec/batch), lr: 0.105066
2019-12-27 16:54:41.517312: step 83320/136300 (epoch 62/100), loss = 0.356483 (0.234 sec/batch), lr: 0.105066
2019-12-27 16:54:46.366904: step 83340/136300 (epoch 62/100), loss = 0.353187 (0.234 sec/batch), lr: 0.105066
2019-12-27 16:54:52.618097: step 83360/136300 (epoch 62/100), loss = 0.194604 (0.240 sec/batch), lr: 0.105066
2019-12-27 16:54:57.317036: step 83380/136300 (epoch 62/100), loss = 0.279423 (0.238 sec/batch), lr: 0.105066
2019-12-27 16:55:02.070716: step 83400/136300 (epoch 62/100), loss = 0.189477 (0.231 sec/batch), lr: 0.105066
2019-12-27 16:55:06.902433: step 83420/136300 (epoch 62/100), loss = 0.197100 (0.226 sec/batch), lr: 0.105066
2019-12-27 16:55:11.671743: step 83440/136300 (epoch 62/100), loss = 0.337901 (0.177 sec/batch), lr: 0.105066
2019-12-27 16:55:16.381006: step 83460/136300 (epoch 62/100), loss = 0.169858 (0.219 sec/batch), lr: 0.105066
2019-12-27 16:55:21.188508: step 83480/136300 (epoch 62/100), loss = 0.177380 (0.230 sec/batch), lr: 0.105066
2019-12-27 16:55:25.992639: step 83500/136300 (epoch 62/100), loss = 0.202068 (0.230 sec/batch), lr: 0.105066
2019-12-27 16:55:30.690510: step 83520/136300 (epoch 62/100), loss = 0.144223 (0.226 sec/batch), lr: 0.105066
2019-12-27 16:55:35.427279: step 83540/136300 (epoch 62/100), loss = 0.233726 (0.225 sec/batch), lr: 0.105066
2019-12-27 16:55:41.479748: step 83560/136300 (epoch 62/100), loss = 0.286615 (0.242 sec/batch), lr: 0.105066
2019-12-27 16:55:46.275077: step 83580/136300 (epoch 62/100), loss = 0.274390 (0.240 sec/batch), lr: 0.105066
2019-12-27 16:55:51.117533: step 83600/136300 (epoch 62/100), loss = 0.300814 (0.242 sec/batch), lr: 0.105066
2019-12-27 16:55:55.913182: step 83620/136300 (epoch 62/100), loss = 0.516510 (0.237 sec/batch), lr: 0.105066
2019-12-27 16:56:00.691516: step 83640/136300 (epoch 62/100), loss = 0.370205 (0.215 sec/batch), lr: 0.105066
2019-12-27 16:56:05.563489: step 83660/136300 (epoch 62/100), loss = 0.397206 (0.232 sec/batch), lr: 0.105066
2019-12-27 16:56:10.452175: step 83680/136300 (epoch 62/100), loss = 0.273448 (0.241 sec/batch), lr: 0.105066
2019-12-27 16:56:15.213824: step 83700/136300 (epoch 62/100), loss = 0.392129 (0.235 sec/batch), lr: 0.105066
2019-12-27 16:56:20.010912: step 83720/136300 (epoch 62/100), loss = 0.191713 (0.238 sec/batch), lr: 0.105066
2019-12-27 16:56:24.714288: step 83740/136300 (epoch 62/100), loss = 0.346895 (0.225 sec/batch), lr: 0.105066
2019-12-27 16:56:30.770629: step 83760/136300 (epoch 62/100), loss = 0.212284 (0.198 sec/batch), lr: 0.105066
2019-12-27 16:56:35.574237: step 83780/136300 (epoch 62/100), loss = 0.270719 (0.231 sec/batch), lr: 0.105066
2019-12-27 16:56:40.265078: step 83800/136300 (epoch 62/100), loss = 0.097747 (0.223 sec/batch), lr: 0.105066
2019-12-27 16:56:45.052295: step 83820/136300 (epoch 62/100), loss = 0.351892 (0.230 sec/batch), lr: 0.105066
2019-12-27 16:56:49.984847: step 83840/136300 (epoch 62/100), loss = 0.448630 (0.232 sec/batch), lr: 0.105066
2019-12-27 16:56:54.815603: step 83860/136300 (epoch 62/100), loss = 0.248165 (0.228 sec/batch), lr: 0.105066
2019-12-27 16:56:59.678069: step 83880/136300 (epoch 62/100), loss = 0.305054 (0.234 sec/batch), lr: 0.105066
2019-12-27 16:57:04.426782: step 83900/136300 (epoch 62/100), loss = 0.384679 (0.231 sec/batch), lr: 0.105066
2019-12-27 16:57:09.208925: step 83920/136300 (epoch 62/100), loss = 0.173193 (0.215 sec/batch), lr: 0.105066
2019-12-27 16:57:14.105428: step 83940/136300 (epoch 62/100), loss = 0.224488 (0.237 sec/batch), lr: 0.105066
2019-12-27 16:57:20.049677: step 83960/136300 (epoch 62/100), loss = 0.150470 (0.209 sec/batch), lr: 0.105066
2019-12-27 16:57:24.798161: step 83980/136300 (epoch 62/100), loss = 0.148824 (0.229 sec/batch), lr: 0.105066
2019-12-27 16:57:29.713950: step 84000/136300 (epoch 62/100), loss = 0.133439 (0.226 sec/batch), lr: 0.105066
2019-12-27 16:57:34.353617: step 84020/136300 (epoch 62/100), loss = 0.212273 (0.203 sec/batch), lr: 0.105066
2019-12-27 16:57:39.206196: step 84040/136300 (epoch 62/100), loss = 0.150222 (0.227 sec/batch), lr: 0.105066
2019-12-27 16:57:44.069678: step 84060/136300 (epoch 62/100), loss = 0.201584 (0.212 sec/batch), lr: 0.105066
2019-12-27 16:57:48.853127: step 84080/136300 (epoch 62/100), loss = 0.152198 (0.234 sec/batch), lr: 0.105066
2019-12-27 16:57:53.787465: step 84100/136300 (epoch 62/100), loss = 0.226064 (0.241 sec/batch), lr: 0.105066
2019-12-27 16:57:58.648046: step 84120/136300 (epoch 62/100), loss = 0.521198 (0.232 sec/batch), lr: 0.105066
2019-12-27 16:58:03.441925: step 84140/136300 (epoch 62/100), loss = 0.137044 (0.241 sec/batch), lr: 0.105066
2019-12-27 16:58:09.604725: step 84160/136300 (epoch 62/100), loss = 0.177673 (0.213 sec/batch), lr: 0.105066
2019-12-27 16:58:14.405487: step 84180/136300 (epoch 62/100), loss = 0.213401 (0.210 sec/batch), lr: 0.105066
2019-12-27 16:58:19.029669: step 84200/136300 (epoch 62/100), loss = 0.104415 (0.213 sec/batch), lr: 0.105066
2019-12-27 16:58:23.780826: step 84220/136300 (epoch 62/100), loss = 0.244690 (0.234 sec/batch), lr: 0.105066
2019-12-27 16:58:28.537558: step 84240/136300 (epoch 62/100), loss = 0.179549 (0.199 sec/batch), lr: 0.105066
2019-12-27 16:58:33.359921: step 84260/136300 (epoch 62/100), loss = 0.099716 (0.238 sec/batch), lr: 0.105066
2019-12-27 16:58:38.177712: step 84280/136300 (epoch 62/100), loss = 0.310363 (0.198 sec/batch), lr: 0.105066
2019-12-27 16:58:42.901203: step 84300/136300 (epoch 62/100), loss = 0.422337 (0.209 sec/batch), lr: 0.105066
2019-12-27 16:58:47.560698: step 84320/136300 (epoch 62/100), loss = 0.125153 (0.239 sec/batch), lr: 0.105066
2019-12-27 16:58:52.367527: step 84340/136300 (epoch 62/100), loss = 0.283006 (0.208 sec/batch), lr: 0.105066
2019-12-27 16:58:58.393115: step 84360/136300 (epoch 62/100), loss = 0.176665 (0.208 sec/batch), lr: 0.105066
2019-12-27 16:59:03.278137: step 84380/136300 (epoch 62/100), loss = 0.212071 (0.214 sec/batch), lr: 0.105066
2019-12-27 16:59:08.111604: step 84400/136300 (epoch 62/100), loss = 0.200818 (0.203 sec/batch), lr: 0.105066
2019-12-27 16:59:12.795345: step 84420/136300 (epoch 62/100), loss = 0.191750 (0.245 sec/batch), lr: 0.105066
2019-12-27 16:59:17.611567: step 84440/136300 (epoch 62/100), loss = 0.134773 (0.241 sec/batch), lr: 0.105066
2019-12-27 16:59:22.322928: step 84460/136300 (epoch 62/100), loss = 0.227753 (0.211 sec/batch), lr: 0.105066
2019-12-27 16:59:27.200957: step 84480/136300 (epoch 62/100), loss = 0.457533 (0.198 sec/batch), lr: 0.105066
2019-12-27 16:59:32.079807: step 84500/136300 (epoch 62/100), loss = 0.242459 (0.240 sec/batch), lr: 0.105066
Evaluating on dev set...
Precision (micro): 72.057%
   Recall (micro): 62.951%
       F1 (micro): 67.197%
epoch 62: train_loss = 0.243881, dev_loss = 0.447157, dev_f1 = 0.6720
model saved to ./saved_models/01/checkpoint_epoch_62.pt
new best model saved.

2019-12-27 17:00:10.562572: step 84520/136300 (epoch 63/100), loss = 0.235731 (0.231 sec/batch), lr: 0.105066
2019-12-27 17:00:16.733963: step 84540/136300 (epoch 63/100), loss = 0.160466 (0.182 sec/batch), lr: 0.105066
2019-12-27 17:00:21.514020: step 84560/136300 (epoch 63/100), loss = 0.291225 (0.168 sec/batch), lr: 0.105066
2019-12-27 17:00:26.255702: step 84580/136300 (epoch 63/100), loss = 0.428426 (0.221 sec/batch), lr: 0.105066
2019-12-27 17:00:31.189634: step 84600/136300 (epoch 63/100), loss = 0.301460 (0.228 sec/batch), lr: 0.105066
2019-12-27 17:00:35.946825: step 84620/136300 (epoch 63/100), loss = 0.235644 (0.247 sec/batch), lr: 0.105066
2019-12-27 17:00:40.747467: step 84640/136300 (epoch 63/100), loss = 0.201186 (0.221 sec/batch), lr: 0.105066
2019-12-27 17:00:45.548199: step 84660/136300 (epoch 63/100), loss = 0.195177 (0.244 sec/batch), lr: 0.105066
2019-12-27 17:00:50.433423: step 84680/136300 (epoch 63/100), loss = 0.134558 (0.243 sec/batch), lr: 0.105066
2019-12-27 17:00:55.294103: step 84700/136300 (epoch 63/100), loss = 0.223005 (0.214 sec/batch), lr: 0.105066
2019-12-27 17:01:01.538559: step 84720/136300 (epoch 63/100), loss = 0.206776 (0.239 sec/batch), lr: 0.105066
2019-12-27 17:01:06.231611: step 84740/136300 (epoch 63/100), loss = 0.370654 (0.224 sec/batch), lr: 0.105066
2019-12-27 17:01:11.050714: step 84760/136300 (epoch 63/100), loss = 0.237366 (0.238 sec/batch), lr: 0.105066
2019-12-27 17:01:15.946555: step 84780/136300 (epoch 63/100), loss = 0.156742 (0.225 sec/batch), lr: 0.105066
2019-12-27 17:01:20.790831: step 84800/136300 (epoch 63/100), loss = 0.246391 (0.239 sec/batch), lr: 0.105066
2019-12-27 17:01:25.524710: step 84820/136300 (epoch 63/100), loss = 0.212826 (0.246 sec/batch), lr: 0.105066
2019-12-27 17:01:30.340935: step 84840/136300 (epoch 63/100), loss = 0.450680 (0.238 sec/batch), lr: 0.105066
2019-12-27 17:01:35.168227: step 84860/136300 (epoch 63/100), loss = 0.151150 (0.208 sec/batch), lr: 0.105066
2019-12-27 17:01:39.947118: step 84880/136300 (epoch 63/100), loss = 0.109270 (0.231 sec/batch), lr: 0.105066
2019-12-27 17:01:44.735003: step 84900/136300 (epoch 63/100), loss = 0.199626 (0.248 sec/batch), lr: 0.105066
2019-12-27 17:01:50.829196: step 84920/136300 (epoch 63/100), loss = 0.150586 (0.245 sec/batch), lr: 0.105066
2019-12-27 17:01:55.654155: step 84940/136300 (epoch 63/100), loss = 0.217345 (0.231 sec/batch), lr: 0.105066
2019-12-27 17:02:00.560817: step 84960/136300 (epoch 63/100), loss = 0.522911 (0.248 sec/batch), lr: 0.105066
2019-12-27 17:02:05.385208: step 84980/136300 (epoch 63/100), loss = 0.416277 (0.232 sec/batch), lr: 0.105066
2019-12-27 17:02:10.213379: step 85000/136300 (epoch 63/100), loss = 0.288472 (0.235 sec/batch), lr: 0.105066
2019-12-27 17:02:15.055361: step 85020/136300 (epoch 63/100), loss = 0.232804 (0.233 sec/batch), lr: 0.105066
2019-12-27 17:02:19.906481: step 85040/136300 (epoch 63/100), loss = 0.137831 (0.244 sec/batch), lr: 0.105066
2019-12-27 17:02:24.709110: step 85060/136300 (epoch 63/100), loss = 0.376869 (0.203 sec/batch), lr: 0.105066
2019-12-27 17:02:29.573291: step 85080/136300 (epoch 63/100), loss = 0.151884 (0.243 sec/batch), lr: 0.105066
2019-12-27 17:02:34.360025: step 85100/136300 (epoch 63/100), loss = 0.095718 (0.238 sec/batch), lr: 0.105066
2019-12-27 17:02:40.602277: step 85120/136300 (epoch 63/100), loss = 0.241983 (0.251 sec/batch), lr: 0.105066
2019-12-27 17:02:45.391616: step 85140/136300 (epoch 63/100), loss = 0.181435 (0.250 sec/batch), lr: 0.105066
2019-12-27 17:02:50.144263: step 85160/136300 (epoch 63/100), loss = 0.165116 (0.243 sec/batch), lr: 0.105066
2019-12-27 17:02:54.969874: step 85180/136300 (epoch 63/100), loss = 0.254582 (0.234 sec/batch), lr: 0.105066
2019-12-27 17:02:59.894680: step 85200/136300 (epoch 63/100), loss = 0.213661 (0.241 sec/batch), lr: 0.105066
2019-12-27 17:03:04.767211: step 85220/136300 (epoch 63/100), loss = 0.298633 (0.231 sec/batch), lr: 0.105066
2019-12-27 17:03:09.702563: step 85240/136300 (epoch 63/100), loss = 0.205514 (0.235 sec/batch), lr: 0.105066
2019-12-27 17:03:14.548513: step 85260/136300 (epoch 63/100), loss = 0.355883 (0.244 sec/batch), lr: 0.105066
2019-12-27 17:03:19.297837: step 85280/136300 (epoch 63/100), loss = 0.189752 (0.239 sec/batch), lr: 0.105066
2019-12-27 17:03:24.213700: step 85300/136300 (epoch 63/100), loss = 0.205949 (0.221 sec/batch), lr: 0.105066
2019-12-27 17:03:30.378805: step 85320/136300 (epoch 63/100), loss = 0.438230 (0.222 sec/batch), lr: 0.105066
2019-12-27 17:03:35.161240: step 85340/136300 (epoch 63/100), loss = 0.185641 (0.220 sec/batch), lr: 0.105066
2019-12-27 17:03:40.076613: step 85360/136300 (epoch 63/100), loss = 0.233529 (0.228 sec/batch), lr: 0.105066
2019-12-27 17:03:44.720038: step 85380/136300 (epoch 63/100), loss = 0.272920 (0.242 sec/batch), lr: 0.105066
2019-12-27 17:03:49.566207: step 85400/136300 (epoch 63/100), loss = 0.234218 (0.227 sec/batch), lr: 0.105066
2019-12-27 17:03:54.468809: step 85420/136300 (epoch 63/100), loss = 0.244633 (0.221 sec/batch), lr: 0.105066
2019-12-27 17:03:59.216576: step 85440/136300 (epoch 63/100), loss = 0.258206 (0.211 sec/batch), lr: 0.105066
2019-12-27 17:04:04.137224: step 85460/136300 (epoch 63/100), loss = 0.188848 (0.217 sec/batch), lr: 0.105066
2019-12-27 17:04:08.967827: step 85480/136300 (epoch 63/100), loss = 0.209053 (0.213 sec/batch), lr: 0.105066
2019-12-27 17:04:13.782012: step 85500/136300 (epoch 63/100), loss = 0.339228 (0.218 sec/batch), lr: 0.105066
2019-12-27 17:04:19.817653: step 85520/136300 (epoch 63/100), loss = 0.240158 (0.199 sec/batch), lr: 0.105066
2019-12-27 17:04:24.632514: step 85540/136300 (epoch 63/100), loss = 0.135649 (0.239 sec/batch), lr: 0.105066
2019-12-27 17:04:29.253692: step 85560/136300 (epoch 63/100), loss = 0.150972 (0.212 sec/batch), lr: 0.105066
2019-12-27 17:04:33.977630: step 85580/136300 (epoch 63/100), loss = 0.228323 (0.241 sec/batch), lr: 0.105066
2019-12-27 17:04:38.762508: step 85600/136300 (epoch 63/100), loss = 0.231450 (0.203 sec/batch), lr: 0.105066
2019-12-27 17:04:43.570014: step 85620/136300 (epoch 63/100), loss = 0.200773 (0.240 sec/batch), lr: 0.105066
2019-12-27 17:04:48.429641: step 85640/136300 (epoch 63/100), loss = 0.340949 (0.234 sec/batch), lr: 0.105066
2019-12-27 17:04:53.146854: step 85660/136300 (epoch 63/100), loss = 0.299452 (0.238 sec/batch), lr: 0.105066
2019-12-27 17:04:57.801767: step 85680/136300 (epoch 63/100), loss = 0.243008 (0.205 sec/batch), lr: 0.105066
2019-12-27 17:05:02.604779: step 85700/136300 (epoch 63/100), loss = 0.294202 (0.213 sec/batch), lr: 0.105066
2019-12-27 17:05:08.900871: step 85720/136300 (epoch 63/100), loss = 0.242210 (0.198 sec/batch), lr: 0.105066
2019-12-27 17:05:13.794813: step 85740/136300 (epoch 63/100), loss = 0.274228 (0.212 sec/batch), lr: 0.105066
2019-12-27 17:05:18.659679: step 85760/136300 (epoch 63/100), loss = 0.274101 (0.215 sec/batch), lr: 0.105066
2019-12-27 17:05:23.289676: step 85780/136300 (epoch 63/100), loss = 0.137196 (0.246 sec/batch), lr: 0.105066
2019-12-27 17:05:28.042709: step 85800/136300 (epoch 63/100), loss = 0.254139 (0.228 sec/batch), lr: 0.105066
2019-12-27 17:05:32.814305: step 85820/136300 (epoch 63/100), loss = 0.276453 (0.227 sec/batch), lr: 0.105066
2019-12-27 17:05:37.686111: step 85840/136300 (epoch 63/100), loss = 0.038770 (0.227 sec/batch), lr: 0.105066
2019-12-27 17:05:42.509362: step 85860/136300 (epoch 63/100), loss = 0.166975 (0.230 sec/batch), lr: 0.105066
Evaluating on dev set...
Precision (micro): 69.994%
   Recall (micro): 63.466%
       F1 (micro): 66.570%
epoch 63: train_loss = 0.243168, dev_loss = 0.454070, dev_f1 = 0.6657
model saved to ./saved_models/01/checkpoint_epoch_63.pt

2019-12-27 17:06:20.693117: step 85880/136300 (epoch 64/100), loss = 0.123046 (0.243 sec/batch), lr: 0.094560
2019-12-27 17:06:26.849744: step 85900/136300 (epoch 64/100), loss = 0.182852 (0.208 sec/batch), lr: 0.094560
2019-12-27 17:06:31.649025: step 85920/136300 (epoch 64/100), loss = 0.190688 (0.241 sec/batch), lr: 0.094560
2019-12-27 17:06:36.321688: step 85940/136300 (epoch 64/100), loss = 0.219975 (0.230 sec/batch), lr: 0.094560
2019-12-27 17:06:41.236594: step 85960/136300 (epoch 64/100), loss = 0.219763 (0.233 sec/batch), lr: 0.094560
2019-12-27 17:06:45.927990: step 85980/136300 (epoch 64/100), loss = 0.216239 (0.179 sec/batch), lr: 0.094560
2019-12-27 17:06:50.675879: step 86000/136300 (epoch 64/100), loss = 0.392476 (0.209 sec/batch), lr: 0.094560
2019-12-27 17:06:55.477691: step 86020/136300 (epoch 64/100), loss = 0.153915 (0.245 sec/batch), lr: 0.094560
2019-12-27 17:07:00.271895: step 86040/136300 (epoch 64/100), loss = 0.337322 (0.214 sec/batch), lr: 0.094560
2019-12-27 17:07:05.122379: step 86060/136300 (epoch 64/100), loss = 0.161310 (0.224 sec/batch), lr: 0.094560
2019-12-27 17:07:09.882014: step 86080/136300 (epoch 64/100), loss = 0.309422 (0.238 sec/batch), lr: 0.094560
2019-12-27 17:07:15.924231: step 86100/136300 (epoch 64/100), loss = 0.377148 (0.243 sec/batch), lr: 0.094560
2019-12-27 17:07:20.639812: step 86120/136300 (epoch 64/100), loss = 0.246684 (0.239 sec/batch), lr: 0.094560
2019-12-27 17:07:25.482222: step 86140/136300 (epoch 64/100), loss = 0.248531 (0.231 sec/batch), lr: 0.094560
2019-12-27 17:07:30.277500: step 86160/136300 (epoch 64/100), loss = 0.139970 (0.234 sec/batch), lr: 0.094560
2019-12-27 17:07:34.975247: step 86180/136300 (epoch 64/100), loss = 0.125647 (0.237 sec/batch), lr: 0.094560
2019-12-27 17:07:39.702527: step 86200/136300 (epoch 64/100), loss = 0.157741 (0.219 sec/batch), lr: 0.094560
2019-12-27 17:07:44.515379: step 86220/136300 (epoch 64/100), loss = 0.270388 (0.231 sec/batch), lr: 0.094560
2019-12-27 17:07:49.331886: step 86240/136300 (epoch 64/100), loss = 0.205141 (0.230 sec/batch), lr: 0.094560
2019-12-27 17:07:53.993725: step 86260/136300 (epoch 64/100), loss = 0.215914 (0.216 sec/batch), lr: 0.094560
2019-12-27 17:08:00.111980: step 86280/136300 (epoch 64/100), loss = 0.237701 (0.215 sec/batch), lr: 0.094560
2019-12-27 17:08:04.873854: step 86300/136300 (epoch 64/100), loss = 0.201644 (0.241 sec/batch), lr: 0.094560
2019-12-27 17:08:09.761243: step 86320/136300 (epoch 64/100), loss = 0.216546 (0.230 sec/batch), lr: 0.094560
2019-12-27 17:08:14.564501: step 86340/136300 (epoch 64/100), loss = 0.453462 (0.193 sec/batch), lr: 0.094560
2019-12-27 17:08:19.337570: step 86360/136300 (epoch 64/100), loss = 0.268326 (0.247 sec/batch), lr: 0.094560
2019-12-27 17:08:24.158035: step 86380/136300 (epoch 64/100), loss = 0.235070 (0.221 sec/batch), lr: 0.094560
2019-12-27 17:08:29.001687: step 86400/136300 (epoch 64/100), loss = 0.346158 (0.238 sec/batch), lr: 0.094560
2019-12-27 17:08:33.820506: step 86420/136300 (epoch 64/100), loss = 0.307303 (0.231 sec/batch), lr: 0.094560
2019-12-27 17:08:38.591897: step 86440/136300 (epoch 64/100), loss = 0.082159 (0.228 sec/batch), lr: 0.094560
2019-12-27 17:08:43.410956: step 86460/136300 (epoch 64/100), loss = 0.146700 (0.248 sec/batch), lr: 0.094560
2019-12-27 17:08:49.271402: step 86480/136300 (epoch 64/100), loss = 0.406721 (0.244 sec/batch), lr: 0.094560
2019-12-27 17:08:54.013543: step 86500/136300 (epoch 64/100), loss = 0.191215 (0.214 sec/batch), lr: 0.094560
2019-12-27 17:08:58.731224: step 86520/136300 (epoch 64/100), loss = 0.134284 (0.234 sec/batch), lr: 0.094560
2019-12-27 17:09:03.511084: step 86540/136300 (epoch 64/100), loss = 0.385065 (0.206 sec/batch), lr: 0.094560
2019-12-27 17:09:08.402340: step 86560/136300 (epoch 64/100), loss = 0.141192 (0.229 sec/batch), lr: 0.094560
2019-12-27 17:09:13.259217: step 86580/136300 (epoch 64/100), loss = 0.183189 (0.233 sec/batch), lr: 0.094560
2019-12-27 17:09:18.123885: step 86600/136300 (epoch 64/100), loss = 0.253242 (0.233 sec/batch), lr: 0.094560
2019-12-27 17:09:22.941274: step 86620/136300 (epoch 64/100), loss = 0.262419 (0.204 sec/batch), lr: 0.094560
2019-12-27 17:09:27.673667: step 86640/136300 (epoch 64/100), loss = 0.248862 (0.252 sec/batch), lr: 0.094560
2019-12-27 17:09:32.535823: step 86660/136300 (epoch 64/100), loss = 0.233418 (0.248 sec/batch), lr: 0.094560
2019-12-27 17:09:38.786863: step 86680/136300 (epoch 64/100), loss = 0.446837 (0.235 sec/batch), lr: 0.094560
2019-12-27 17:09:43.501694: step 86700/136300 (epoch 64/100), loss = 0.260750 (0.177 sec/batch), lr: 0.094560
2019-12-27 17:09:48.460287: step 86720/136300 (epoch 64/100), loss = 0.068500 (0.223 sec/batch), lr: 0.094560
2019-12-27 17:09:53.021885: step 86740/136300 (epoch 64/100), loss = 0.189353 (0.168 sec/batch), lr: 0.094560
2019-12-27 17:09:57.873614: step 86760/136300 (epoch 64/100), loss = 0.189274 (0.199 sec/batch), lr: 0.094560
2019-12-27 17:10:02.783797: step 86780/136300 (epoch 64/100), loss = 0.187499 (0.234 sec/batch), lr: 0.094560
2019-12-27 17:10:07.525216: step 86800/136300 (epoch 64/100), loss = 0.180158 (0.201 sec/batch), lr: 0.094560
2019-12-27 17:10:12.429100: step 86820/136300 (epoch 64/100), loss = 0.260621 (0.235 sec/batch), lr: 0.094560
2019-12-27 17:10:17.306315: step 86840/136300 (epoch 64/100), loss = 0.220024 (0.224 sec/batch), lr: 0.094560
2019-12-27 17:10:22.093373: step 86860/136300 (epoch 64/100), loss = 0.208568 (0.243 sec/batch), lr: 0.094560
2019-12-27 17:10:28.153158: step 86880/136300 (epoch 64/100), loss = 0.241239 (0.222 sec/batch), lr: 0.094560
2019-12-27 17:10:33.005952: step 86900/136300 (epoch 64/100), loss = 0.245610 (0.246 sec/batch), lr: 0.094560
2019-12-27 17:10:37.592892: step 86920/136300 (epoch 64/100), loss = 0.229594 (0.237 sec/batch), lr: 0.094560
2019-12-27 17:10:42.277925: step 86940/136300 (epoch 64/100), loss = 0.215795 (0.239 sec/batch), lr: 0.094560
2019-12-27 17:10:47.063105: step 86960/136300 (epoch 64/100), loss = 0.084848 (0.202 sec/batch), lr: 0.094560
2019-12-27 17:10:51.894217: step 86980/136300 (epoch 64/100), loss = 0.275680 (0.243 sec/batch), lr: 0.094560
2019-12-27 17:10:56.732200: step 87000/136300 (epoch 64/100), loss = 0.352533 (0.224 sec/batch), lr: 0.094560
2019-12-27 17:11:01.432961: step 87020/136300 (epoch 64/100), loss = 0.341316 (0.199 sec/batch), lr: 0.094560
2019-12-27 17:11:06.140744: step 87040/136300 (epoch 64/100), loss = 0.175114 (0.240 sec/batch), lr: 0.094560
2019-12-27 17:11:10.884425: step 87060/136300 (epoch 64/100), loss = 0.304174 (0.243 sec/batch), lr: 0.094560
2019-12-27 17:11:16.990454: step 87080/136300 (epoch 64/100), loss = 0.247775 (0.212 sec/batch), lr: 0.094560
2019-12-27 17:11:21.865939: step 87100/136300 (epoch 64/100), loss = 0.210055 (0.198 sec/batch), lr: 0.094560
2019-12-27 17:11:26.764889: step 87120/136300 (epoch 64/100), loss = 0.211013 (0.233 sec/batch), lr: 0.094560
2019-12-27 17:11:31.331234: step 87140/136300 (epoch 64/100), loss = 0.238778 (0.193 sec/batch), lr: 0.094560
2019-12-27 17:11:36.126059: step 87160/136300 (epoch 64/100), loss = 0.229136 (0.238 sec/batch), lr: 0.094560
2019-12-27 17:11:40.864725: step 87180/136300 (epoch 64/100), loss = 0.257728 (0.234 sec/batch), lr: 0.094560
2019-12-27 17:11:45.769798: step 87200/136300 (epoch 64/100), loss = 0.227326 (0.238 sec/batch), lr: 0.094560
2019-12-27 17:11:50.554755: step 87220/136300 (epoch 64/100), loss = 0.194293 (0.239 sec/batch), lr: 0.094560
Evaluating on dev set...
Precision (micro): 70.928%
   Recall (micro): 62.877%
       F1 (micro): 66.660%
epoch 64: train_loss = 0.238876, dev_loss = 0.448956, dev_f1 = 0.6666
model saved to ./saved_models/01/checkpoint_epoch_64.pt

2019-12-27 17:12:28.752857: step 87240/136300 (epoch 65/100), loss = 0.258923 (0.204 sec/batch), lr: 0.094560
2019-12-27 17:12:34.661784: step 87260/136300 (epoch 65/100), loss = 0.214606 (0.241 sec/batch), lr: 0.094560
2019-12-27 17:12:39.478903: step 87280/136300 (epoch 65/100), loss = 0.258661 (0.245 sec/batch), lr: 0.094560
2019-12-27 17:12:44.133460: step 87300/136300 (epoch 65/100), loss = 0.331899 (0.199 sec/batch), lr: 0.094560
2019-12-27 17:12:49.031210: step 87320/136300 (epoch 65/100), loss = 0.209504 (0.236 sec/batch), lr: 0.094560
2019-12-27 17:12:53.819952: step 87340/136300 (epoch 65/100), loss = 0.204216 (0.232 sec/batch), lr: 0.094560
2019-12-27 17:12:58.518254: step 87360/136300 (epoch 65/100), loss = 0.372493 (0.229 sec/batch), lr: 0.094560
2019-12-27 17:13:03.361089: step 87380/136300 (epoch 65/100), loss = 0.170034 (0.229 sec/batch), lr: 0.094560
2019-12-27 17:13:08.115362: step 87400/136300 (epoch 65/100), loss = 0.117647 (0.209 sec/batch), lr: 0.094560
2019-12-27 17:13:12.968017: step 87420/136300 (epoch 65/100), loss = 0.176433 (0.202 sec/batch), lr: 0.094560
2019-12-27 17:13:17.785756: step 87440/136300 (epoch 65/100), loss = 0.243555 (0.232 sec/batch), lr: 0.094560
2019-12-27 17:13:23.757818: step 87460/136300 (epoch 65/100), loss = 0.327244 (0.237 sec/batch), lr: 0.094560
2019-12-27 17:13:28.582134: step 87480/136300 (epoch 65/100), loss = 0.304582 (0.238 sec/batch), lr: 0.094560
2019-12-27 17:13:33.369474: step 87500/136300 (epoch 65/100), loss = 0.171946 (0.242 sec/batch), lr: 0.094560
2019-12-27 17:13:38.134391: step 87520/136300 (epoch 65/100), loss = 0.294103 (0.238 sec/batch), lr: 0.094560
2019-12-27 17:13:42.918051: step 87540/136300 (epoch 65/100), loss = 0.268334 (0.229 sec/batch), lr: 0.094560
2019-12-27 17:13:47.628422: step 87560/136300 (epoch 65/100), loss = 0.093809 (0.214 sec/batch), lr: 0.094560
2019-12-27 17:13:52.401962: step 87580/136300 (epoch 65/100), loss = 0.084509 (0.206 sec/batch), lr: 0.094560
2019-12-27 17:13:57.239665: step 87600/136300 (epoch 65/100), loss = 0.267849 (0.247 sec/batch), lr: 0.094560
2019-12-27 17:14:01.921946: step 87620/136300 (epoch 65/100), loss = 0.209663 (0.176 sec/batch), lr: 0.094560
2019-12-27 17:14:06.575558: step 87640/136300 (epoch 65/100), loss = 0.258306 (0.249 sec/batch), lr: 0.094560
2019-12-27 17:14:12.537181: step 87660/136300 (epoch 65/100), loss = 0.054499 (0.216 sec/batch), lr: 0.094560
2019-12-27 17:14:17.456165: step 87680/136300 (epoch 65/100), loss = 0.229015 (0.220 sec/batch), lr: 0.094560
2019-12-27 17:14:22.274170: step 87700/136300 (epoch 65/100), loss = 0.195189 (0.173 sec/batch), lr: 0.094560
2019-12-27 17:14:27.055231: step 87720/136300 (epoch 65/100), loss = 0.259318 (0.231 sec/batch), lr: 0.094560
2019-12-27 17:14:31.884743: step 87740/136300 (epoch 65/100), loss = 0.247040 (0.240 sec/batch), lr: 0.094560
2019-12-27 17:14:36.693660: step 87760/136300 (epoch 65/100), loss = 0.401060 (0.217 sec/batch), lr: 0.094560
2019-12-27 17:14:41.586012: step 87780/136300 (epoch 65/100), loss = 0.220173 (0.178 sec/batch), lr: 0.094560
2019-12-27 17:14:46.344243: step 87800/136300 (epoch 65/100), loss = 0.411466 (0.167 sec/batch), lr: 0.094560
2019-12-27 17:14:51.124497: step 87820/136300 (epoch 65/100), loss = 0.302449 (0.227 sec/batch), lr: 0.094560
2019-12-27 17:14:57.296892: step 87840/136300 (epoch 65/100), loss = 0.217844 (0.177 sec/batch), lr: 0.094560
2019-12-27 17:15:02.037810: step 87860/136300 (epoch 65/100), loss = 0.292207 (0.218 sec/batch), lr: 0.094560
2019-12-27 17:15:06.759962: step 87880/136300 (epoch 65/100), loss = 0.190915 (0.204 sec/batch), lr: 0.094560
2019-12-27 17:15:11.554092: step 87900/136300 (epoch 65/100), loss = 0.289644 (0.216 sec/batch), lr: 0.094560
2019-12-27 17:15:16.450762: step 87920/136300 (epoch 65/100), loss = 0.209021 (0.249 sec/batch), lr: 0.094560
2019-12-27 17:15:21.314609: step 87940/136300 (epoch 65/100), loss = 0.253692 (0.229 sec/batch), lr: 0.094560
2019-12-27 17:15:26.194107: step 87960/136300 (epoch 65/100), loss = 0.170313 (0.230 sec/batch), lr: 0.094560
2019-12-27 17:15:31.041084: step 87980/136300 (epoch 65/100), loss = 0.331076 (0.215 sec/batch), lr: 0.094560
2019-12-27 17:15:35.707210: step 88000/136300 (epoch 65/100), loss = 0.178987 (0.170 sec/batch), lr: 0.094560
2019-12-27 17:15:40.594791: step 88020/136300 (epoch 65/100), loss = 0.264023 (0.239 sec/batch), lr: 0.094560
2019-12-27 17:15:46.740439: step 88040/136300 (epoch 65/100), loss = 0.269281 (0.216 sec/batch), lr: 0.094560
2019-12-27 17:15:51.452499: step 88060/136300 (epoch 65/100), loss = 0.323245 (0.232 sec/batch), lr: 0.094560
2019-12-27 17:15:56.350291: step 88080/136300 (epoch 65/100), loss = 0.453934 (0.225 sec/batch), lr: 0.094560
2019-12-27 17:16:01.043523: step 88100/136300 (epoch 65/100), loss = 0.213066 (0.235 sec/batch), lr: 0.094560
2019-12-27 17:16:05.811481: step 88120/136300 (epoch 65/100), loss = 0.330812 (0.238 sec/batch), lr: 0.094560
2019-12-27 17:16:10.686264: step 88140/136300 (epoch 65/100), loss = 0.240105 (0.238 sec/batch), lr: 0.094560
2019-12-27 17:16:15.472216: step 88160/136300 (epoch 65/100), loss = 0.237669 (0.233 sec/batch), lr: 0.094560
2019-12-27 17:16:20.319009: step 88180/136300 (epoch 65/100), loss = 0.359494 (0.238 sec/batch), lr: 0.094560
2019-12-27 17:16:25.254905: step 88200/136300 (epoch 65/100), loss = 0.152486 (0.244 sec/batch), lr: 0.094560
2019-12-27 17:16:30.017358: step 88220/136300 (epoch 65/100), loss = 0.257069 (0.175 sec/batch), lr: 0.094560
2019-12-27 17:16:36.311330: step 88240/136300 (epoch 65/100), loss = 0.283646 (0.213 sec/batch), lr: 0.094560
2019-12-27 17:16:41.212723: step 88260/136300 (epoch 65/100), loss = 0.251726 (0.236 sec/batch), lr: 0.094560
2019-12-27 17:16:45.797809: step 88280/136300 (epoch 65/100), loss = 0.185218 (0.232 sec/batch), lr: 0.094560
2019-12-27 17:16:50.493704: step 88300/136300 (epoch 65/100), loss = 0.359578 (0.179 sec/batch), lr: 0.094560
2019-12-27 17:16:55.304684: step 88320/136300 (epoch 65/100), loss = 0.408650 (0.237 sec/batch), lr: 0.094560
2019-12-27 17:17:00.056272: step 88340/136300 (epoch 65/100), loss = 0.317256 (0.201 sec/batch), lr: 0.094560
2019-12-27 17:17:04.933157: step 88360/136300 (epoch 65/100), loss = 0.522625 (0.238 sec/batch), lr: 0.094560
2019-12-27 17:17:09.662635: step 88380/136300 (epoch 65/100), loss = 0.222957 (0.229 sec/batch), lr: 0.094560
2019-12-27 17:17:14.363482: step 88400/136300 (epoch 65/100), loss = 0.237144 (0.210 sec/batch), lr: 0.094560
2019-12-27 17:17:19.100102: step 88420/136300 (epoch 65/100), loss = 0.406378 (0.232 sec/batch), lr: 0.094560
2019-12-27 17:17:25.373335: step 88440/136300 (epoch 65/100), loss = 0.122341 (0.200 sec/batch), lr: 0.094560
2019-12-27 17:17:30.297960: step 88460/136300 (epoch 65/100), loss = 0.130160 (0.232 sec/batch), lr: 0.094560
2019-12-27 17:17:35.151385: step 88480/136300 (epoch 65/100), loss = 0.174034 (0.238 sec/batch), lr: 0.094560
2019-12-27 17:17:39.821799: step 88500/136300 (epoch 65/100), loss = 0.284992 (0.215 sec/batch), lr: 0.094560
2019-12-27 17:17:44.576194: step 88520/136300 (epoch 65/100), loss = 0.466870 (0.248 sec/batch), lr: 0.094560
2019-12-27 17:17:49.270352: step 88540/136300 (epoch 65/100), loss = 0.141516 (0.216 sec/batch), lr: 0.094560
2019-12-27 17:17:54.188466: step 88560/136300 (epoch 65/100), loss = 0.209383 (0.243 sec/batch), lr: 0.094560
2019-12-27 17:17:58.997493: step 88580/136300 (epoch 65/100), loss = 0.198148 (0.195 sec/batch), lr: 0.094560
Evaluating on dev set...
Precision (micro): 72.064%
   Recall (micro): 62.546%
       F1 (micro): 66.969%
epoch 65: train_loss = 0.238615, dev_loss = 0.444075, dev_f1 = 0.6697
model saved to ./saved_models/01/checkpoint_epoch_65.pt

2019-12-27 17:18:37.311736: step 88600/136300 (epoch 66/100), loss = 0.186767 (0.243 sec/batch), lr: 0.094560
2019-12-27 17:18:43.254898: step 88620/136300 (epoch 66/100), loss = 0.300178 (0.226 sec/batch), lr: 0.094560
2019-12-27 17:18:48.026022: step 88640/136300 (epoch 66/100), loss = 0.096503 (0.235 sec/batch), lr: 0.094560
2019-12-27 17:18:52.740714: step 88660/136300 (epoch 66/100), loss = 0.281675 (0.215 sec/batch), lr: 0.094560
2019-12-27 17:18:57.631436: step 88680/136300 (epoch 66/100), loss = 0.159589 (0.218 sec/batch), lr: 0.094560
2019-12-27 17:19:02.393539: step 88700/136300 (epoch 66/100), loss = 0.159783 (0.240 sec/batch), lr: 0.094560
2019-12-27 17:19:07.121817: step 88720/136300 (epoch 66/100), loss = 0.235281 (0.236 sec/batch), lr: 0.094560
2019-12-27 17:19:11.958093: step 88740/136300 (epoch 66/100), loss = 0.351440 (0.234 sec/batch), lr: 0.094560
2019-12-27 17:19:16.733623: step 88760/136300 (epoch 66/100), loss = 0.178400 (0.210 sec/batch), lr: 0.094560
2019-12-27 17:19:21.598655: step 88780/136300 (epoch 66/100), loss = 0.318671 (0.229 sec/batch), lr: 0.094560
2019-12-27 17:19:26.384527: step 88800/136300 (epoch 66/100), loss = 0.167807 (0.244 sec/batch), lr: 0.094560
2019-12-27 17:19:32.477647: step 88820/136300 (epoch 66/100), loss = 0.145294 (0.238 sec/batch), lr: 0.094560
2019-12-27 17:19:37.281889: step 88840/136300 (epoch 66/100), loss = 0.268684 (0.217 sec/batch), lr: 0.094560
2019-12-27 17:19:42.033892: step 88860/136300 (epoch 66/100), loss = 0.127633 (0.224 sec/batch), lr: 0.094560
2019-12-27 17:19:46.802224: step 88880/136300 (epoch 66/100), loss = 0.507564 (0.218 sec/batch), lr: 0.094560
2019-12-27 17:19:51.620761: step 88900/136300 (epoch 66/100), loss = 0.304170 (0.194 sec/batch), lr: 0.094560
2019-12-27 17:19:56.301861: step 88920/136300 (epoch 66/100), loss = 0.181201 (0.216 sec/batch), lr: 0.094560
2019-12-27 17:20:01.072692: step 88940/136300 (epoch 66/100), loss = 0.155067 (0.217 sec/batch), lr: 0.094560
2019-12-27 17:20:05.958773: step 88960/136300 (epoch 66/100), loss = 0.112788 (0.221 sec/batch), lr: 0.094560
2019-12-27 17:20:10.684496: step 88980/136300 (epoch 66/100), loss = 0.140293 (0.242 sec/batch), lr: 0.094560
2019-12-27 17:20:15.270069: step 89000/136300 (epoch 66/100), loss = 0.316054 (0.213 sec/batch), lr: 0.094560
2019-12-27 17:20:21.365407: step 89020/136300 (epoch 66/100), loss = 0.072560 (0.203 sec/batch), lr: 0.094560
2019-12-27 17:20:26.302352: step 89040/136300 (epoch 66/100), loss = 0.518167 (0.232 sec/batch), lr: 0.094560
2019-12-27 17:20:31.175072: step 89060/136300 (epoch 66/100), loss = 0.117744 (0.236 sec/batch), lr: 0.094560
2019-12-27 17:20:35.960716: step 89080/136300 (epoch 66/100), loss = 0.246333 (0.243 sec/batch), lr: 0.094560
2019-12-27 17:20:40.777982: step 89100/136300 (epoch 66/100), loss = 0.274506 (0.243 sec/batch), lr: 0.094560
2019-12-27 17:20:45.555019: step 89120/136300 (epoch 66/100), loss = 0.359101 (0.198 sec/batch), lr: 0.094560
2019-12-27 17:20:50.494146: step 89140/136300 (epoch 66/100), loss = 0.243165 (0.243 sec/batch), lr: 0.094560
2019-12-27 17:20:55.220230: step 89160/136300 (epoch 66/100), loss = 0.143073 (0.244 sec/batch), lr: 0.094560
2019-12-27 17:20:59.970859: step 89180/136300 (epoch 66/100), loss = 0.316758 (0.208 sec/batch), lr: 0.094560
2019-12-27 17:21:04.735157: step 89200/136300 (epoch 66/100), loss = 0.412841 (0.244 sec/batch), lr: 0.094560
2019-12-27 17:21:10.776959: step 89220/136300 (epoch 66/100), loss = 0.226772 (0.234 sec/batch), lr: 0.094560
2019-12-27 17:21:15.482069: step 89240/136300 (epoch 66/100), loss = 0.101972 (0.218 sec/batch), lr: 0.094560
2019-12-27 17:21:20.238503: step 89260/136300 (epoch 66/100), loss = 0.221075 (0.227 sec/batch), lr: 0.094560
2019-12-27 17:21:25.097896: step 89280/136300 (epoch 66/100), loss = 0.554283 (0.217 sec/batch), lr: 0.094560
2019-12-27 17:21:30.004483: step 89300/136300 (epoch 66/100), loss = 0.209343 (0.244 sec/batch), lr: 0.094560
2019-12-27 17:21:34.861211: step 89320/136300 (epoch 66/100), loss = 0.303461 (0.221 sec/batch), lr: 0.094560
2019-12-27 17:21:39.752513: step 89340/136300 (epoch 66/100), loss = 0.294592 (0.247 sec/batch), lr: 0.094560
2019-12-27 17:21:44.480086: step 89360/136300 (epoch 66/100), loss = 0.274407 (0.225 sec/batch), lr: 0.094560
2019-12-27 17:21:49.243279: step 89380/136300 (epoch 66/100), loss = 0.265729 (0.180 sec/batch), lr: 0.094560
2019-12-27 17:21:55.443490: step 89400/136300 (epoch 66/100), loss = 0.238072 (0.219 sec/batch), lr: 0.094560
2019-12-27 17:22:00.181095: step 89420/136300 (epoch 66/100), loss = 0.159955 (0.214 sec/batch), lr: 0.094560
2019-12-27 17:22:05.076253: step 89440/136300 (epoch 66/100), loss = 0.311882 (0.246 sec/batch), lr: 0.094560
2019-12-27 17:22:09.784978: step 89460/136300 (epoch 66/100), loss = 0.208077 (0.205 sec/batch), lr: 0.094560
2019-12-27 17:22:14.505090: step 89480/136300 (epoch 66/100), loss = 0.250987 (0.229 sec/batch), lr: 0.094560
2019-12-27 17:22:19.364488: step 89500/136300 (epoch 66/100), loss = 0.179234 (0.241 sec/batch), lr: 0.094560
2019-12-27 17:22:24.262902: step 89520/136300 (epoch 66/100), loss = 0.117284 (0.217 sec/batch), lr: 0.094560
2019-12-27 17:22:29.025068: step 89540/136300 (epoch 66/100), loss = 0.409136 (0.233 sec/batch), lr: 0.094560
2019-12-27 17:22:33.955434: step 89560/136300 (epoch 66/100), loss = 0.240194 (0.249 sec/batch), lr: 0.094560
2019-12-27 17:22:38.781116: step 89580/136300 (epoch 66/100), loss = 0.337437 (0.217 sec/batch), lr: 0.094560
2019-12-27 17:22:44.884074: step 89600/136300 (epoch 66/100), loss = 0.230154 (0.238 sec/batch), lr: 0.094560
2019-12-27 17:22:49.736472: step 89620/136300 (epoch 66/100), loss = 0.136184 (0.232 sec/batch), lr: 0.094560
2019-12-27 17:22:54.337285: step 89640/136300 (epoch 66/100), loss = 0.331584 (0.228 sec/batch), lr: 0.094560
2019-12-27 17:22:59.053653: step 89660/136300 (epoch 66/100), loss = 0.479718 (0.216 sec/batch), lr: 0.094560
2019-12-27 17:23:03.860419: step 89680/136300 (epoch 66/100), loss = 0.230395 (0.204 sec/batch), lr: 0.094560
2019-12-27 17:23:08.609240: step 89700/136300 (epoch 66/100), loss = 0.269697 (0.234 sec/batch), lr: 0.094560
2019-12-27 17:23:13.484080: step 89720/136300 (epoch 66/100), loss = 0.326853 (0.236 sec/batch), lr: 0.094560
2019-12-27 17:23:18.203381: step 89740/136300 (epoch 66/100), loss = 0.122440 (0.223 sec/batch), lr: 0.094560
2019-12-27 17:23:22.905109: step 89760/136300 (epoch 66/100), loss = 0.305669 (0.217 sec/batch), lr: 0.094560
2019-12-27 17:23:27.648211: step 89780/136300 (epoch 66/100), loss = 0.373421 (0.234 sec/batch), lr: 0.094560
2019-12-27 17:23:33.602038: step 89800/136300 (epoch 66/100), loss = 0.169976 (0.214 sec/batch), lr: 0.094560
2019-12-27 17:23:38.507962: step 89820/136300 (epoch 66/100), loss = 0.454145 (0.247 sec/batch), lr: 0.094560
2019-12-27 17:23:43.293163: step 89840/136300 (epoch 66/100), loss = 0.297356 (0.230 sec/batch), lr: 0.094560
2019-12-27 17:23:48.092885: step 89860/136300 (epoch 66/100), loss = 0.341489 (0.241 sec/batch), lr: 0.094560
2019-12-27 17:23:52.779430: step 89880/136300 (epoch 66/100), loss = 0.174061 (0.231 sec/batch), lr: 0.094560
2019-12-27 17:23:57.502270: step 89900/136300 (epoch 66/100), loss = 0.105952 (0.235 sec/batch), lr: 0.094560
2019-12-27 17:24:02.360486: step 89920/136300 (epoch 66/100), loss = 0.222079 (0.235 sec/batch), lr: 0.094560
2019-12-27 17:24:07.226563: step 89940/136300 (epoch 66/100), loss = 0.253204 (0.230 sec/batch), lr: 0.094560
Evaluating on dev set...
Precision (micro): 71.870%
   Recall (micro): 62.509%
       F1 (micro): 66.863%
epoch 66: train_loss = 0.238036, dev_loss = 0.454440, dev_f1 = 0.6686
model saved to ./saved_models/01/checkpoint_epoch_66.pt

2019-12-27 17:24:45.489433: step 89960/136300 (epoch 67/100), loss = 0.255058 (0.228 sec/batch), lr: 0.085104
2019-12-27 17:24:51.541291: step 89980/136300 (epoch 67/100), loss = 0.147083 (0.213 sec/batch), lr: 0.085104
2019-12-27 17:24:56.281005: step 90000/136300 (epoch 67/100), loss = 0.366499 (0.217 sec/batch), lr: 0.085104
2019-12-27 17:25:01.080511: step 90020/136300 (epoch 67/100), loss = 0.160933 (0.216 sec/batch), lr: 0.085104
2019-12-27 17:25:05.909626: step 90040/136300 (epoch 67/100), loss = 0.246575 (0.202 sec/batch), lr: 0.085104
2019-12-27 17:25:10.685969: step 90060/136300 (epoch 67/100), loss = 0.263557 (0.224 sec/batch), lr: 0.085104
2019-12-27 17:25:15.432169: step 90080/136300 (epoch 67/100), loss = 0.296971 (0.242 sec/batch), lr: 0.085104
2019-12-27 17:25:20.263704: step 90100/136300 (epoch 67/100), loss = 0.365773 (0.235 sec/batch), lr: 0.085104
2019-12-27 17:25:25.054340: step 90120/136300 (epoch 67/100), loss = 0.152636 (0.222 sec/batch), lr: 0.085104
2019-12-27 17:25:29.920916: step 90140/136300 (epoch 67/100), loss = 0.209551 (0.216 sec/batch), lr: 0.085104
2019-12-27 17:25:34.731232: step 90160/136300 (epoch 67/100), loss = 0.196752 (0.228 sec/batch), lr: 0.085104
2019-12-27 17:25:40.895937: step 90180/136300 (epoch 67/100), loss = 0.167804 (0.236 sec/batch), lr: 0.085104
2019-12-27 17:25:45.630810: step 90200/136300 (epoch 67/100), loss = 0.212496 (0.239 sec/batch), lr: 0.085104
2019-12-27 17:25:50.401855: step 90220/136300 (epoch 67/100), loss = 0.165328 (0.214 sec/batch), lr: 0.085104
2019-12-27 17:25:55.168732: step 90240/136300 (epoch 67/100), loss = 0.222875 (0.215 sec/batch), lr: 0.085104
2019-12-27 17:25:59.995348: step 90260/136300 (epoch 67/100), loss = 0.143119 (0.241 sec/batch), lr: 0.085104
2019-12-27 17:26:04.739593: step 90280/136300 (epoch 67/100), loss = 0.204383 (0.243 sec/batch), lr: 0.085104
2019-12-27 17:26:09.443713: step 90300/136300 (epoch 67/100), loss = 0.149572 (0.229 sec/batch), lr: 0.085104
2019-12-27 17:26:14.309545: step 90320/136300 (epoch 67/100), loss = 0.209546 (0.238 sec/batch), lr: 0.085104
2019-12-27 17:26:19.006847: step 90340/136300 (epoch 67/100), loss = 0.154985 (0.216 sec/batch), lr: 0.085104
2019-12-27 17:26:23.699964: step 90360/136300 (epoch 67/100), loss = 0.186677 (0.226 sec/batch), lr: 0.085104
2019-12-27 17:26:29.685882: step 90380/136300 (epoch 67/100), loss = 0.150122 (0.214 sec/batch), lr: 0.085104
2019-12-27 17:26:34.537691: step 90400/136300 (epoch 67/100), loss = 0.175120 (0.238 sec/batch), lr: 0.085104
2019-12-27 17:26:39.406248: step 90420/136300 (epoch 67/100), loss = 0.156142 (0.231 sec/batch), lr: 0.085104
2019-12-27 17:26:44.168504: step 90440/136300 (epoch 67/100), loss = 0.158612 (0.214 sec/batch), lr: 0.085104
2019-12-27 17:26:48.965504: step 90460/136300 (epoch 67/100), loss = 0.324574 (0.224 sec/batch), lr: 0.085104
2019-12-27 17:26:53.801863: step 90480/136300 (epoch 67/100), loss = 0.205887 (0.246 sec/batch), lr: 0.085104
2019-12-27 17:26:58.752649: step 90500/136300 (epoch 67/100), loss = 0.261236 (0.236 sec/batch), lr: 0.085104
2019-12-27 17:27:03.481779: step 90520/136300 (epoch 67/100), loss = 0.194359 (0.241 sec/batch), lr: 0.085104
2019-12-27 17:27:08.264690: step 90540/136300 (epoch 67/100), loss = 0.219749 (0.218 sec/batch), lr: 0.085104
2019-12-27 17:27:12.932867: step 90560/136300 (epoch 67/100), loss = 0.197492 (0.213 sec/batch), lr: 0.085104
2019-12-27 17:27:19.096541: step 90580/136300 (epoch 67/100), loss = 0.181899 (0.222 sec/batch), lr: 0.085104
2019-12-27 17:27:23.857782: step 90600/136300 (epoch 67/100), loss = 0.311488 (0.218 sec/batch), lr: 0.085104
2019-12-27 17:27:28.593708: step 90620/136300 (epoch 67/100), loss = 0.032162 (0.223 sec/batch), lr: 0.085104
2019-12-27 17:27:33.408839: step 90640/136300 (epoch 67/100), loss = 0.447656 (0.204 sec/batch), lr: 0.085104
2019-12-27 17:27:38.289472: step 90660/136300 (epoch 67/100), loss = 0.266633 (0.207 sec/batch), lr: 0.085104
2019-12-27 17:27:43.186375: step 90680/136300 (epoch 67/100), loss = 0.270878 (0.247 sec/batch), lr: 0.085104
2019-12-27 17:27:48.029297: step 90700/136300 (epoch 67/100), loss = 0.232540 (0.248 sec/batch), lr: 0.085104
2019-12-27 17:27:52.753091: step 90720/136300 (epoch 67/100), loss = 0.239688 (0.212 sec/batch), lr: 0.085104
2019-12-27 17:27:57.542428: step 90740/136300 (epoch 67/100), loss = 0.136115 (0.244 sec/batch), lr: 0.085104
2019-12-27 17:28:03.865562: step 90760/136300 (epoch 67/100), loss = 0.231031 (0.218 sec/batch), lr: 0.085104
2019-12-27 17:28:08.601751: step 90780/136300 (epoch 67/100), loss = 0.430253 (0.200 sec/batch), lr: 0.085104
2019-12-27 17:28:13.486270: step 90800/136300 (epoch 67/100), loss = 0.108689 (0.251 sec/batch), lr: 0.085104
2019-12-27 17:28:18.293041: step 90820/136300 (epoch 67/100), loss = 0.198725 (0.215 sec/batch), lr: 0.085104
2019-12-27 17:28:22.969572: step 90840/136300 (epoch 67/100), loss = 0.327677 (0.209 sec/batch), lr: 0.085104
2019-12-27 17:28:27.801422: step 90860/136300 (epoch 67/100), loss = 0.153862 (0.229 sec/batch), lr: 0.085104
2019-12-27 17:28:32.698748: step 90880/136300 (epoch 67/100), loss = 0.128732 (0.221 sec/batch), lr: 0.085104
2019-12-27 17:28:37.473735: step 90900/136300 (epoch 67/100), loss = 0.130408 (0.248 sec/batch), lr: 0.085104
2019-12-27 17:28:42.411274: step 90920/136300 (epoch 67/100), loss = 0.177806 (0.246 sec/batch), lr: 0.085104
2019-12-27 17:28:47.249133: step 90940/136300 (epoch 67/100), loss = 0.386695 (0.240 sec/batch), lr: 0.085104
2019-12-27 17:28:53.443109: step 90960/136300 (epoch 67/100), loss = 0.294680 (1.648 sec/batch), lr: 0.085104
2019-12-27 17:28:58.237651: step 90980/136300 (epoch 67/100), loss = 0.128261 (0.221 sec/batch), lr: 0.085104
2019-12-27 17:29:02.874310: step 91000/136300 (epoch 67/100), loss = 0.303037 (0.197 sec/batch), lr: 0.085104
2019-12-27 17:29:07.587772: step 91020/136300 (epoch 67/100), loss = 0.292969 (0.232 sec/batch), lr: 0.085104
2019-12-27 17:29:12.403043: step 91040/136300 (epoch 67/100), loss = 0.123439 (0.228 sec/batch), lr: 0.085104
2019-12-27 17:29:17.140894: step 91060/136300 (epoch 67/100), loss = 0.111016 (0.232 sec/batch), lr: 0.085104
2019-12-27 17:29:22.021963: step 91080/136300 (epoch 67/100), loss = 0.422554 (0.243 sec/batch), lr: 0.085104
2019-12-27 17:29:26.845805: step 91100/136300 (epoch 67/100), loss = 0.190558 (0.235 sec/batch), lr: 0.085104
2019-12-27 17:29:31.565320: step 91120/136300 (epoch 67/100), loss = 0.230990 (0.246 sec/batch), lr: 0.085104
2019-12-27 17:29:36.284555: step 91140/136300 (epoch 67/100), loss = 0.191839 (0.244 sec/batch), lr: 0.085104
2019-12-27 17:29:42.399109: step 91160/136300 (epoch 67/100), loss = 0.201065 (0.238 sec/batch), lr: 0.085104
2019-12-27 17:29:47.261038: step 91180/136300 (epoch 67/100), loss = 0.334595 (0.236 sec/batch), lr: 0.085104
2019-12-27 17:29:52.146654: step 91200/136300 (epoch 67/100), loss = 0.136120 (0.238 sec/batch), lr: 0.085104
2019-12-27 17:29:56.923764: step 91220/136300 (epoch 67/100), loss = 0.216038 (0.240 sec/batch), lr: 0.085104
2019-12-27 17:30:01.599854: step 91240/136300 (epoch 67/100), loss = 0.177890 (0.228 sec/batch), lr: 0.085104
2019-12-27 17:30:06.337543: step 91260/136300 (epoch 67/100), loss = 0.165954 (0.210 sec/batch), lr: 0.085104
2019-12-27 17:30:11.141971: step 91280/136300 (epoch 67/100), loss = 0.235073 (0.242 sec/batch), lr: 0.085104
2019-12-27 17:30:16.022578: step 91300/136300 (epoch 67/100), loss = 0.178046 (0.242 sec/batch), lr: 0.085104
2019-12-27 17:30:20.881459: step 91320/136300 (epoch 67/100), loss = 0.253156 (0.201 sec/batch), lr: 0.085104
Evaluating on dev set...
Precision (micro): 70.903%
   Recall (micro): 62.399%
       F1 (micro): 66.380%
epoch 67: train_loss = 0.234637, dev_loss = 0.457229, dev_f1 = 0.6638
model saved to ./saved_models/01/checkpoint_epoch_67.pt

2019-12-27 17:31:00.300601: step 91340/136300 (epoch 68/100), loss = 0.141102 (0.228 sec/batch), lr: 0.076593
2019-12-27 17:31:04.975170: step 91360/136300 (epoch 68/100), loss = 0.183110 (0.214 sec/batch), lr: 0.076593
2019-12-27 17:31:09.785831: step 91380/136300 (epoch 68/100), loss = 0.328455 (0.215 sec/batch), lr: 0.076593
2019-12-27 17:31:14.620886: step 91400/136300 (epoch 68/100), loss = 0.172276 (0.237 sec/batch), lr: 0.076593
2019-12-27 17:31:19.396455: step 91420/136300 (epoch 68/100), loss = 0.099859 (0.238 sec/batch), lr: 0.076593
2019-12-27 17:31:24.128550: step 91440/136300 (epoch 68/100), loss = 0.214165 (0.223 sec/batch), lr: 0.076593
2019-12-27 17:31:28.883637: step 91460/136300 (epoch 68/100), loss = 0.366103 (0.216 sec/batch), lr: 0.076593
2019-12-27 17:31:33.669988: step 91480/136300 (epoch 68/100), loss = 0.172390 (0.231 sec/batch), lr: 0.076593
2019-12-27 17:31:38.542935: step 91500/136300 (epoch 68/100), loss = 0.088094 (0.239 sec/batch), lr: 0.076593
2019-12-27 17:31:43.365668: step 91520/136300 (epoch 68/100), loss = 0.422679 (0.221 sec/batch), lr: 0.076593
2019-12-27 17:31:49.341255: step 91540/136300 (epoch 68/100), loss = 0.328140 (0.226 sec/batch), lr: 0.076593
2019-12-27 17:31:54.057022: step 91560/136300 (epoch 68/100), loss = 0.224388 (0.242 sec/batch), lr: 0.076593
2019-12-27 17:31:58.820914: step 91580/136300 (epoch 68/100), loss = 0.228048 (0.237 sec/batch), lr: 0.076593
2019-12-27 17:32:03.645313: step 91600/136300 (epoch 68/100), loss = 0.146880 (0.232 sec/batch), lr: 0.076593
2019-12-27 17:32:08.400346: step 91620/136300 (epoch 68/100), loss = 0.355692 (0.212 sec/batch), lr: 0.076593
2019-12-27 17:32:13.140053: step 91640/136300 (epoch 68/100), loss = 0.255926 (0.240 sec/batch), lr: 0.076593
2019-12-27 17:32:17.916478: step 91660/136300 (epoch 68/100), loss = 0.343518 (0.198 sec/batch), lr: 0.076593
2019-12-27 17:32:22.753884: step 91680/136300 (epoch 68/100), loss = 0.236437 (0.241 sec/batch), lr: 0.076593
2019-12-27 17:32:27.471970: step 91700/136300 (epoch 68/100), loss = 0.220901 (0.244 sec/batch), lr: 0.076593
2019-12-27 17:32:32.198661: step 91720/136300 (epoch 68/100), loss = 0.242859 (0.233 sec/batch), lr: 0.076593
2019-12-27 17:32:38.385930: step 91740/136300 (epoch 68/100), loss = 0.179086 (0.240 sec/batch), lr: 0.076593
2019-12-27 17:32:43.199378: step 91760/136300 (epoch 68/100), loss = 0.286858 (0.235 sec/batch), lr: 0.076593
2019-12-27 17:32:48.027270: step 91780/136300 (epoch 68/100), loss = 0.110700 (0.205 sec/batch), lr: 0.076593
2019-12-27 17:32:52.841043: step 91800/136300 (epoch 68/100), loss = 0.277889 (0.237 sec/batch), lr: 0.076593
2019-12-27 17:32:57.607239: step 91820/136300 (epoch 68/100), loss = 0.131307 (0.227 sec/batch), lr: 0.076593
2019-12-27 17:33:02.461363: step 91840/136300 (epoch 68/100), loss = 0.203707 (0.230 sec/batch), lr: 0.076593
2019-12-27 17:33:07.348915: step 91860/136300 (epoch 68/100), loss = 0.301650 (0.211 sec/batch), lr: 0.076593
2019-12-27 17:33:12.133174: step 91880/136300 (epoch 68/100), loss = 0.205478 (0.241 sec/batch), lr: 0.076593
2019-12-27 17:33:16.927517: step 91900/136300 (epoch 68/100), loss = 0.263602 (0.236 sec/batch), lr: 0.076593
2019-12-27 17:33:21.601149: step 91920/136300 (epoch 68/100), loss = 0.296400 (0.223 sec/batch), lr: 0.076593
2019-12-27 17:33:27.578230: step 91940/136300 (epoch 68/100), loss = 0.283817 (0.236 sec/batch), lr: 0.076593
2019-12-27 17:33:32.383370: step 91960/136300 (epoch 68/100), loss = 0.347469 (0.226 sec/batch), lr: 0.076593
2019-12-27 17:33:37.093625: step 91980/136300 (epoch 68/100), loss = 0.247317 (0.224 sec/batch), lr: 0.076593
2019-12-27 17:33:41.909600: step 92000/136300 (epoch 68/100), loss = 0.185972 (0.240 sec/batch), lr: 0.076593
2019-12-27 17:33:46.780468: step 92020/136300 (epoch 68/100), loss = 0.160699 (0.228 sec/batch), lr: 0.076593
2019-12-27 17:33:51.648796: step 92040/136300 (epoch 68/100), loss = 0.158678 (0.238 sec/batch), lr: 0.076593
2019-12-27 17:33:56.543966: step 92060/136300 (epoch 68/100), loss = 0.397526 (0.248 sec/batch), lr: 0.076593
2019-12-27 17:34:01.258296: step 92080/136300 (epoch 68/100), loss = 0.175145 (0.211 sec/batch), lr: 0.076593
2019-12-27 17:34:06.021103: step 92100/136300 (epoch 68/100), loss = 0.214229 (0.173 sec/batch), lr: 0.076593
2019-12-27 17:34:12.434103: step 92120/136300 (epoch 68/100), loss = 0.262433 (0.242 sec/batch), lr: 0.076593
2019-12-27 17:34:17.161998: step 92140/136300 (epoch 68/100), loss = 0.271533 (0.218 sec/batch), lr: 0.076593
2019-12-27 17:34:21.998815: step 92160/136300 (epoch 68/100), loss = 0.277045 (0.236 sec/batch), lr: 0.076593
2019-12-27 17:34:26.858449: step 92180/136300 (epoch 68/100), loss = 0.419252 (0.209 sec/batch), lr: 0.076593
2019-12-27 17:34:31.540975: step 92200/136300 (epoch 68/100), loss = 0.192088 (0.236 sec/batch), lr: 0.076593
2019-12-27 17:34:36.367772: step 92220/136300 (epoch 68/100), loss = 0.231157 (0.209 sec/batch), lr: 0.076593
2019-12-27 17:34:41.247697: step 92240/136300 (epoch 68/100), loss = 0.186792 (0.217 sec/batch), lr: 0.076593
2019-12-27 17:34:46.043196: step 92260/136300 (epoch 68/100), loss = 0.300341 (0.243 sec/batch), lr: 0.076593
2019-12-27 17:34:50.971477: step 92280/136300 (epoch 68/100), loss = 0.384372 (0.230 sec/batch), lr: 0.076593
2019-12-27 17:34:55.825743: step 92300/136300 (epoch 68/100), loss = 0.099856 (0.243 sec/batch), lr: 0.076593
2019-12-27 17:35:00.573595: step 92320/136300 (epoch 68/100), loss = 0.290222 (0.235 sec/batch), lr: 0.076593
2019-12-27 17:35:06.588083: step 92340/136300 (epoch 68/100), loss = 0.076231 (0.223 sec/batch), lr: 0.076593
2019-12-27 17:35:11.309818: step 92360/136300 (epoch 68/100), loss = 0.332295 (0.196 sec/batch), lr: 0.076593
2019-12-27 17:35:15.986248: step 92380/136300 (epoch 68/100), loss = 0.278055 (0.215 sec/batch), lr: 0.076593
2019-12-27 17:35:20.753037: step 92400/136300 (epoch 68/100), loss = 0.226773 (0.236 sec/batch), lr: 0.076593
2019-12-27 17:35:25.493558: step 92420/136300 (epoch 68/100), loss = 0.345952 (0.236 sec/batch), lr: 0.076593
2019-12-27 17:35:30.394411: step 92440/136300 (epoch 68/100), loss = 0.191586 (0.239 sec/batch), lr: 0.076593
2019-12-27 17:35:35.205581: step 92460/136300 (epoch 68/100), loss = 0.140574 (0.237 sec/batch), lr: 0.076593
2019-12-27 17:35:39.939289: step 92480/136300 (epoch 68/100), loss = 0.251778 (0.233 sec/batch), lr: 0.076593
2019-12-27 17:35:44.633697: step 92500/136300 (epoch 68/100), loss = 0.174452 (0.234 sec/batch), lr: 0.076593
2019-12-27 17:35:49.439697: step 92520/136300 (epoch 68/100), loss = 0.165601 (0.229 sec/batch), lr: 0.076593
2019-12-27 17:35:55.786202: step 92540/136300 (epoch 68/100), loss = 0.200585 (0.236 sec/batch), lr: 0.076593
2019-12-27 17:36:00.664883: step 92560/136300 (epoch 68/100), loss = 0.194994 (0.242 sec/batch), lr: 0.076593
2019-12-27 17:36:05.469688: step 92580/136300 (epoch 68/100), loss = 0.325925 (0.218 sec/batch), lr: 0.076593
2019-12-27 17:36:10.131703: step 92600/136300 (epoch 68/100), loss = 0.205447 (0.211 sec/batch), lr: 0.076593
2019-12-27 17:36:14.946736: step 92620/136300 (epoch 68/100), loss = 0.123460 (0.235 sec/batch), lr: 0.076593
2019-12-27 17:36:19.677181: step 92640/136300 (epoch 68/100), loss = 0.251515 (0.238 sec/batch), lr: 0.076593
2019-12-27 17:36:24.533713: step 92660/136300 (epoch 68/100), loss = 0.181190 (0.198 sec/batch), lr: 0.076593
2019-12-27 17:36:29.453908: step 92680/136300 (epoch 68/100), loss = 0.193780 (0.239 sec/batch), lr: 0.076593
Evaluating on dev set...
Precision (micro): 70.929%
   Recall (micro): 63.061%
       F1 (micro): 66.764%
epoch 68: train_loss = 0.231693, dev_loss = 0.456480, dev_f1 = 0.6676
model saved to ./saved_models/01/checkpoint_epoch_68.pt

2019-12-27 17:37:07.663162: step 92700/136300 (epoch 69/100), loss = 0.299100 (0.224 sec/batch), lr: 0.076593
2019-12-27 17:37:13.538069: step 92720/136300 (epoch 69/100), loss = 0.241548 (0.204 sec/batch), lr: 0.076593
2019-12-27 17:37:18.375030: step 92740/136300 (epoch 69/100), loss = 0.063613 (0.218 sec/batch), lr: 0.076593
2019-12-27 17:37:23.156060: step 92760/136300 (epoch 69/100), loss = 0.300348 (0.243 sec/batch), lr: 0.076593
2019-12-27 17:37:27.981821: step 92780/136300 (epoch 69/100), loss = 0.151987 (0.202 sec/batch), lr: 0.076593
2019-12-27 17:37:32.699716: step 92800/136300 (epoch 69/100), loss = 0.174290 (0.214 sec/batch), lr: 0.076593
2019-12-27 17:37:37.487749: step 92820/136300 (epoch 69/100), loss = 0.342687 (0.233 sec/batch), lr: 0.076593
2019-12-27 17:37:42.273353: step 92840/136300 (epoch 69/100), loss = 0.176913 (0.224 sec/batch), lr: 0.076593
2019-12-27 17:37:47.124966: step 92860/136300 (epoch 69/100), loss = 0.218398 (0.236 sec/batch), lr: 0.076593
2019-12-27 17:37:51.968769: step 92880/136300 (epoch 69/100), loss = 0.147526 (0.246 sec/batch), lr: 0.076593
2019-12-27 17:37:57.955624: step 92900/136300 (epoch 69/100), loss = 0.133401 (0.225 sec/batch), lr: 0.076593
2019-12-27 17:38:02.656621: step 92920/136300 (epoch 69/100), loss = 0.304828 (0.225 sec/batch), lr: 0.076593
2019-12-27 17:38:07.431852: step 92940/136300 (epoch 69/100), loss = 0.237704 (0.199 sec/batch), lr: 0.076593
2019-12-27 17:38:12.270829: step 92960/136300 (epoch 69/100), loss = 0.229957 (0.204 sec/batch), lr: 0.076593
2019-12-27 17:38:17.105724: step 92980/136300 (epoch 69/100), loss = 0.192285 (0.233 sec/batch), lr: 0.076593
2019-12-27 17:38:21.786431: step 93000/136300 (epoch 69/100), loss = 0.236512 (0.216 sec/batch), lr: 0.076593
2019-12-27 17:38:26.596765: step 93020/136300 (epoch 69/100), loss = 0.209385 (0.202 sec/batch), lr: 0.076593
2019-12-27 17:38:31.423014: step 93040/136300 (epoch 69/100), loss = 0.272392 (0.230 sec/batch), lr: 0.076593
2019-12-27 17:38:36.137765: step 93060/136300 (epoch 69/100), loss = 0.126147 (0.234 sec/batch), lr: 0.076593
2019-12-27 17:38:40.896449: step 93080/136300 (epoch 69/100), loss = 0.269885 (0.234 sec/batch), lr: 0.076593
2019-12-27 17:38:46.995744: step 93100/136300 (epoch 69/100), loss = 0.162820 (0.227 sec/batch), lr: 0.076593
2019-12-27 17:38:51.801235: step 93120/136300 (epoch 69/100), loss = 0.301305 (0.247 sec/batch), lr: 0.076593
2019-12-27 17:38:56.644784: step 93140/136300 (epoch 69/100), loss = 0.150903 (0.204 sec/batch), lr: 0.076593
2019-12-27 17:39:01.445735: step 93160/136300 (epoch 69/100), loss = 0.197726 (0.232 sec/batch), lr: 0.076593
2019-12-27 17:39:06.253470: step 93180/136300 (epoch 69/100), loss = 0.142448 (0.215 sec/batch), lr: 0.076593
2019-12-27 17:39:11.093600: step 93200/136300 (epoch 69/100), loss = 0.189723 (0.203 sec/batch), lr: 0.076593
2019-12-27 17:39:15.933670: step 93220/136300 (epoch 69/100), loss = 0.245100 (0.242 sec/batch), lr: 0.076593
2019-12-27 17:39:20.669872: step 93240/136300 (epoch 69/100), loss = 0.515839 (0.231 sec/batch), lr: 0.076593
2019-12-27 17:39:25.420450: step 93260/136300 (epoch 69/100), loss = 0.259446 (0.205 sec/batch), lr: 0.076593
2019-12-27 17:39:30.128507: step 93280/136300 (epoch 69/100), loss = 0.204948 (0.205 sec/batch), lr: 0.076593
2019-12-27 17:39:36.034698: step 93300/136300 (epoch 69/100), loss = 0.160543 (0.226 sec/batch), lr: 0.076593
2019-12-27 17:39:40.821153: step 93320/136300 (epoch 69/100), loss = 0.108817 (0.214 sec/batch), lr: 0.076593
2019-12-27 17:39:45.540990: step 93340/136300 (epoch 69/100), loss = 0.215176 (0.210 sec/batch), lr: 0.076593
2019-12-27 17:39:50.340860: step 93360/136300 (epoch 69/100), loss = 0.237505 (0.211 sec/batch), lr: 0.076593
2019-12-27 17:39:55.287872: step 93380/136300 (epoch 69/100), loss = 0.248954 (0.237 sec/batch), lr: 0.076593
2019-12-27 17:40:00.140770: step 93400/136300 (epoch 69/100), loss = 0.263088 (0.243 sec/batch), lr: 0.076593
2019-12-27 17:40:05.023771: step 93420/136300 (epoch 69/100), loss = 0.197384 (0.204 sec/batch), lr: 0.076593
2019-12-27 17:40:09.800673: step 93440/136300 (epoch 69/100), loss = 0.266268 (0.182 sec/batch), lr: 0.076593
2019-12-27 17:40:14.616986: step 93460/136300 (epoch 69/100), loss = 0.178610 (0.227 sec/batch), lr: 0.076593
2019-12-27 17:40:19.505981: step 93480/136300 (epoch 69/100), loss = 0.227067 (0.228 sec/batch), lr: 0.076593
2019-12-27 17:40:25.722001: step 93500/136300 (epoch 69/100), loss = 0.291187 (0.242 sec/batch), lr: 0.076593
2019-12-27 17:40:30.463532: step 93520/136300 (epoch 69/100), loss = 0.219655 (0.217 sec/batch), lr: 0.076593
2019-12-27 17:40:35.396513: step 93540/136300 (epoch 69/100), loss = 0.272624 (0.244 sec/batch), lr: 0.076593
2019-12-27 17:40:40.061406: step 93560/136300 (epoch 69/100), loss = 0.234911 (0.240 sec/batch), lr: 0.076593
2019-12-27 17:40:44.893829: step 93580/136300 (epoch 69/100), loss = 0.158557 (0.230 sec/batch), lr: 0.076593
2019-12-27 17:40:49.771851: step 93600/136300 (epoch 69/100), loss = 0.197884 (0.202 sec/batch), lr: 0.076593
2019-12-27 17:40:54.530975: step 93620/136300 (epoch 69/100), loss = 0.178825 (0.233 sec/batch), lr: 0.076593
2019-12-27 17:40:59.456708: step 93640/136300 (epoch 69/100), loss = 0.178120 (0.222 sec/batch), lr: 0.076593
2019-12-27 17:41:04.309574: step 93660/136300 (epoch 69/100), loss = 0.175333 (0.238 sec/batch), lr: 0.076593
2019-12-27 17:41:09.095218: step 93680/136300 (epoch 69/100), loss = 0.274707 (0.228 sec/batch), lr: 0.076593
2019-12-27 17:41:15.241690: step 93700/136300 (epoch 69/100), loss = 0.190323 (0.230 sec/batch), lr: 0.076593
2019-12-27 17:41:20.043026: step 93720/136300 (epoch 69/100), loss = 0.237679 (0.199 sec/batch), lr: 0.076593
2019-12-27 17:41:24.676044: step 93740/136300 (epoch 69/100), loss = 0.177052 (0.216 sec/batch), lr: 0.076593
2019-12-27 17:41:29.416710: step 93760/136300 (epoch 69/100), loss = 0.308720 (0.233 sec/batch), lr: 0.076593
2019-12-27 17:41:34.223564: step 93780/136300 (epoch 69/100), loss = 0.186687 (0.233 sec/batch), lr: 0.076593
2019-12-27 17:41:39.018479: step 93800/136300 (epoch 69/100), loss = 0.149666 (0.234 sec/batch), lr: 0.076593
2019-12-27 17:41:43.892488: step 93820/136300 (epoch 69/100), loss = 0.152234 (0.230 sec/batch), lr: 0.076593
2019-12-27 17:41:48.625958: step 93840/136300 (epoch 69/100), loss = 0.221528 (0.231 sec/batch), lr: 0.076593
2019-12-27 17:41:53.266195: step 93860/136300 (epoch 69/100), loss = 0.213072 (0.213 sec/batch), lr: 0.076593
2019-12-27 17:41:58.117927: step 93880/136300 (epoch 69/100), loss = 0.242186 (0.231 sec/batch), lr: 0.076593
2019-12-27 17:42:04.217550: step 93900/136300 (epoch 69/100), loss = 0.307714 (0.227 sec/batch), lr: 0.076593
2019-12-27 17:42:09.097183: step 93920/136300 (epoch 69/100), loss = 0.331660 (0.211 sec/batch), lr: 0.076593
2019-12-27 17:42:13.936508: step 93940/136300 (epoch 69/100), loss = 0.244059 (0.204 sec/batch), lr: 0.076593
2019-12-27 17:42:18.576554: step 93960/136300 (epoch 69/100), loss = 0.199956 (0.244 sec/batch), lr: 0.076593
2019-12-27 17:42:23.387259: step 93980/136300 (epoch 69/100), loss = 0.422172 (0.243 sec/batch), lr: 0.076593
2019-12-27 17:42:28.132965: step 94000/136300 (epoch 69/100), loss = 0.148952 (0.199 sec/batch), lr: 0.076593
2019-12-27 17:42:33.036754: step 94020/136300 (epoch 69/100), loss = 0.294958 (0.228 sec/batch), lr: 0.076593
2019-12-27 17:42:37.883006: step 94040/136300 (epoch 69/100), loss = 0.183923 (0.241 sec/batch), lr: 0.076593
Evaluating on dev set...
Precision (micro): 72.618%
   Recall (micro): 61.277%
       F1 (micro): 66.467%
epoch 69: train_loss = 0.230311, dev_loss = 0.463967, dev_f1 = 0.6647
model saved to ./saved_models/01/checkpoint_epoch_69.pt

2019-12-27 17:43:16.209222: step 94060/136300 (epoch 70/100), loss = 0.322732 (0.237 sec/batch), lr: 0.068934
2019-12-27 17:43:22.149388: step 94080/136300 (epoch 70/100), loss = 0.299368 (0.249 sec/batch), lr: 0.068934
2019-12-27 17:43:26.949785: step 94100/136300 (epoch 70/100), loss = 0.119656 (0.243 sec/batch), lr: 0.068934
2019-12-27 17:43:31.635361: step 94120/136300 (epoch 70/100), loss = 0.183238 (0.221 sec/batch), lr: 0.068934
2019-12-27 17:43:36.533498: step 94140/136300 (epoch 70/100), loss = 0.248315 (0.237 sec/batch), lr: 0.068934
2019-12-27 17:43:41.235534: step 94160/136300 (epoch 70/100), loss = 0.163516 (0.218 sec/batch), lr: 0.068934
2019-12-27 17:43:46.027299: step 94180/136300 (epoch 70/100), loss = 0.281633 (0.237 sec/batch), lr: 0.068934
2019-12-27 17:43:50.771520: step 94200/136300 (epoch 70/100), loss = 0.223316 (0.182 sec/batch), lr: 0.068934
2019-12-27 17:43:55.627307: step 94220/136300 (epoch 70/100), loss = 0.086136 (0.232 sec/batch), lr: 0.068934
2019-12-27 17:44:00.497530: step 94240/136300 (epoch 70/100), loss = 0.274969 (0.205 sec/batch), lr: 0.068934
2019-12-27 17:44:06.489884: step 94260/136300 (epoch 70/100), loss = 0.118533 (1.415 sec/batch), lr: 0.068934
2019-12-27 17:44:11.166070: step 94280/136300 (epoch 70/100), loss = 0.252286 (0.179 sec/batch), lr: 0.068934
2019-12-27 17:44:15.934383: step 94300/136300 (epoch 70/100), loss = 0.165573 (0.240 sec/batch), lr: 0.068934
2019-12-27 17:44:20.810385: step 94320/136300 (epoch 70/100), loss = 0.235370 (0.230 sec/batch), lr: 0.068934
2019-12-27 17:44:25.606438: step 94340/136300 (epoch 70/100), loss = 0.147635 (0.239 sec/batch), lr: 0.068934
2019-12-27 17:44:30.295871: step 94360/136300 (epoch 70/100), loss = 0.347606 (0.200 sec/batch), lr: 0.068934
2019-12-27 17:44:35.081786: step 94380/136300 (epoch 70/100), loss = 0.259948 (0.233 sec/batch), lr: 0.068934
2019-12-27 17:44:39.904060: step 94400/136300 (epoch 70/100), loss = 0.110277 (0.245 sec/batch), lr: 0.068934
2019-12-27 17:44:44.624268: step 94420/136300 (epoch 70/100), loss = 0.224150 (0.180 sec/batch), lr: 0.068934
2019-12-27 17:44:49.356111: step 94440/136300 (epoch 70/100), loss = 0.191102 (0.211 sec/batch), lr: 0.068934
2019-12-27 17:44:55.375258: step 94460/136300 (epoch 70/100), loss = 0.199747 (0.232 sec/batch), lr: 0.068934
2019-12-27 17:45:00.176500: step 94480/136300 (epoch 70/100), loss = 0.336209 (0.238 sec/batch), lr: 0.068934
2019-12-27 17:45:05.029177: step 94500/136300 (epoch 70/100), loss = 0.203970 (0.214 sec/batch), lr: 0.068934
2019-12-27 17:45:09.829323: step 94520/136300 (epoch 70/100), loss = 0.198663 (0.203 sec/batch), lr: 0.068934
2019-12-27 17:45:14.636515: step 94540/136300 (epoch 70/100), loss = 0.235180 (0.228 sec/batch), lr: 0.068934
2019-12-27 17:45:19.503061: step 94560/136300 (epoch 70/100), loss = 0.397691 (0.247 sec/batch), lr: 0.068934
2019-12-27 17:45:24.374557: step 94580/136300 (epoch 70/100), loss = 0.362043 (0.228 sec/batch), lr: 0.068934
2019-12-27 17:45:29.232964: step 94600/136300 (epoch 70/100), loss = 0.160438 (0.217 sec/batch), lr: 0.068934
2019-12-27 17:45:34.067914: step 94620/136300 (epoch 70/100), loss = 0.187928 (0.238 sec/batch), lr: 0.068934
2019-12-27 17:45:38.870973: step 94640/136300 (epoch 70/100), loss = 0.451708 (0.232 sec/batch), lr: 0.068934
2019-12-27 17:45:44.887489: step 94660/136300 (epoch 70/100), loss = 0.261595 (0.219 sec/batch), lr: 0.068934
2019-12-27 17:45:49.692926: step 94680/136300 (epoch 70/100), loss = 0.143804 (0.219 sec/batch), lr: 0.068934
2019-12-27 17:45:54.463883: step 94700/136300 (epoch 70/100), loss = 0.268507 (0.238 sec/batch), lr: 0.068934
2019-12-27 17:45:59.301346: step 94720/136300 (epoch 70/100), loss = 0.179790 (0.241 sec/batch), lr: 0.068934
2019-12-27 17:46:04.222879: step 94740/136300 (epoch 70/100), loss = 0.160020 (0.222 sec/batch), lr: 0.068934
2019-12-27 17:46:09.119577: step 94760/136300 (epoch 70/100), loss = 0.231564 (0.232 sec/batch), lr: 0.068934
2019-12-27 17:46:14.057791: step 94780/136300 (epoch 70/100), loss = 0.321764 (0.239 sec/batch), lr: 0.068934
2019-12-27 17:46:18.900108: step 94800/136300 (epoch 70/100), loss = 0.391427 (0.229 sec/batch), lr: 0.068934
2019-12-27 17:46:23.658649: step 94820/136300 (epoch 70/100), loss = 0.238303 (0.215 sec/batch), lr: 0.068934
2019-12-27 17:46:28.602107: step 94840/136300 (epoch 70/100), loss = 0.249692 (0.247 sec/batch), lr: 0.068934
2019-12-27 17:46:34.965398: step 94860/136300 (epoch 70/100), loss = 0.154942 (0.220 sec/batch), lr: 0.068934
2019-12-27 17:46:39.756777: step 94880/136300 (epoch 70/100), loss = 0.324281 (0.223 sec/batch), lr: 0.068934
2019-12-27 17:46:44.681277: step 94900/136300 (epoch 70/100), loss = 0.412679 (0.215 sec/batch), lr: 0.068934
2019-12-27 17:46:49.354429: step 94920/136300 (epoch 70/100), loss = 0.250095 (0.229 sec/batch), lr: 0.068934
2019-12-27 17:46:54.254568: step 94940/136300 (epoch 70/100), loss = 0.156507 (0.219 sec/batch), lr: 0.068934
2019-12-27 17:46:59.202677: step 94960/136300 (epoch 70/100), loss = 0.151716 (0.233 sec/batch), lr: 0.068934
2019-12-27 17:47:04.005444: step 94980/136300 (epoch 70/100), loss = 0.172991 (0.228 sec/batch), lr: 0.068934
2019-12-27 17:47:08.957221: step 95000/136300 (epoch 70/100), loss = 0.257798 (0.233 sec/batch), lr: 0.068934
2019-12-27 17:47:13.841764: step 95020/136300 (epoch 70/100), loss = 0.291163 (0.208 sec/batch), lr: 0.068934
2019-12-27 17:47:18.696223: step 95040/136300 (epoch 70/100), loss = 0.255989 (0.228 sec/batch), lr: 0.068934
2019-12-27 17:47:24.904692: step 95060/136300 (epoch 70/100), loss = 0.100223 (0.240 sec/batch), lr: 0.068934
2019-12-27 17:47:29.729969: step 95080/136300 (epoch 70/100), loss = 0.192379 (0.180 sec/batch), lr: 0.068934
2019-12-27 17:47:34.425146: step 95100/136300 (epoch 70/100), loss = 0.086864 (0.236 sec/batch), lr: 0.068934
2019-12-27 17:47:39.161441: step 95120/136300 (epoch 70/100), loss = 0.234132 (0.220 sec/batch), lr: 0.068934
2019-12-27 17:47:44.026880: step 95140/136300 (epoch 70/100), loss = 0.233831 (0.241 sec/batch), lr: 0.068934
2019-12-27 17:47:48.850050: step 95160/136300 (epoch 70/100), loss = 0.198624 (0.215 sec/batch), lr: 0.068934
2019-12-27 17:47:53.752253: step 95180/136300 (epoch 70/100), loss = 0.516019 (0.205 sec/batch), lr: 0.068934
2019-12-27 17:47:58.506343: step 95200/136300 (epoch 70/100), loss = 0.183573 (0.215 sec/batch), lr: 0.068934
2019-12-27 17:48:03.234370: step 95220/136300 (epoch 70/100), loss = 0.227955 (0.247 sec/batch), lr: 0.068934
2019-12-27 17:48:08.068033: step 95240/136300 (epoch 70/100), loss = 0.370785 (0.234 sec/batch), lr: 0.068934
2019-12-27 17:48:14.355381: step 95260/136300 (epoch 70/100), loss = 0.276007 (0.242 sec/batch), lr: 0.068934
2019-12-27 17:48:19.256954: step 95280/136300 (epoch 70/100), loss = 0.291939 (0.247 sec/batch), lr: 0.068934
2019-12-27 17:48:24.152086: step 95300/136300 (epoch 70/100), loss = 0.111984 (0.207 sec/batch), lr: 0.068934
2019-12-27 17:48:28.795039: step 95320/136300 (epoch 70/100), loss = 0.309538 (0.242 sec/batch), lr: 0.068934
2019-12-27 17:48:33.613072: step 95340/136300 (epoch 70/100), loss = 0.222316 (0.208 sec/batch), lr: 0.068934
2019-12-27 17:48:38.437166: step 95360/136300 (epoch 70/100), loss = 0.093554 (0.242 sec/batch), lr: 0.068934
2019-12-27 17:48:43.354163: step 95380/136300 (epoch 70/100), loss = 0.173128 (0.238 sec/batch), lr: 0.068934
2019-12-27 17:48:48.226786: step 95400/136300 (epoch 70/100), loss = 0.176091 (0.246 sec/batch), lr: 0.068934
Evaluating on dev set...
Precision (micro): 71.764%
   Recall (micro): 62.417%
       F1 (micro): 66.765%
epoch 70: train_loss = 0.231096, dev_loss = 0.464011, dev_f1 = 0.6677
model saved to ./saved_models/01/checkpoint_epoch_70.pt

2019-12-27 17:49:26.576474: step 95420/136300 (epoch 71/100), loss = 0.189976 (0.175 sec/batch), lr: 0.068934
2019-12-27 17:49:32.748115: step 95440/136300 (epoch 71/100), loss = 0.291677 (0.244 sec/batch), lr: 0.068934
2019-12-27 17:49:37.558238: step 95460/136300 (epoch 71/100), loss = 0.244552 (0.217 sec/batch), lr: 0.068934
2019-12-27 17:49:42.285519: step 95480/136300 (epoch 71/100), loss = 0.287160 (0.234 sec/batch), lr: 0.068934
2019-12-27 17:49:47.236991: step 95500/136300 (epoch 71/100), loss = 0.147878 (0.237 sec/batch), lr: 0.068934
2019-12-27 17:49:52.030369: step 95520/136300 (epoch 71/100), loss = 0.089649 (0.198 sec/batch), lr: 0.068934
2019-12-27 17:49:56.796973: step 95540/136300 (epoch 71/100), loss = 0.173624 (0.251 sec/batch), lr: 0.068934
2019-12-27 17:50:01.611777: step 95560/136300 (epoch 71/100), loss = 0.206630 (0.164 sec/batch), lr: 0.068934
2019-12-27 17:50:06.482652: step 95580/136300 (epoch 71/100), loss = 0.280278 (0.241 sec/batch), lr: 0.068934
2019-12-27 17:50:11.370426: step 95600/136300 (epoch 71/100), loss = 0.256657 (0.223 sec/batch), lr: 0.068934
2019-12-27 17:50:16.167945: step 95620/136300 (epoch 71/100), loss = 0.335635 (0.238 sec/batch), lr: 0.068934
2019-12-27 17:50:22.282189: step 95640/136300 (epoch 71/100), loss = 0.223817 (0.240 sec/batch), lr: 0.068934
2019-12-27 17:50:27.063594: step 95660/136300 (epoch 71/100), loss = 0.197658 (0.176 sec/batch), lr: 0.068934
2019-12-27 17:50:31.964367: step 95680/136300 (epoch 71/100), loss = 0.300557 (0.211 sec/batch), lr: 0.068934
2019-12-27 17:50:36.799243: step 95700/136300 (epoch 71/100), loss = 0.299403 (0.245 sec/batch), lr: 0.068934
2019-12-27 17:50:41.549213: step 95720/136300 (epoch 71/100), loss = 0.197241 (0.239 sec/batch), lr: 0.068934
2019-12-27 17:50:46.342272: step 95740/136300 (epoch 71/100), loss = 0.223921 (0.207 sec/batch), lr: 0.068934
2019-12-27 17:50:51.188001: step 95760/136300 (epoch 71/100), loss = 0.165933 (0.228 sec/batch), lr: 0.068934
2019-12-27 17:50:56.050206: step 95780/136300 (epoch 71/100), loss = 0.147158 (0.182 sec/batch), lr: 0.068934
2019-12-27 17:51:00.776716: step 95800/136300 (epoch 71/100), loss = 0.214147 (0.206 sec/batch), lr: 0.068934
2019-12-27 17:51:06.858854: step 95820/136300 (epoch 71/100), loss = 0.070646 (0.210 sec/batch), lr: 0.068934
2019-12-27 17:51:11.642299: step 95840/136300 (epoch 71/100), loss = 0.226963 (0.234 sec/batch), lr: 0.068934
2019-12-27 17:51:16.578469: step 95860/136300 (epoch 71/100), loss = 0.105578 (0.229 sec/batch), lr: 0.068934
2019-12-27 17:51:21.463816: step 95880/136300 (epoch 71/100), loss = 0.193144 (0.245 sec/batch), lr: 0.068934
2019-12-27 17:51:26.233945: step 95900/136300 (epoch 71/100), loss = 0.239914 (0.228 sec/batch), lr: 0.068934
2019-12-27 17:51:31.128385: step 95920/136300 (epoch 71/100), loss = 0.289992 (0.239 sec/batch), lr: 0.068934
2019-12-27 17:51:36.008004: step 95940/136300 (epoch 71/100), loss = 0.227682 (0.241 sec/batch), lr: 0.068934
2019-12-27 17:51:40.881009: step 95960/136300 (epoch 71/100), loss = 0.329548 (0.240 sec/batch), lr: 0.068934
2019-12-27 17:51:45.700659: step 95980/136300 (epoch 71/100), loss = 0.209241 (0.206 sec/batch), lr: 0.068934
2019-12-27 17:51:50.554694: step 96000/136300 (epoch 71/100), loss = 0.114791 (0.242 sec/batch), lr: 0.068934
2019-12-27 17:51:56.574065: step 96020/136300 (epoch 71/100), loss = 0.271780 (0.249 sec/batch), lr: 0.068934
2019-12-27 17:52:01.394681: step 96040/136300 (epoch 71/100), loss = 0.245250 (0.241 sec/batch), lr: 0.068934
2019-12-27 17:52:06.102393: step 96060/136300 (epoch 71/100), loss = 0.158699 (0.221 sec/batch), lr: 0.068934
2019-12-27 17:52:10.965973: step 96080/136300 (epoch 71/100), loss = 0.202686 (0.230 sec/batch), lr: 0.068934
2019-12-27 17:52:15.879983: step 96100/136300 (epoch 71/100), loss = 0.212307 (0.232 sec/batch), lr: 0.068934
2019-12-27 17:52:20.778235: step 96120/136300 (epoch 71/100), loss = 0.326933 (0.215 sec/batch), lr: 0.068934
2019-12-27 17:52:25.688931: step 96140/136300 (epoch 71/100), loss = 0.197406 (0.212 sec/batch), lr: 0.068934
2019-12-27 17:52:30.586447: step 96160/136300 (epoch 71/100), loss = 0.102816 (0.202 sec/batch), lr: 0.068934
2019-12-27 17:52:35.317379: step 96180/136300 (epoch 71/100), loss = 0.045227 (0.221 sec/batch), lr: 0.068934
2019-12-27 17:52:40.230546: step 96200/136300 (epoch 71/100), loss = 0.381157 (0.227 sec/batch), lr: 0.068934
2019-12-27 17:52:46.522434: step 96220/136300 (epoch 71/100), loss = 0.101378 (0.238 sec/batch), lr: 0.068934
2019-12-27 17:52:51.341355: step 96240/136300 (epoch 71/100), loss = 0.175944 (0.236 sec/batch), lr: 0.068934
2019-12-27 17:52:56.296707: step 96260/136300 (epoch 71/100), loss = 0.186874 (0.232 sec/batch), lr: 0.068934
2019-12-27 17:53:00.964221: step 96280/136300 (epoch 71/100), loss = 0.207040 (0.222 sec/batch), lr: 0.068934
2019-12-27 17:53:05.833367: step 96300/136300 (epoch 71/100), loss = 0.184183 (0.240 sec/batch), lr: 0.068934
2019-12-27 17:53:10.751294: step 96320/136300 (epoch 71/100), loss = 0.184749 (0.227 sec/batch), lr: 0.068934
2019-12-27 17:53:15.572681: step 96340/136300 (epoch 71/100), loss = 0.232431 (0.239 sec/batch), lr: 0.068934
2019-12-27 17:53:20.487656: step 96360/136300 (epoch 71/100), loss = 0.067210 (0.243 sec/batch), lr: 0.068934
2019-12-27 17:53:25.432942: step 96380/136300 (epoch 71/100), loss = 0.363504 (0.221 sec/batch), lr: 0.068934
2019-12-27 17:53:30.249687: step 96400/136300 (epoch 71/100), loss = 0.187033 (0.240 sec/batch), lr: 0.068934
2019-12-27 17:53:36.507416: step 96420/136300 (epoch 71/100), loss = 0.197602 (0.243 sec/batch), lr: 0.068934
2019-12-27 17:53:41.389010: step 96440/136300 (epoch 71/100), loss = 0.162313 (0.195 sec/batch), lr: 0.068934
2019-12-27 17:53:46.034114: step 96460/136300 (epoch 71/100), loss = 0.187708 (0.219 sec/batch), lr: 0.068934
2019-12-27 17:53:50.762145: step 96480/136300 (epoch 71/100), loss = 0.218199 (0.211 sec/batch), lr: 0.068934
2019-12-27 17:53:55.638672: step 96500/136300 (epoch 71/100), loss = 0.208856 (0.217 sec/batch), lr: 0.068934
2019-12-27 17:54:00.476279: step 96520/136300 (epoch 71/100), loss = 0.172152 (0.251 sec/batch), lr: 0.068934
2019-12-27 17:54:05.379926: step 96540/136300 (epoch 71/100), loss = 0.295516 (0.232 sec/batch), lr: 0.068934
2019-12-27 17:54:10.152128: step 96560/136300 (epoch 71/100), loss = 0.272297 (0.226 sec/batch), lr: 0.068934
2019-12-27 17:54:14.866610: step 96580/136300 (epoch 71/100), loss = 0.194130 (0.232 sec/batch), lr: 0.068934
2019-12-27 17:54:19.650644: step 96600/136300 (epoch 71/100), loss = 0.251511 (0.232 sec/batch), lr: 0.068934
2019-12-27 17:54:26.053336: step 96620/136300 (epoch 71/100), loss = 0.132708 (0.238 sec/batch), lr: 0.068934
2019-12-27 17:54:30.989342: step 96640/136300 (epoch 71/100), loss = 0.274914 (0.233 sec/batch), lr: 0.068934
2019-12-27 17:54:35.896787: step 96660/136300 (epoch 71/100), loss = 0.273885 (0.240 sec/batch), lr: 0.068934
2019-12-27 17:54:40.546352: step 96680/136300 (epoch 71/100), loss = 0.180864 (0.214 sec/batch), lr: 0.068934
2019-12-27 17:54:45.343608: step 96700/136300 (epoch 71/100), loss = 0.167278 (0.181 sec/batch), lr: 0.068934
2019-12-27 17:54:50.134436: step 96720/136300 (epoch 71/100), loss = 0.266619 (0.217 sec/batch), lr: 0.068934
2019-12-27 17:54:55.085148: step 96740/136300 (epoch 71/100), loss = 0.225142 (0.241 sec/batch), lr: 0.068934
2019-12-27 17:54:59.923670: step 96760/136300 (epoch 71/100), loss = 0.100879 (0.207 sec/batch), lr: 0.068934
Evaluating on dev set...
Precision (micro): 72.723%
   Recall (micro): 61.405%
       F1 (micro): 66.587%
epoch 71: train_loss = 0.228221, dev_loss = 0.458522, dev_f1 = 0.6659
model saved to ./saved_models/01/checkpoint_epoch_71.pt

2019-12-27 17:55:38.434859: step 96780/136300 (epoch 72/100), loss = 0.311167 (0.231 sec/batch), lr: 0.062041
2019-12-27 17:55:44.404297: step 96800/136300 (epoch 72/100), loss = 0.161450 (0.229 sec/batch), lr: 0.062041
2019-12-27 17:55:49.258577: step 96820/136300 (epoch 72/100), loss = 0.339739 (0.246 sec/batch), lr: 0.062041
2019-12-27 17:55:53.997213: step 96840/136300 (epoch 72/100), loss = 0.211418 (0.242 sec/batch), lr: 0.062041
2019-12-27 17:55:58.899588: step 96860/136300 (epoch 72/100), loss = 0.235803 (0.216 sec/batch), lr: 0.062041
2019-12-27 17:56:03.738665: step 96880/136300 (epoch 72/100), loss = 0.206245 (0.231 sec/batch), lr: 0.062041
2019-12-27 17:56:08.485219: step 96900/136300 (epoch 72/100), loss = 0.116966 (0.234 sec/batch), lr: 0.062041
2019-12-27 17:56:13.373325: step 96920/136300 (epoch 72/100), loss = 0.147033 (0.246 sec/batch), lr: 0.062041
2019-12-27 17:56:18.193869: step 96940/136300 (epoch 72/100), loss = 0.233427 (0.223 sec/batch), lr: 0.062041
2019-12-27 17:56:23.101347: step 96960/136300 (epoch 72/100), loss = 0.234997 (0.234 sec/batch), lr: 0.062041
2019-12-27 17:56:27.925883: step 96980/136300 (epoch 72/100), loss = 0.139051 (0.239 sec/batch), lr: 0.062041
2019-12-27 17:56:34.234347: step 97000/136300 (epoch 72/100), loss = 0.380686 (0.216 sec/batch), lr: 0.062041
2019-12-27 17:56:39.094638: step 97020/136300 (epoch 72/100), loss = 0.139300 (0.212 sec/batch), lr: 0.062041
2019-12-27 17:56:43.918182: step 97040/136300 (epoch 72/100), loss = 0.113915 (0.242 sec/batch), lr: 0.062041
2019-12-27 17:56:48.724628: step 97060/136300 (epoch 72/100), loss = 0.310733 (0.241 sec/batch), lr: 0.062041
2019-12-27 17:56:53.553354: step 97080/136300 (epoch 72/100), loss = 0.219337 (0.216 sec/batch), lr: 0.062041
2019-12-27 17:56:58.326610: step 97100/136300 (epoch 72/100), loss = 0.239693 (0.245 sec/batch), lr: 0.062041
2019-12-27 17:57:03.141872: step 97120/136300 (epoch 72/100), loss = 0.180954 (0.240 sec/batch), lr: 0.062041
2019-12-27 17:57:08.011077: step 97140/136300 (epoch 72/100), loss = 0.222088 (0.179 sec/batch), lr: 0.062041
2019-12-27 17:57:12.810921: step 97160/136300 (epoch 72/100), loss = 0.188200 (0.242 sec/batch), lr: 0.062041
2019-12-27 17:57:17.441219: step 97180/136300 (epoch 72/100), loss = 0.127490 (0.234 sec/batch), lr: 0.062041
2019-12-27 17:57:23.609888: step 97200/136300 (epoch 72/100), loss = 0.247233 (0.245 sec/batch), lr: 0.062041
2019-12-27 17:57:28.566863: step 97220/136300 (epoch 72/100), loss = 0.097037 (0.238 sec/batch), lr: 0.062041
2019-12-27 17:57:33.473870: step 97240/136300 (epoch 72/100), loss = 0.200029 (0.233 sec/batch), lr: 0.062041
2019-12-27 17:57:38.245346: step 97260/136300 (epoch 72/100), loss = 0.158383 (0.231 sec/batch), lr: 0.062041
2019-12-27 17:57:43.108250: step 97280/136300 (epoch 72/100), loss = 0.165340 (0.230 sec/batch), lr: 0.062041
2019-12-27 17:57:47.979977: step 97300/136300 (epoch 72/100), loss = 0.187888 (0.233 sec/batch), lr: 0.062041
2019-12-27 17:57:52.951732: step 97320/136300 (epoch 72/100), loss = 0.167349 (0.242 sec/batch), lr: 0.062041
2019-12-27 17:57:57.766776: step 97340/136300 (epoch 72/100), loss = 0.190863 (0.243 sec/batch), lr: 0.062041
2019-12-27 17:58:02.527342: step 97360/136300 (epoch 72/100), loss = 0.107760 (0.235 sec/batch), lr: 0.062041
2019-12-27 17:58:08.730288: step 97380/136300 (epoch 72/100), loss = 0.352537 (1.670 sec/batch), lr: 0.062041
2019-12-27 17:58:13.468850: step 97400/136300 (epoch 72/100), loss = 0.197301 (0.220 sec/batch), lr: 0.062041
2019-12-27 17:58:18.236321: step 97420/136300 (epoch 72/100), loss = 0.175012 (0.209 sec/batch), lr: 0.062041
2019-12-27 17:58:23.050564: step 97440/136300 (epoch 72/100), loss = 0.257821 (0.239 sec/batch), lr: 0.062041
2019-12-27 17:58:27.945861: step 97460/136300 (epoch 72/100), loss = 0.164724 (0.240 sec/batch), lr: 0.062041
2019-12-27 17:58:32.869783: step 97480/136300 (epoch 72/100), loss = 0.145450 (0.236 sec/batch), lr: 0.062041
2019-12-27 17:58:37.780127: step 97500/136300 (epoch 72/100), loss = 0.341877 (0.241 sec/batch), lr: 0.062041
2019-12-27 17:58:42.680626: step 97520/136300 (epoch 72/100), loss = 0.400688 (0.202 sec/batch), lr: 0.062041
2019-12-27 17:58:47.418640: step 97540/136300 (epoch 72/100), loss = 0.235551 (0.203 sec/batch), lr: 0.062041
2019-12-27 17:58:52.275923: step 97560/136300 (epoch 72/100), loss = 0.345014 (0.250 sec/batch), lr: 0.062041
2019-12-27 17:58:58.493228: step 97580/136300 (epoch 72/100), loss = 0.136746 (0.246 sec/batch), lr: 0.062041
2019-12-27 17:59:03.236548: step 97600/136300 (epoch 72/100), loss = 0.159912 (0.240 sec/batch), lr: 0.062041
2019-12-27 17:59:08.205793: step 97620/136300 (epoch 72/100), loss = 0.288846 (0.236 sec/batch), lr: 0.062041
2019-12-27 17:59:12.932739: step 97640/136300 (epoch 72/100), loss = 0.224112 (0.236 sec/batch), lr: 0.062041
2019-12-27 17:59:17.731762: step 97660/136300 (epoch 72/100), loss = 0.236521 (0.242 sec/batch), lr: 0.062041
2019-12-27 17:59:22.650345: step 97680/136300 (epoch 72/100), loss = 0.278898 (0.243 sec/batch), lr: 0.062041
2019-12-27 17:59:27.491687: step 97700/136300 (epoch 72/100), loss = 0.265784 (0.162 sec/batch), lr: 0.062041
2019-12-27 17:59:32.382782: step 97720/136300 (epoch 72/100), loss = 0.410418 (0.240 sec/batch), lr: 0.062041
2019-12-27 17:59:37.352432: step 97740/136300 (epoch 72/100), loss = 0.303125 (0.223 sec/batch), lr: 0.062041
2019-12-27 17:59:42.229279: step 97760/136300 (epoch 72/100), loss = 0.217354 (0.231 sec/batch), lr: 0.062041
2019-12-27 17:59:48.646549: step 97780/136300 (epoch 72/100), loss = 0.291058 (0.223 sec/batch), lr: 0.062041
2019-12-27 17:59:53.556733: step 97800/136300 (epoch 72/100), loss = 0.213644 (0.239 sec/batch), lr: 0.062041
2019-12-27 17:59:58.176680: step 97820/136300 (epoch 72/100), loss = 0.304902 (0.231 sec/batch), lr: 0.062041
2019-12-27 18:00:02.959181: step 97840/136300 (epoch 72/100), loss = 0.159978 (0.236 sec/batch), lr: 0.062041
2019-12-27 18:00:07.775618: step 97860/136300 (epoch 72/100), loss = 0.239887 (0.240 sec/batch), lr: 0.062041
2019-12-27 18:00:12.594730: step 97880/136300 (epoch 72/100), loss = 0.304306 (0.244 sec/batch), lr: 0.062041
2019-12-27 18:00:17.470573: step 97900/136300 (epoch 72/100), loss = 0.298539 (0.200 sec/batch), lr: 0.062041
2019-12-27 18:00:22.249826: step 97920/136300 (epoch 72/100), loss = 0.315548 (0.235 sec/batch), lr: 0.062041
2019-12-27 18:00:27.006236: step 97940/136300 (epoch 72/100), loss = 0.271317 (0.218 sec/batch), lr: 0.062041
2019-12-27 18:00:31.756620: step 97960/136300 (epoch 72/100), loss = 0.253440 (0.164 sec/batch), lr: 0.062041
2019-12-27 18:00:37.962166: step 97980/136300 (epoch 72/100), loss = 0.270085 (0.236 sec/batch), lr: 0.062041
2019-12-27 18:00:42.894506: step 98000/136300 (epoch 72/100), loss = 0.187323 (0.232 sec/batch), lr: 0.062041
2019-12-27 18:00:47.781352: step 98020/136300 (epoch 72/100), loss = 0.460906 (0.231 sec/batch), lr: 0.062041
2019-12-27 18:00:52.518709: step 98040/136300 (epoch 72/100), loss = 0.135685 (0.172 sec/batch), lr: 0.062041
2019-12-27 18:00:57.287237: step 98060/136300 (epoch 72/100), loss = 0.194948 (0.198 sec/batch), lr: 0.062041
2019-12-27 18:01:02.065159: step 98080/136300 (epoch 72/100), loss = 0.157428 (0.200 sec/batch), lr: 0.062041
2019-12-27 18:01:07.006662: step 98100/136300 (epoch 72/100), loss = 0.199636 (0.234 sec/batch), lr: 0.062041
2019-12-27 18:01:11.906161: step 98120/136300 (epoch 72/100), loss = 0.129056 (0.226 sec/batch), lr: 0.062041
Evaluating on dev set...
Precision (micro): 71.149%
   Recall (micro): 62.196%
       F1 (micro): 66.372%
epoch 72: train_loss = 0.227182, dev_loss = 0.453145, dev_f1 = 0.6637
model saved to ./saved_models/01/checkpoint_epoch_72.pt

2019-12-27 18:01:50.338170: step 98140/136300 (epoch 73/100), loss = 0.125228 (0.205 sec/batch), lr: 0.055837
2019-12-27 18:01:56.394610: step 98160/136300 (epoch 73/100), loss = 0.443723 (0.223 sec/batch), lr: 0.055837
2019-12-27 18:02:01.205493: step 98180/136300 (epoch 73/100), loss = 0.379786 (0.220 sec/batch), lr: 0.055837
2019-12-27 18:02:05.986602: step 98200/136300 (epoch 73/100), loss = 0.166934 (0.199 sec/batch), lr: 0.055837
2019-12-27 18:02:10.925337: step 98220/136300 (epoch 73/100), loss = 0.249991 (0.232 sec/batch), lr: 0.055837
2019-12-27 18:02:15.718367: step 98240/136300 (epoch 73/100), loss = 0.344390 (0.218 sec/batch), lr: 0.055837
2019-12-27 18:02:20.519953: step 98260/136300 (epoch 73/100), loss = 0.204730 (0.206 sec/batch), lr: 0.055837
2019-12-27 18:02:25.405133: step 98280/136300 (epoch 73/100), loss = 0.603382 (0.218 sec/batch), lr: 0.055837
2019-12-27 18:02:30.247910: step 98300/136300 (epoch 73/100), loss = 0.149843 (0.242 sec/batch), lr: 0.055837
2019-12-27 18:02:35.142441: step 98320/136300 (epoch 73/100), loss = 0.289208 (0.233 sec/batch), lr: 0.055837
2019-12-27 18:02:39.945956: step 98340/136300 (epoch 73/100), loss = 0.196243 (0.175 sec/batch), lr: 0.055837
2019-12-27 18:02:46.227490: step 98360/136300 (epoch 73/100), loss = 0.245072 (0.184 sec/batch), lr: 0.055837
2019-12-27 18:02:51.081165: step 98380/136300 (epoch 73/100), loss = 0.206799 (0.235 sec/batch), lr: 0.055837
2019-12-27 18:02:55.857308: step 98400/136300 (epoch 73/100), loss = 0.093132 (0.232 sec/batch), lr: 0.055837
2019-12-27 18:03:00.674649: step 98420/136300 (epoch 73/100), loss = 0.129633 (0.202 sec/batch), lr: 0.055837
2019-12-27 18:03:05.553301: step 98440/136300 (epoch 73/100), loss = 0.410739 (0.222 sec/batch), lr: 0.055837
2019-12-27 18:03:10.271405: step 98460/136300 (epoch 73/100), loss = 0.147148 (0.204 sec/batch), lr: 0.055837
2019-12-27 18:03:15.078192: step 98480/136300 (epoch 73/100), loss = 0.106762 (0.232 sec/batch), lr: 0.055837
2019-12-27 18:03:19.997945: step 98500/136300 (epoch 73/100), loss = 0.179475 (0.249 sec/batch), lr: 0.055837
2019-12-27 18:03:24.739402: step 98520/136300 (epoch 73/100), loss = 0.300095 (0.230 sec/batch), lr: 0.055837
2019-12-27 18:03:29.398626: step 98540/136300 (epoch 73/100), loss = 0.158385 (0.168 sec/batch), lr: 0.055837
2019-12-27 18:03:35.525423: step 98560/136300 (epoch 73/100), loss = 0.116787 (0.202 sec/batch), lr: 0.055837
2019-12-27 18:03:40.465030: step 98580/136300 (epoch 73/100), loss = 0.431752 (0.212 sec/batch), lr: 0.055837
2019-12-27 18:03:45.374267: step 98600/136300 (epoch 73/100), loss = 0.291088 (0.225 sec/batch), lr: 0.055837
2019-12-27 18:03:50.196014: step 98620/136300 (epoch 73/100), loss = 0.186190 (0.218 sec/batch), lr: 0.055837
2019-12-27 18:03:55.044846: step 98640/136300 (epoch 73/100), loss = 0.180232 (0.231 sec/batch), lr: 0.055837
2019-12-27 18:03:59.926908: step 98660/136300 (epoch 73/100), loss = 0.184939 (0.229 sec/batch), lr: 0.055837
2019-12-27 18:04:04.848948: step 98680/136300 (epoch 73/100), loss = 0.242445 (0.176 sec/batch), lr: 0.055837
2019-12-27 18:04:09.628137: step 98700/136300 (epoch 73/100), loss = 0.155861 (0.180 sec/batch), lr: 0.055837
2019-12-27 18:04:14.464774: step 98720/136300 (epoch 73/100), loss = 0.186733 (0.234 sec/batch), lr: 0.055837
2019-12-27 18:04:19.223484: step 98740/136300 (epoch 73/100), loss = 0.193000 (0.221 sec/batch), lr: 0.055837
2019-12-27 18:04:25.505101: step 98760/136300 (epoch 73/100), loss = 0.214705 (0.229 sec/batch), lr: 0.055837
2019-12-27 18:04:30.268785: step 98780/136300 (epoch 73/100), loss = 0.263981 (0.203 sec/batch), lr: 0.055837
2019-12-27 18:04:35.049124: step 98800/136300 (epoch 73/100), loss = 0.324226 (0.202 sec/batch), lr: 0.055837
2019-12-27 18:04:39.949729: step 98820/136300 (epoch 73/100), loss = 0.115116 (0.236 sec/batch), lr: 0.055837
2019-12-27 18:04:44.863939: step 98840/136300 (epoch 73/100), loss = 0.172688 (0.218 sec/batch), lr: 0.055837
2019-12-27 18:04:49.780924: step 98860/136300 (epoch 73/100), loss = 0.336205 (0.205 sec/batch), lr: 0.055837
2019-12-27 18:04:54.688344: step 98880/136300 (epoch 73/100), loss = 0.106559 (0.239 sec/batch), lr: 0.055837
2019-12-27 18:04:59.468464: step 98900/136300 (epoch 73/100), loss = 0.118492 (0.242 sec/batch), lr: 0.055837
2019-12-27 18:05:04.310772: step 98920/136300 (epoch 73/100), loss = 0.213549 (0.244 sec/batch), lr: 0.055837
2019-12-27 18:05:10.533167: step 98940/136300 (epoch 73/100), loss = 0.322279 (0.231 sec/batch), lr: 0.055837
2019-12-27 18:05:15.324703: step 98960/136300 (epoch 73/100), loss = 0.236328 (0.239 sec/batch), lr: 0.055837
2019-12-27 18:05:20.242927: step 98980/136300 (epoch 73/100), loss = 0.147292 (0.233 sec/batch), lr: 0.055837
2019-12-27 18:05:25.048525: step 99000/136300 (epoch 73/100), loss = 0.205587 (0.210 sec/batch), lr: 0.055837
2019-12-27 18:05:29.806464: step 99020/136300 (epoch 73/100), loss = 0.346612 (0.234 sec/batch), lr: 0.055837
2019-12-27 18:05:34.704251: step 99040/136300 (epoch 73/100), loss = 0.158864 (0.232 sec/batch), lr: 0.055837
2019-12-27 18:05:39.676727: step 99060/136300 (epoch 73/100), loss = 0.190566 (0.241 sec/batch), lr: 0.055837
2019-12-27 18:05:44.471609: step 99080/136300 (epoch 73/100), loss = 0.157494 (0.219 sec/batch), lr: 0.055837
2019-12-27 18:05:49.434814: step 99100/136300 (epoch 73/100), loss = 0.180191 (0.232 sec/batch), lr: 0.055837
2019-12-27 18:05:54.358422: step 99120/136300 (epoch 73/100), loss = 0.155017 (0.216 sec/batch), lr: 0.055837
2019-12-27 18:06:00.579432: step 99140/136300 (epoch 73/100), loss = 0.262320 (0.216 sec/batch), lr: 0.055837
2019-12-27 18:06:05.473907: step 99160/136300 (epoch 73/100), loss = 0.161536 (0.242 sec/batch), lr: 0.055837
2019-12-27 18:06:10.114274: step 99180/136300 (epoch 73/100), loss = 0.186516 (0.230 sec/batch), lr: 0.055837
2019-12-27 18:06:14.890175: step 99200/136300 (epoch 73/100), loss = 0.161732 (0.228 sec/batch), lr: 0.055837
2019-12-27 18:06:19.739905: step 99220/136300 (epoch 73/100), loss = 0.496501 (0.227 sec/batch), lr: 0.055837
2019-12-27 18:06:24.503594: step 99240/136300 (epoch 73/100), loss = 0.120280 (0.244 sec/batch), lr: 0.055837
2019-12-27 18:06:29.431501: step 99260/136300 (epoch 73/100), loss = 0.264947 (0.244 sec/batch), lr: 0.055837
2019-12-27 18:06:34.213460: step 99280/136300 (epoch 73/100), loss = 0.252262 (0.200 sec/batch), lr: 0.055837
2019-12-27 18:06:38.974831: step 99300/136300 (epoch 73/100), loss = 0.174387 (0.170 sec/batch), lr: 0.055837
2019-12-27 18:06:43.743250: step 99320/136300 (epoch 73/100), loss = 0.261453 (0.234 sec/batch), lr: 0.055837
2019-12-27 18:06:50.032418: step 99340/136300 (epoch 73/100), loss = 0.212926 (0.231 sec/batch), lr: 0.055837
2019-12-27 18:06:54.925712: step 99360/136300 (epoch 73/100), loss = 0.234332 (0.234 sec/batch), lr: 0.055837
2019-12-27 18:06:59.831836: step 99380/136300 (epoch 73/100), loss = 0.249410 (0.245 sec/batch), lr: 0.055837
2019-12-27 18:07:04.634369: step 99400/136300 (epoch 73/100), loss = 0.198242 (0.218 sec/batch), lr: 0.055837
2019-12-27 18:07:09.373078: step 99420/136300 (epoch 73/100), loss = 0.120107 (0.241 sec/batch), lr: 0.055837
2019-12-27 18:07:14.118528: step 99440/136300 (epoch 73/100), loss = 0.109052 (0.225 sec/batch), lr: 0.055837
2019-12-27 18:07:19.013505: step 99460/136300 (epoch 73/100), loss = 0.376966 (0.242 sec/batch), lr: 0.055837
2019-12-27 18:07:23.928613: step 99480/136300 (epoch 73/100), loss = 0.196179 (0.224 sec/batch), lr: 0.055837
Evaluating on dev set...
Precision (micro): 71.357%
   Recall (micro): 63.245%
       F1 (micro): 67.057%
epoch 73: train_loss = 0.226221, dev_loss = 0.459925, dev_f1 = 0.6706
model saved to ./saved_models/01/checkpoint_epoch_73.pt

2019-12-27 18:08:02.263750: step 99500/136300 (epoch 74/100), loss = 0.259970 (0.174 sec/batch), lr: 0.055837
2019-12-27 18:08:08.555441: step 99520/136300 (epoch 74/100), loss = 0.212693 (0.198 sec/batch), lr: 0.055837
2019-12-27 18:08:13.341666: step 99540/136300 (epoch 74/100), loss = 0.136099 (0.243 sec/batch), lr: 0.055837
2019-12-27 18:08:18.178695: step 99560/136300 (epoch 74/100), loss = 0.223780 (0.239 sec/batch), lr: 0.055837
2019-12-27 18:08:23.054507: step 99580/136300 (epoch 74/100), loss = 0.060711 (0.237 sec/batch), lr: 0.055837
2019-12-27 18:08:27.843798: step 99600/136300 (epoch 74/100), loss = 0.241861 (0.203 sec/batch), lr: 0.055837
2019-12-27 18:08:32.605489: step 99620/136300 (epoch 74/100), loss = 0.280810 (0.177 sec/batch), lr: 0.055837
2019-12-27 18:08:37.462239: step 99640/136300 (epoch 74/100), loss = 0.301002 (0.239 sec/batch), lr: 0.055837
2019-12-27 18:08:42.289983: step 99660/136300 (epoch 74/100), loss = 0.191331 (0.246 sec/batch), lr: 0.055837
2019-12-27 18:08:47.190764: step 99680/136300 (epoch 74/100), loss = 0.192792 (0.241 sec/batch), lr: 0.055837
2019-12-27 18:08:52.019095: step 99700/136300 (epoch 74/100), loss = 0.149748 (0.205 sec/batch), lr: 0.055837
2019-12-27 18:08:58.077683: step 99720/136300 (epoch 74/100), loss = 0.161174 (0.203 sec/batch), lr: 0.055837
2019-12-27 18:09:02.853434: step 99740/136300 (epoch 74/100), loss = 0.131475 (0.223 sec/batch), lr: 0.055837
2019-12-27 18:09:07.695480: step 99760/136300 (epoch 74/100), loss = 0.277917 (0.247 sec/batch), lr: 0.055837
2019-12-27 18:09:12.503317: step 99780/136300 (epoch 74/100), loss = 0.167838 (0.221 sec/batch), lr: 0.055837
2019-12-27 18:09:17.347605: step 99800/136300 (epoch 74/100), loss = 0.279582 (0.217 sec/batch), lr: 0.055837
2019-12-27 18:09:22.139724: step 99820/136300 (epoch 74/100), loss = 0.167800 (0.240 sec/batch), lr: 0.055837
2019-12-27 18:09:26.915178: step 99840/136300 (epoch 74/100), loss = 0.132271 (0.204 sec/batch), lr: 0.055837
2019-12-27 18:09:31.818360: step 99860/136300 (epoch 74/100), loss = 0.418943 (0.220 sec/batch), lr: 0.055837
2019-12-27 18:09:36.592231: step 99880/136300 (epoch 74/100), loss = 0.176191 (0.224 sec/batch), lr: 0.055837
2019-12-27 18:09:41.321168: step 99900/136300 (epoch 74/100), loss = 0.236528 (0.177 sec/batch), lr: 0.055837
2019-12-27 18:09:47.537241: step 99920/136300 (epoch 74/100), loss = 0.268676 (0.185 sec/batch), lr: 0.055837
2019-12-27 18:09:52.403977: step 99940/136300 (epoch 74/100), loss = 0.141660 (0.206 sec/batch), lr: 0.055837
2019-12-27 18:09:57.321443: step 99960/136300 (epoch 74/100), loss = 0.249309 (0.239 sec/batch), lr: 0.055837
2019-12-27 18:10:02.141439: step 99980/136300 (epoch 74/100), loss = 0.259716 (0.207 sec/batch), lr: 0.055837
2019-12-27 18:10:06.957765: step 100000/136300 (epoch 74/100), loss = 0.165054 (0.233 sec/batch), lr: 0.055837
2019-12-27 18:10:11.800020: step 100020/136300 (epoch 74/100), loss = 0.136135 (0.173 sec/batch), lr: 0.055837
2019-12-27 18:10:16.792578: step 100040/136300 (epoch 74/100), loss = 0.190680 (0.240 sec/batch), lr: 0.055837
2019-12-27 18:10:21.550396: step 100060/136300 (epoch 74/100), loss = 0.225544 (0.201 sec/batch), lr: 0.055837
2019-12-27 18:10:26.391963: step 100080/136300 (epoch 74/100), loss = 0.387871 (0.221 sec/batch), lr: 0.055837
2019-12-27 18:10:31.107344: step 100100/136300 (epoch 74/100), loss = 0.118796 (0.224 sec/batch), lr: 0.055837
2019-12-27 18:10:37.133623: step 100120/136300 (epoch 74/100), loss = 0.382814 (0.229 sec/batch), lr: 0.055837
2019-12-27 18:10:41.937978: step 100140/136300 (epoch 74/100), loss = 0.156353 (0.234 sec/batch), lr: 0.055837
2019-12-27 18:10:46.721318: step 100160/136300 (epoch 74/100), loss = 0.156976 (0.243 sec/batch), lr: 0.055837
2019-12-27 18:10:51.599596: step 100180/136300 (epoch 74/100), loss = 0.374710 (0.243 sec/batch), lr: 0.055837
2019-12-27 18:10:56.521061: step 100200/136300 (epoch 74/100), loss = 0.303324 (0.229 sec/batch), lr: 0.055837
2019-12-27 18:11:01.419734: step 100220/136300 (epoch 74/100), loss = 0.079097 (0.228 sec/batch), lr: 0.055837
2019-12-27 18:11:06.310665: step 100240/136300 (epoch 74/100), loss = 0.206426 (0.241 sec/batch), lr: 0.055837
2019-12-27 18:11:11.120217: step 100260/136300 (epoch 74/100), loss = 0.204920 (0.231 sec/batch), lr: 0.055837
2019-12-27 18:11:15.923137: step 100280/136300 (epoch 74/100), loss = 0.477521 (0.231 sec/batch), lr: 0.055837
2019-12-27 18:11:22.329101: step 100300/136300 (epoch 74/100), loss = 0.323408 (0.238 sec/batch), lr: 0.055837
2019-12-27 18:11:27.118214: step 100320/136300 (epoch 74/100), loss = 0.270438 (0.227 sec/batch), lr: 0.055837
2019-12-27 18:11:31.990574: step 100340/136300 (epoch 74/100), loss = 0.298681 (0.240 sec/batch), lr: 0.055837
2019-12-27 18:11:36.869617: step 100360/136300 (epoch 74/100), loss = 0.237164 (0.229 sec/batch), lr: 0.055837
2019-12-27 18:11:41.589344: step 100380/136300 (epoch 74/100), loss = 0.221311 (0.206 sec/batch), lr: 0.055837
2019-12-27 18:11:46.436880: step 100400/136300 (epoch 74/100), loss = 0.214926 (0.231 sec/batch), lr: 0.055837
2019-12-27 18:11:51.377630: step 100420/136300 (epoch 74/100), loss = 0.212627 (0.247 sec/batch), lr: 0.055837
2019-12-27 18:11:56.154210: step 100440/136300 (epoch 74/100), loss = 0.144714 (0.184 sec/batch), lr: 0.055837
2019-12-27 18:12:01.154476: step 100460/136300 (epoch 74/100), loss = 0.210520 (0.248 sec/batch), lr: 0.055837
2019-12-27 18:12:06.030322: step 100480/136300 (epoch 74/100), loss = 0.146564 (0.232 sec/batch), lr: 0.055837
2019-12-27 18:12:10.854002: step 100500/136300 (epoch 74/100), loss = 0.193838 (0.235 sec/batch), lr: 0.055837
2019-12-27 18:12:16.986810: step 100520/136300 (epoch 74/100), loss = 0.157133 (0.232 sec/batch), lr: 0.055837
2019-12-27 18:12:21.693345: step 100540/136300 (epoch 74/100), loss = 0.166883 (0.213 sec/batch), lr: 0.055837
2019-12-27 18:12:26.409193: step 100560/136300 (epoch 74/100), loss = 0.217139 (0.230 sec/batch), lr: 0.055837
2019-12-27 18:12:31.273185: step 100580/136300 (epoch 74/100), loss = 0.209041 (0.236 sec/batch), lr: 0.055837
2019-12-27 18:12:36.050948: step 100600/136300 (epoch 74/100), loss = 0.334843 (0.226 sec/batch), lr: 0.055837
2019-12-27 18:12:40.962935: step 100620/136300 (epoch 74/100), loss = 0.244130 (0.221 sec/batch), lr: 0.055837
2019-12-27 18:12:45.834403: step 100640/136300 (epoch 74/100), loss = 0.137132 (0.240 sec/batch), lr: 0.055837
2019-12-27 18:12:50.582095: step 100660/136300 (epoch 74/100), loss = 0.228237 (0.236 sec/batch), lr: 0.055837
2019-12-27 18:12:55.338406: step 100680/136300 (epoch 74/100), loss = 0.176026 (0.245 sec/batch), lr: 0.055837
2019-12-27 18:13:01.585492: step 100700/136300 (epoch 74/100), loss = 0.131159 (0.234 sec/batch), lr: 0.055837
2019-12-27 18:13:06.489318: step 100720/136300 (epoch 74/100), loss = 0.259532 (0.241 sec/batch), lr: 0.055837
2019-12-27 18:13:11.408577: step 100740/136300 (epoch 74/100), loss = 0.169866 (0.238 sec/batch), lr: 0.055837
2019-12-27 18:13:16.217242: step 100760/136300 (epoch 74/100), loss = 0.525441 (0.186 sec/batch), lr: 0.055837
2019-12-27 18:13:20.940484: step 100780/136300 (epoch 74/100), loss = 0.112381 (0.221 sec/batch), lr: 0.055837
2019-12-27 18:13:25.730270: step 100800/136300 (epoch 74/100), loss = 0.153524 (0.216 sec/batch), lr: 0.055837
2019-12-27 18:13:30.544671: step 100820/136300 (epoch 74/100), loss = 0.144455 (0.240 sec/batch), lr: 0.055837
2019-12-27 18:13:35.459689: step 100840/136300 (epoch 74/100), loss = 0.237304 (0.243 sec/batch), lr: 0.055837
2019-12-27 18:13:40.398373: step 100860/136300 (epoch 74/100), loss = 0.321752 (0.220 sec/batch), lr: 0.055837
Evaluating on dev set...
Precision (micro): 70.445%
   Recall (micro): 63.797%
       F1 (micro): 66.956%
epoch 74: train_loss = 0.224499, dev_loss = 0.450667, dev_f1 = 0.6696
model saved to ./saved_models/01/checkpoint_epoch_74.pt

2019-12-27 18:14:20.044439: step 100880/136300 (epoch 75/100), loss = 0.105156 (1.601 sec/batch), lr: 0.050253
2019-12-27 18:14:24.759647: step 100900/136300 (epoch 75/100), loss = 0.180904 (0.220 sec/batch), lr: 0.050253
2019-12-27 18:14:29.618444: step 100920/136300 (epoch 75/100), loss = 0.303798 (0.233 sec/batch), lr: 0.050253
2019-12-27 18:14:34.473944: step 100940/136300 (epoch 75/100), loss = 0.196391 (0.244 sec/batch), lr: 0.050253
2019-12-27 18:14:39.286985: step 100960/136300 (epoch 75/100), loss = 0.224302 (0.218 sec/batch), lr: 0.050253
2019-12-27 18:14:44.077676: step 100980/136300 (epoch 75/100), loss = 0.324891 (0.238 sec/batch), lr: 0.050253
2019-12-27 18:14:48.888384: step 101000/136300 (epoch 75/100), loss = 0.265412 (0.231 sec/batch), lr: 0.050253
2019-12-27 18:14:53.700092: step 101020/136300 (epoch 75/100), loss = 0.256818 (0.216 sec/batch), lr: 0.050253
2019-12-27 18:14:58.610302: step 101040/136300 (epoch 75/100), loss = 0.155533 (0.240 sec/batch), lr: 0.050253
2019-12-27 18:15:03.480992: step 101060/136300 (epoch 75/100), loss = 0.307833 (0.241 sec/batch), lr: 0.050253
2019-12-27 18:15:09.530265: step 101080/136300 (epoch 75/100), loss = 0.126492 (0.223 sec/batch), lr: 0.050253
2019-12-27 18:15:14.270564: step 101100/136300 (epoch 75/100), loss = 0.166068 (0.227 sec/batch), lr: 0.050253
2019-12-27 18:15:19.087872: step 101120/136300 (epoch 75/100), loss = 0.222101 (0.242 sec/batch), lr: 0.050253
2019-12-27 18:15:23.961171: step 101140/136300 (epoch 75/100), loss = 0.113167 (0.240 sec/batch), lr: 0.050253
2019-12-27 18:15:28.778625: step 101160/136300 (epoch 75/100), loss = 0.133897 (0.240 sec/batch), lr: 0.050253
2019-12-27 18:15:33.538359: step 101180/136300 (epoch 75/100), loss = 0.074592 (0.236 sec/batch), lr: 0.050253
2019-12-27 18:15:38.406615: step 101200/136300 (epoch 75/100), loss = 0.430594 (0.239 sec/batch), lr: 0.050253
2019-12-27 18:15:43.245258: step 101220/136300 (epoch 75/100), loss = 0.228615 (0.215 sec/batch), lr: 0.050253
2019-12-27 18:15:48.003871: step 101240/136300 (epoch 75/100), loss = 0.138421 (0.223 sec/batch), lr: 0.050253
2019-12-27 18:15:52.785656: step 101260/136300 (epoch 75/100), loss = 0.259692 (0.205 sec/batch), lr: 0.050253
2019-12-27 18:15:58.990696: step 101280/136300 (epoch 75/100), loss = 0.102007 (0.211 sec/batch), lr: 0.050253
2019-12-27 18:16:03.852451: step 101300/136300 (epoch 75/100), loss = 0.283415 (0.233 sec/batch), lr: 0.050253
2019-12-27 18:16:08.751490: step 101320/136300 (epoch 75/100), loss = 0.107773 (0.242 sec/batch), lr: 0.050253
2019-12-27 18:16:13.575336: step 101340/136300 (epoch 75/100), loss = 0.124597 (0.234 sec/batch), lr: 0.050253
2019-12-27 18:16:18.399680: step 101360/136300 (epoch 75/100), loss = 0.132669 (0.235 sec/batch), lr: 0.050253
2019-12-27 18:16:23.290739: step 101380/136300 (epoch 75/100), loss = 0.431158 (0.216 sec/batch), lr: 0.050253
2019-12-27 18:16:28.238565: step 101400/136300 (epoch 75/100), loss = 0.113827 (0.242 sec/batch), lr: 0.050253
2019-12-27 18:16:33.031627: step 101420/136300 (epoch 75/100), loss = 0.227079 (0.238 sec/batch), lr: 0.050253
2019-12-27 18:16:37.870144: step 101440/136300 (epoch 75/100), loss = 0.171698 (0.244 sec/batch), lr: 0.050253
2019-12-27 18:16:42.595834: step 101460/136300 (epoch 75/100), loss = 0.124791 (0.239 sec/batch), lr: 0.050253
2019-12-27 18:16:48.569024: step 101480/136300 (epoch 75/100), loss = 0.183562 (0.214 sec/batch), lr: 0.050253
2019-12-27 18:16:53.426498: step 101500/136300 (epoch 75/100), loss = 0.275023 (0.233 sec/batch), lr: 0.050253
2019-12-27 18:16:58.178304: step 101520/136300 (epoch 75/100), loss = 0.170542 (0.239 sec/batch), lr: 0.050253
2019-12-27 18:17:03.026783: step 101540/136300 (epoch 75/100), loss = 0.095824 (0.245 sec/batch), lr: 0.050253
2019-12-27 18:17:07.964471: step 101560/136300 (epoch 75/100), loss = 0.272908 (0.201 sec/batch), lr: 0.050253
2019-12-27 18:17:12.872953: step 101580/136300 (epoch 75/100), loss = 0.144231 (0.226 sec/batch), lr: 0.050253
2019-12-27 18:17:17.806425: step 101600/136300 (epoch 75/100), loss = 0.344677 (0.235 sec/batch), lr: 0.050253
2019-12-27 18:17:22.595004: step 101620/136300 (epoch 75/100), loss = 0.286903 (0.222 sec/batch), lr: 0.050253
2019-12-27 18:17:27.443050: step 101640/136300 (epoch 75/100), loss = 0.190968 (0.233 sec/batch), lr: 0.050253
2019-12-27 18:17:33.860373: step 101660/136300 (epoch 75/100), loss = 0.175481 (1.704 sec/batch), lr: 0.050253
2019-12-27 18:17:38.617495: step 101680/136300 (epoch 75/100), loss = 0.236543 (0.175 sec/batch), lr: 0.050253
2019-12-27 18:17:43.485988: step 101700/136300 (epoch 75/100), loss = 0.205826 (0.246 sec/batch), lr: 0.050253
2019-12-27 18:17:48.409722: step 101720/136300 (epoch 75/100), loss = 0.208812 (0.215 sec/batch), lr: 0.050253
2019-12-27 18:17:53.108587: step 101740/136300 (epoch 75/100), loss = 0.281388 (0.234 sec/batch), lr: 0.050253
2019-12-27 18:17:58.001441: step 101760/136300 (epoch 75/100), loss = 0.334947 (0.235 sec/batch), lr: 0.050253
2019-12-27 18:18:02.910633: step 101780/136300 (epoch 75/100), loss = 0.225533 (0.242 sec/batch), lr: 0.050253
2019-12-27 18:18:07.722507: step 101800/136300 (epoch 75/100), loss = 0.187938 (0.238 sec/batch), lr: 0.050253
2019-12-27 18:18:12.693175: step 101820/136300 (epoch 75/100), loss = 0.194791 (0.246 sec/batch), lr: 0.050253
2019-12-27 18:18:17.560743: step 101840/136300 (epoch 75/100), loss = 0.304756 (0.235 sec/batch), lr: 0.050253
2019-12-27 18:18:22.372308: step 101860/136300 (epoch 75/100), loss = 0.290163 (0.215 sec/batch), lr: 0.050253
2019-12-27 18:18:28.576740: step 101880/136300 (epoch 75/100), loss = 0.226438 (0.247 sec/batch), lr: 0.050253
2019-12-27 18:18:33.374439: step 101900/136300 (epoch 75/100), loss = 0.169380 (0.201 sec/batch), lr: 0.050253
2019-12-27 18:18:38.083595: step 101920/136300 (epoch 75/100), loss = 0.326120 (0.231 sec/batch), lr: 0.050253
2019-12-27 18:18:42.869727: step 101940/136300 (epoch 75/100), loss = 0.261680 (0.214 sec/batch), lr: 0.050253
2019-12-27 18:18:47.647617: step 101960/136300 (epoch 75/100), loss = 0.200323 (0.180 sec/batch), lr: 0.050253
2019-12-27 18:18:52.585667: step 101980/136300 (epoch 75/100), loss = 0.243462 (0.243 sec/batch), lr: 0.050253
2019-12-27 18:18:57.445533: step 102000/136300 (epoch 75/100), loss = 0.233327 (0.225 sec/batch), lr: 0.050253
2019-12-27 18:19:02.223198: step 102020/136300 (epoch 75/100), loss = 0.201356 (0.218 sec/batch), lr: 0.050253
2019-12-27 18:19:06.957768: step 102040/136300 (epoch 75/100), loss = 0.187632 (0.241 sec/batch), lr: 0.050253
2019-12-27 18:19:11.805902: step 102060/136300 (epoch 75/100), loss = 0.196968 (0.226 sec/batch), lr: 0.050253
2019-12-27 18:19:18.196400: step 102080/136300 (epoch 75/100), loss = 0.212232 (0.249 sec/batch), lr: 0.050253
2019-12-27 18:19:23.107113: step 102100/136300 (epoch 75/100), loss = 0.165497 (0.237 sec/batch), lr: 0.050253
2019-12-27 18:19:27.978127: step 102120/136300 (epoch 75/100), loss = 0.236094 (0.237 sec/batch), lr: 0.050253
2019-12-27 18:19:32.688198: step 102140/136300 (epoch 75/100), loss = 0.198179 (0.227 sec/batch), lr: 0.050253
2019-12-27 18:19:37.523211: step 102160/136300 (epoch 75/100), loss = 0.178651 (0.212 sec/batch), lr: 0.050253
2019-12-27 18:19:42.281754: step 102180/136300 (epoch 75/100), loss = 0.194192 (0.233 sec/batch), lr: 0.050253
2019-12-27 18:19:47.215882: step 102200/136300 (epoch 75/100), loss = 0.133236 (0.243 sec/batch), lr: 0.050253
2019-12-27 18:19:52.139415: step 102220/136300 (epoch 75/100), loss = 0.213609 (0.241 sec/batch), lr: 0.050253
Evaluating on dev set...
Precision (micro): 71.441%
   Recall (micro): 62.859%
       F1 (micro): 66.875%
epoch 75: train_loss = 0.222974, dev_loss = 0.452551, dev_f1 = 0.6688
model saved to ./saved_models/01/checkpoint_epoch_75.pt

2019-12-27 18:20:30.448088: step 102240/136300 (epoch 76/100), loss = 0.167504 (0.233 sec/batch), lr: 0.045228
2019-12-27 18:20:36.452773: step 102260/136300 (epoch 76/100), loss = 0.178046 (0.201 sec/batch), lr: 0.045228
2019-12-27 18:20:41.306494: step 102280/136300 (epoch 76/100), loss = 0.248608 (0.238 sec/batch), lr: 0.045228
2019-12-27 18:20:46.097458: step 102300/136300 (epoch 76/100), loss = 0.201684 (0.250 sec/batch), lr: 0.045228
2019-12-27 18:20:50.989414: step 102320/136300 (epoch 76/100), loss = 0.138499 (0.213 sec/batch), lr: 0.045228
2019-12-27 18:20:55.729088: step 102340/136300 (epoch 76/100), loss = 0.185314 (0.205 sec/batch), lr: 0.045228
2019-12-27 18:21:00.537180: step 102360/136300 (epoch 76/100), loss = 0.201532 (0.215 sec/batch), lr: 0.045228
2019-12-27 18:21:05.363475: step 102380/136300 (epoch 76/100), loss = 0.330246 (0.245 sec/batch), lr: 0.045228
2019-12-27 18:21:10.233292: step 102400/136300 (epoch 76/100), loss = 0.196331 (0.229 sec/batch), lr: 0.045228
2019-12-27 18:21:15.102376: step 102420/136300 (epoch 76/100), loss = 0.181646 (0.239 sec/batch), lr: 0.045228
2019-12-27 18:21:21.269273: step 102440/136300 (epoch 76/100), loss = 0.143523 (0.182 sec/batch), lr: 0.045228
2019-12-27 18:21:26.006421: step 102460/136300 (epoch 76/100), loss = 0.315882 (0.227 sec/batch), lr: 0.045228
2019-12-27 18:21:30.838505: step 102480/136300 (epoch 76/100), loss = 0.249614 (0.240 sec/batch), lr: 0.045228
2019-12-27 18:21:35.702653: step 102500/136300 (epoch 76/100), loss = 0.201202 (0.215 sec/batch), lr: 0.045228
2019-12-27 18:21:40.527992: step 102520/136300 (epoch 76/100), loss = 0.265303 (0.200 sec/batch), lr: 0.045228
2019-12-27 18:21:45.258726: step 102540/136300 (epoch 76/100), loss = 0.249801 (0.203 sec/batch), lr: 0.045228
2019-12-27 18:21:50.113028: step 102560/136300 (epoch 76/100), loss = 0.197060 (0.241 sec/batch), lr: 0.045228
2019-12-27 18:21:54.938565: step 102580/136300 (epoch 76/100), loss = 0.127434 (0.245 sec/batch), lr: 0.045228
2019-12-27 18:21:59.677987: step 102600/136300 (epoch 76/100), loss = 0.144623 (0.208 sec/batch), lr: 0.045228
2019-12-27 18:22:04.469431: step 102620/136300 (epoch 76/100), loss = 0.238816 (0.213 sec/batch), lr: 0.045228
2019-12-27 18:22:10.655864: step 102640/136300 (epoch 76/100), loss = 0.119812 (0.222 sec/batch), lr: 0.045228
2019-12-27 18:22:15.442604: step 102660/136300 (epoch 76/100), loss = 0.207898 (0.214 sec/batch), lr: 0.045228
2019-12-27 18:22:20.355805: step 102680/136300 (epoch 76/100), loss = 0.065754 (0.234 sec/batch), lr: 0.045228
2019-12-27 18:22:25.163539: step 102700/136300 (epoch 76/100), loss = 0.270931 (0.224 sec/batch), lr: 0.045228
2019-12-27 18:22:30.018818: step 102720/136300 (epoch 76/100), loss = 0.277777 (0.232 sec/batch), lr: 0.045228
2019-12-27 18:22:34.916873: step 102740/136300 (epoch 76/100), loss = 0.189211 (0.233 sec/batch), lr: 0.045228
2019-12-27 18:22:39.797158: step 102760/136300 (epoch 76/100), loss = 0.239907 (0.241 sec/batch), lr: 0.045228
2019-12-27 18:22:44.616923: step 102780/136300 (epoch 76/100), loss = 0.203603 (0.244 sec/batch), lr: 0.045228
2019-12-27 18:22:49.469283: step 102800/136300 (epoch 76/100), loss = 0.115111 (0.229 sec/batch), lr: 0.045228
2019-12-27 18:22:54.210042: step 102820/136300 (epoch 76/100), loss = 0.399728 (0.174 sec/batch), lr: 0.045228
2019-12-27 18:23:00.269881: step 102840/136300 (epoch 76/100), loss = 0.184321 (0.227 sec/batch), lr: 0.045228
2019-12-27 18:23:05.092438: step 102860/136300 (epoch 76/100), loss = 0.170839 (0.241 sec/batch), lr: 0.045228
2019-12-27 18:23:09.839055: step 102880/136300 (epoch 76/100), loss = 0.100308 (0.231 sec/batch), lr: 0.045228
2019-12-27 18:23:14.663906: step 102900/136300 (epoch 76/100), loss = 0.206868 (0.231 sec/batch), lr: 0.045228
2019-12-27 18:23:19.610268: step 102920/136300 (epoch 76/100), loss = 0.087785 (0.251 sec/batch), lr: 0.045228
2019-12-27 18:23:24.478876: step 102940/136300 (epoch 76/100), loss = 0.132356 (0.241 sec/batch), lr: 0.045228
2019-12-27 18:23:29.415439: step 102960/136300 (epoch 76/100), loss = 0.104674 (0.241 sec/batch), lr: 0.045228
2019-12-27 18:23:34.233706: step 102980/136300 (epoch 76/100), loss = 0.142528 (0.204 sec/batch), lr: 0.045228
2019-12-27 18:23:39.028131: step 103000/136300 (epoch 76/100), loss = 0.484091 (0.248 sec/batch), lr: 0.045228
2019-12-27 18:23:43.934497: step 103020/136300 (epoch 76/100), loss = 0.168139 (0.235 sec/batch), lr: 0.045228
2019-12-27 18:23:50.181186: step 103040/136300 (epoch 76/100), loss = 0.152392 (0.232 sec/batch), lr: 0.045228
2019-12-27 18:23:54.973173: step 103060/136300 (epoch 76/100), loss = 0.234313 (0.237 sec/batch), lr: 0.045228
2019-12-27 18:23:59.898491: step 103080/136300 (epoch 76/100), loss = 0.258110 (0.233 sec/batch), lr: 0.045228
2019-12-27 18:24:04.599616: step 103100/136300 (epoch 76/100), loss = 0.190013 (0.249 sec/batch), lr: 0.045228
2019-12-27 18:24:09.476787: step 103120/136300 (epoch 76/100), loss = 0.137652 (0.242 sec/batch), lr: 0.045228
2019-12-27 18:24:14.422747: step 103140/136300 (epoch 76/100), loss = 0.310622 (0.248 sec/batch), lr: 0.045228
2019-12-27 18:24:19.187390: step 103160/136300 (epoch 76/100), loss = 0.129688 (0.217 sec/batch), lr: 0.045228
2019-12-27 18:24:24.163486: step 103180/136300 (epoch 76/100), loss = 0.282102 (0.230 sec/batch), lr: 0.045228
2019-12-27 18:24:29.043848: step 103200/136300 (epoch 76/100), loss = 0.188114 (0.235 sec/batch), lr: 0.045228
2019-12-27 18:24:33.878240: step 103220/136300 (epoch 76/100), loss = 0.198036 (0.206 sec/batch), lr: 0.045228
2019-12-27 18:24:40.118878: step 103240/136300 (epoch 76/100), loss = 0.254663 (0.236 sec/batch), lr: 0.045228
2019-12-27 18:24:44.984988: step 103260/136300 (epoch 76/100), loss = 0.231124 (0.241 sec/batch), lr: 0.045228
2019-12-27 18:24:49.612450: step 103280/136300 (epoch 76/100), loss = 0.183040 (0.230 sec/batch), lr: 0.045228
2019-12-27 18:24:54.367017: step 103300/136300 (epoch 76/100), loss = 0.259790 (0.221 sec/batch), lr: 0.045228
2019-12-27 18:24:59.209747: step 103320/136300 (epoch 76/100), loss = 0.227808 (0.237 sec/batch), lr: 0.045228
2019-12-27 18:25:04.042062: step 103340/136300 (epoch 76/100), loss = 0.134656 (0.215 sec/batch), lr: 0.045228
2019-12-27 18:25:08.946598: step 103360/136300 (epoch 76/100), loss = 0.118819 (0.224 sec/batch), lr: 0.045228
2019-12-27 18:25:13.708115: step 103380/136300 (epoch 76/100), loss = 0.273963 (0.227 sec/batch), lr: 0.045228
2019-12-27 18:25:18.399148: step 103400/136300 (epoch 76/100), loss = 0.141309 (0.225 sec/batch), lr: 0.045228
2019-12-27 18:25:23.259372: step 103420/136300 (epoch 76/100), loss = 0.291644 (0.249 sec/batch), lr: 0.045228
2019-12-27 18:25:29.505987: step 103440/136300 (epoch 76/100), loss = 0.148051 (0.228 sec/batch), lr: 0.045228
2019-12-27 18:25:34.429483: step 103460/136300 (epoch 76/100), loss = 0.199507 (0.228 sec/batch), lr: 0.045228
2019-12-27 18:25:39.316110: step 103480/136300 (epoch 76/100), loss = 0.291231 (0.215 sec/batch), lr: 0.045228
2019-12-27 18:25:43.962193: step 103500/136300 (epoch 76/100), loss = 0.155150 (0.178 sec/batch), lr: 0.045228
2019-12-27 18:25:48.814054: step 103520/136300 (epoch 76/100), loss = 0.439957 (0.237 sec/batch), lr: 0.045228
2019-12-27 18:25:53.639782: step 103540/136300 (epoch 76/100), loss = 0.246123 (0.246 sec/batch), lr: 0.045228
2019-12-27 18:25:58.549240: step 103560/136300 (epoch 76/100), loss = 0.324897 (0.240 sec/batch), lr: 0.045228
2019-12-27 18:26:03.413990: step 103580/136300 (epoch 76/100), loss = 0.219794 (0.238 sec/batch), lr: 0.045228
Evaluating on dev set...
Precision (micro): 70.319%
   Recall (micro): 63.760%
       F1 (micro): 66.879%
epoch 76: train_loss = 0.220693, dev_loss = 0.457387, dev_f1 = 0.6688
model saved to ./saved_models/01/checkpoint_epoch_76.pt

2019-12-27 18:26:41.714285: step 103600/136300 (epoch 77/100), loss = 0.234125 (0.233 sec/batch), lr: 0.045228
2019-12-27 18:26:47.759230: step 103620/136300 (epoch 77/100), loss = 0.160181 (0.216 sec/batch), lr: 0.045228
2019-12-27 18:26:52.599944: step 103640/136300 (epoch 77/100), loss = 0.349612 (0.217 sec/batch), lr: 0.045228
2019-12-27 18:26:57.347145: step 103660/136300 (epoch 77/100), loss = 0.131267 (0.247 sec/batch), lr: 0.045228
2019-12-27 18:27:02.263117: step 103680/136300 (epoch 77/100), loss = 0.240002 (0.202 sec/batch), lr: 0.045228
2019-12-27 18:27:07.005222: step 103700/136300 (epoch 77/100), loss = 0.143970 (0.221 sec/batch), lr: 0.045228
2019-12-27 18:27:11.813910: step 103720/136300 (epoch 77/100), loss = 0.221741 (0.230 sec/batch), lr: 0.045228
2019-12-27 18:27:16.637852: step 103740/136300 (epoch 77/100), loss = 0.287626 (0.207 sec/batch), lr: 0.045228
2019-12-27 18:27:21.475461: step 103760/136300 (epoch 77/100), loss = 0.339620 (0.207 sec/batch), lr: 0.045228
2019-12-27 18:27:26.401537: step 103780/136300 (epoch 77/100), loss = 0.194219 (0.221 sec/batch), lr: 0.045228
2019-12-27 18:27:31.227354: step 103800/136300 (epoch 77/100), loss = 0.115399 (0.232 sec/batch), lr: 0.045228
2019-12-27 18:27:37.354344: step 103820/136300 (epoch 77/100), loss = 0.247649 (0.240 sec/batch), lr: 0.045228
2019-12-27 18:27:42.094175: step 103840/136300 (epoch 77/100), loss = 0.248773 (0.204 sec/batch), lr: 0.045228
2019-12-27 18:27:47.021989: step 103860/136300 (epoch 77/100), loss = 0.192185 (0.236 sec/batch), lr: 0.045228
2019-12-27 18:27:51.831686: step 103880/136300 (epoch 77/100), loss = 0.209872 (0.217 sec/batch), lr: 0.045228
2019-12-27 18:27:56.594359: step 103900/136300 (epoch 77/100), loss = 0.336691 (0.231 sec/batch), lr: 0.045228
2019-12-27 18:28:01.384390: step 103920/136300 (epoch 77/100), loss = 0.197279 (0.246 sec/batch), lr: 0.045228
2019-12-27 18:28:06.226548: step 103940/136300 (epoch 77/100), loss = 0.224510 (0.234 sec/batch), lr: 0.045228
2019-12-27 18:28:11.045890: step 103960/136300 (epoch 77/100), loss = 0.200529 (0.197 sec/batch), lr: 0.045228
2019-12-27 18:28:15.785637: step 103980/136300 (epoch 77/100), loss = 0.119879 (0.223 sec/batch), lr: 0.045228
2019-12-27 18:28:21.843543: step 104000/136300 (epoch 77/100), loss = 0.123322 (0.225 sec/batch), lr: 0.045228
2019-12-27 18:28:26.677676: step 104020/136300 (epoch 77/100), loss = 0.208392 (0.248 sec/batch), lr: 0.045228
2019-12-27 18:28:31.591946: step 104040/136300 (epoch 77/100), loss = 0.089014 (0.227 sec/batch), lr: 0.045228
2019-12-27 18:28:36.441055: step 104060/136300 (epoch 77/100), loss = 0.463965 (0.228 sec/batch), lr: 0.045228
2019-12-27 18:28:41.256951: step 104080/136300 (epoch 77/100), loss = 0.141720 (0.227 sec/batch), lr: 0.045228
2019-12-27 18:28:46.133314: step 104100/136300 (epoch 77/100), loss = 0.190536 (0.240 sec/batch), lr: 0.045228
2019-12-27 18:28:51.020714: step 104120/136300 (epoch 77/100), loss = 0.228590 (0.238 sec/batch), lr: 0.045228
2019-12-27 18:28:55.890094: step 104140/136300 (epoch 77/100), loss = 0.139027 (0.241 sec/batch), lr: 0.045228
2019-12-27 18:29:00.707628: step 104160/136300 (epoch 77/100), loss = 0.148143 (0.234 sec/batch), lr: 0.045228
2019-12-27 18:29:05.522977: step 104180/136300 (epoch 77/100), loss = 0.151845 (0.179 sec/batch), lr: 0.045228
2019-12-27 18:29:11.600000: step 104200/136300 (epoch 77/100), loss = 0.197790 (0.221 sec/batch), lr: 0.045228
2019-12-27 18:29:16.405482: step 104220/136300 (epoch 77/100), loss = 0.241331 (0.237 sec/batch), lr: 0.045228
2019-12-27 18:29:21.162684: step 104240/136300 (epoch 77/100), loss = 0.203255 (0.221 sec/batch), lr: 0.045228
2019-12-27 18:29:26.002642: step 104260/136300 (epoch 77/100), loss = 0.144055 (0.232 sec/batch), lr: 0.045228
2019-12-27 18:29:30.953153: step 104280/136300 (epoch 77/100), loss = 0.230947 (0.237 sec/batch), lr: 0.045228
2019-12-27 18:29:35.834543: step 104300/136300 (epoch 77/100), loss = 0.383049 (0.218 sec/batch), lr: 0.045228
2019-12-27 18:29:40.767235: step 104320/136300 (epoch 77/100), loss = 0.251810 (0.240 sec/batch), lr: 0.045228
2019-12-27 18:29:45.625064: step 104340/136300 (epoch 77/100), loss = 0.243631 (0.233 sec/batch), lr: 0.045228
2019-12-27 18:29:50.398628: step 104360/136300 (epoch 77/100), loss = 0.290105 (0.229 sec/batch), lr: 0.045228
2019-12-27 18:29:55.307856: step 104380/136300 (epoch 77/100), loss = 0.172225 (0.233 sec/batch), lr: 0.045228
2019-12-27 18:30:01.484961: step 104400/136300 (epoch 77/100), loss = 0.202398 (0.216 sec/batch), lr: 0.045228
2019-12-27 18:30:06.270968: step 104420/136300 (epoch 77/100), loss = 0.274026 (0.245 sec/batch), lr: 0.045228
2019-12-27 18:30:11.204196: step 104440/136300 (epoch 77/100), loss = 0.241843 (0.177 sec/batch), lr: 0.045228
2019-12-27 18:30:15.864953: step 104460/136300 (epoch 77/100), loss = 0.216122 (0.228 sec/batch), lr: 0.045228
2019-12-27 18:30:20.774470: step 104480/136300 (epoch 77/100), loss = 0.216704 (0.234 sec/batch), lr: 0.045228
2019-12-27 18:30:25.706187: step 104500/136300 (epoch 77/100), loss = 0.166725 (0.218 sec/batch), lr: 0.045228
2019-12-27 18:30:30.517183: step 104520/136300 (epoch 77/100), loss = 0.167570 (0.239 sec/batch), lr: 0.045228
2019-12-27 18:30:35.463512: step 104540/136300 (epoch 77/100), loss = 0.173046 (0.242 sec/batch), lr: 0.045228
2019-12-27 18:30:40.376536: step 104560/136300 (epoch 77/100), loss = 0.293916 (0.225 sec/batch), lr: 0.045228
2019-12-27 18:30:45.211907: step 104580/136300 (epoch 77/100), loss = 0.141231 (0.226 sec/batch), lr: 0.045228
2019-12-27 18:30:51.466221: step 104600/136300 (epoch 77/100), loss = 0.137458 (0.216 sec/batch), lr: 0.045228
2019-12-27 18:30:56.362019: step 104620/136300 (epoch 77/100), loss = 0.110280 (0.201 sec/batch), lr: 0.045228
2019-12-27 18:31:01.003147: step 104640/136300 (epoch 77/100), loss = 0.160428 (0.204 sec/batch), lr: 0.045228
2019-12-27 18:31:05.723053: step 104660/136300 (epoch 77/100), loss = 0.193736 (0.230 sec/batch), lr: 0.045228
2019-12-27 18:31:10.564579: step 104680/136300 (epoch 77/100), loss = 0.219828 (0.238 sec/batch), lr: 0.045228
2019-12-27 18:31:15.411225: step 104700/136300 (epoch 77/100), loss = 0.139234 (0.202 sec/batch), lr: 0.045228
2019-12-27 18:31:20.325004: step 104720/136300 (epoch 77/100), loss = 0.106984 (0.236 sec/batch), lr: 0.045228
2019-12-27 18:31:25.072946: step 104740/136300 (epoch 77/100), loss = 0.087974 (0.237 sec/batch), lr: 0.045228
2019-12-27 18:31:29.765653: step 104760/136300 (epoch 77/100), loss = 0.163812 (0.177 sec/batch), lr: 0.045228
2019-12-27 18:31:34.593937: step 104780/136300 (epoch 77/100), loss = 0.204778 (0.239 sec/batch), lr: 0.045228
2019-12-27 18:31:40.807344: step 104800/136300 (epoch 77/100), loss = 0.137745 (0.246 sec/batch), lr: 0.045228
2019-12-27 18:31:45.716376: step 104820/136300 (epoch 77/100), loss = 0.193192 (0.233 sec/batch), lr: 0.045228
2019-12-27 18:31:50.654691: step 104840/136300 (epoch 77/100), loss = 0.160583 (0.225 sec/batch), lr: 0.045228
2019-12-27 18:31:55.264497: step 104860/136300 (epoch 77/100), loss = 0.256452 (0.221 sec/batch), lr: 0.045228
2019-12-27 18:32:00.114475: step 104880/136300 (epoch 77/100), loss = 0.159451 (0.233 sec/batch), lr: 0.045228
2019-12-27 18:32:04.898043: step 104900/136300 (epoch 77/100), loss = 0.197002 (0.235 sec/batch), lr: 0.045228
2019-12-27 18:32:09.820865: step 104920/136300 (epoch 77/100), loss = 0.286079 (0.204 sec/batch), lr: 0.045228
2019-12-27 18:32:14.687476: step 104940/136300 (epoch 77/100), loss = 0.100377 (0.233 sec/batch), lr: 0.045228
Evaluating on dev set...
Precision (micro): 71.363%
   Recall (micro): 63.079%
       F1 (micro): 66.966%
epoch 77: train_loss = 0.219877, dev_loss = 0.456899, dev_f1 = 0.6697
model saved to ./saved_models/01/checkpoint_epoch_77.pt

2019-12-27 18:32:53.107257: step 104960/136300 (epoch 78/100), loss = 0.210254 (0.219 sec/batch), lr: 0.045228
2019-12-27 18:32:59.099066: step 104980/136300 (epoch 78/100), loss = 0.076863 (0.239 sec/batch), lr: 0.045228
2019-12-27 18:33:03.930920: step 105000/136300 (epoch 78/100), loss = 0.236500 (0.216 sec/batch), lr: 0.045228
2019-12-27 18:33:08.643077: step 105020/136300 (epoch 78/100), loss = 0.143354 (0.233 sec/batch), lr: 0.045228
2019-12-27 18:33:13.593238: step 105040/136300 (epoch 78/100), loss = 0.306770 (0.244 sec/batch), lr: 0.045228
2019-12-27 18:33:18.426512: step 105060/136300 (epoch 78/100), loss = 0.410971 (0.244 sec/batch), lr: 0.045228
2019-12-27 18:33:23.144454: step 105080/136300 (epoch 78/100), loss = 0.226374 (0.220 sec/batch), lr: 0.045228
2019-12-27 18:33:28.046449: step 105100/136300 (epoch 78/100), loss = 0.205165 (0.235 sec/batch), lr: 0.045228
2019-12-27 18:33:32.838125: step 105120/136300 (epoch 78/100), loss = 0.265052 (0.233 sec/batch), lr: 0.045228
2019-12-27 18:33:37.745665: step 105140/136300 (epoch 78/100), loss = 0.129437 (0.247 sec/batch), lr: 0.045228
2019-12-27 18:33:42.531839: step 105160/136300 (epoch 78/100), loss = 0.201811 (0.176 sec/batch), lr: 0.045228
2019-12-27 18:33:48.689280: step 105180/136300 (epoch 78/100), loss = 0.208453 (0.214 sec/batch), lr: 0.045228
2019-12-27 18:33:53.533145: step 105200/136300 (epoch 78/100), loss = 0.131540 (0.202 sec/batch), lr: 0.045228
2019-12-27 18:33:58.401869: step 105220/136300 (epoch 78/100), loss = 0.200432 (0.238 sec/batch), lr: 0.045228
2019-12-27 18:34:03.207894: step 105240/136300 (epoch 78/100), loss = 0.231576 (0.236 sec/batch), lr: 0.045228
2019-12-27 18:34:07.954115: step 105260/136300 (epoch 78/100), loss = 0.386394 (0.166 sec/batch), lr: 0.045228
2019-12-27 18:34:12.776037: step 105280/136300 (epoch 78/100), loss = 0.189356 (0.236 sec/batch), lr: 0.045228
2019-12-27 18:34:17.599126: step 105300/136300 (epoch 78/100), loss = 0.166443 (0.249 sec/batch), lr: 0.045228
2019-12-27 18:34:22.504342: step 105320/136300 (epoch 78/100), loss = 0.275117 (0.242 sec/batch), lr: 0.045228
2019-12-27 18:34:27.208710: step 105340/136300 (epoch 78/100), loss = 0.410443 (0.219 sec/batch), lr: 0.045228
2019-12-27 18:34:33.236940: step 105360/136300 (epoch 78/100), loss = 0.150539 (1.541 sec/batch), lr: 0.045228
2019-12-27 18:34:37.997338: step 105380/136300 (epoch 78/100), loss = 0.149799 (0.233 sec/batch), lr: 0.045228
2019-12-27 18:34:42.945695: step 105400/136300 (epoch 78/100), loss = 0.192112 (0.229 sec/batch), lr: 0.045228
2019-12-27 18:34:47.809326: step 105420/136300 (epoch 78/100), loss = 0.287381 (0.237 sec/batch), lr: 0.045228
2019-12-27 18:34:52.598091: step 105440/136300 (epoch 78/100), loss = 0.256992 (0.200 sec/batch), lr: 0.045228
2019-12-27 18:34:57.478136: step 105460/136300 (epoch 78/100), loss = 0.393081 (0.212 sec/batch), lr: 0.045228
2019-12-27 18:35:02.349894: step 105480/136300 (epoch 78/100), loss = 0.249162 (0.240 sec/batch), lr: 0.045228
2019-12-27 18:35:07.222148: step 105500/136300 (epoch 78/100), loss = 0.260046 (0.182 sec/batch), lr: 0.045228
2019-12-27 18:35:12.082565: step 105520/136300 (epoch 78/100), loss = 0.226881 (0.243 sec/batch), lr: 0.045228
2019-12-27 18:35:16.897199: step 105540/136300 (epoch 78/100), loss = 0.318869 (0.233 sec/batch), lr: 0.045228
2019-12-27 18:35:22.920296: step 105560/136300 (epoch 78/100), loss = 0.192003 (0.175 sec/batch), lr: 0.045228
2019-12-27 18:35:27.745281: step 105580/136300 (epoch 78/100), loss = 0.281529 (0.216 sec/batch), lr: 0.045228
2019-12-27 18:35:32.510928: step 105600/136300 (epoch 78/100), loss = 0.209169 (0.222 sec/batch), lr: 0.045228
2019-12-27 18:35:37.357275: step 105620/136300 (epoch 78/100), loss = 0.150774 (0.237 sec/batch), lr: 0.045228
2019-12-27 18:35:42.270482: step 105640/136300 (epoch 78/100), loss = 0.113430 (0.213 sec/batch), lr: 0.045228
2019-12-27 18:35:47.181878: step 105660/136300 (epoch 78/100), loss = 0.329380 (0.222 sec/batch), lr: 0.045228
2019-12-27 18:35:52.097925: step 105680/136300 (epoch 78/100), loss = 0.208419 (0.220 sec/batch), lr: 0.045228
2019-12-27 18:35:56.997127: step 105700/136300 (epoch 78/100), loss = 0.153611 (0.235 sec/batch), lr: 0.045228
2019-12-27 18:36:01.701746: step 105720/136300 (epoch 78/100), loss = 0.096934 (0.231 sec/batch), lr: 0.045228
2019-12-27 18:36:06.607996: step 105740/136300 (epoch 78/100), loss = 0.275509 (0.205 sec/batch), lr: 0.045228
2019-12-27 18:36:12.811808: step 105760/136300 (epoch 78/100), loss = 0.179174 (0.182 sec/batch), lr: 0.045228
2019-12-27 18:36:17.637188: step 105780/136300 (epoch 78/100), loss = 0.167886 (0.248 sec/batch), lr: 0.045228
2019-12-27 18:36:22.575881: step 105800/136300 (epoch 78/100), loss = 0.297863 (0.237 sec/batch), lr: 0.045228
2019-12-27 18:36:27.256725: step 105820/136300 (epoch 78/100), loss = 0.164474 (0.180 sec/batch), lr: 0.045228
2019-12-27 18:36:32.110585: step 105840/136300 (epoch 78/100), loss = 0.477899 (0.231 sec/batch), lr: 0.045228
2019-12-27 18:36:37.037878: step 105860/136300 (epoch 78/100), loss = 0.277987 (0.244 sec/batch), lr: 0.045228
2019-12-27 18:36:41.842306: step 105880/136300 (epoch 78/100), loss = 0.244941 (0.225 sec/batch), lr: 0.045228
2019-12-27 18:36:46.757187: step 105900/136300 (epoch 78/100), loss = 0.248602 (0.248 sec/batch), lr: 0.045228
2019-12-27 18:36:51.722551: step 105920/136300 (epoch 78/100), loss = 0.232483 (0.232 sec/batch), lr: 0.045228
2019-12-27 18:36:56.519089: step 105940/136300 (epoch 78/100), loss = 0.199036 (0.225 sec/batch), lr: 0.045228
2019-12-27 18:37:02.768888: step 105960/136300 (epoch 78/100), loss = 0.266557 (0.242 sec/batch), lr: 0.045228
2019-12-27 18:37:07.701921: step 105980/136300 (epoch 78/100), loss = 0.326919 (0.236 sec/batch), lr: 0.045228
2019-12-27 18:37:12.325572: step 106000/136300 (epoch 78/100), loss = 0.188552 (0.234 sec/batch), lr: 0.045228
2019-12-27 18:37:17.064077: step 106020/136300 (epoch 78/100), loss = 0.111808 (0.240 sec/batch), lr: 0.045228
2019-12-27 18:37:21.927856: step 106040/136300 (epoch 78/100), loss = 0.196287 (0.213 sec/batch), lr: 0.045228
2019-12-27 18:37:26.730868: step 106060/136300 (epoch 78/100), loss = 0.297567 (0.228 sec/batch), lr: 0.045228
2019-12-27 18:37:31.650374: step 106080/136300 (epoch 78/100), loss = 0.149507 (0.233 sec/batch), lr: 0.045228
2019-12-27 18:37:36.430065: step 106100/136300 (epoch 78/100), loss = 0.599196 (0.239 sec/batch), lr: 0.045228
2019-12-27 18:37:41.134670: step 106120/136300 (epoch 78/100), loss = 0.117816 (0.202 sec/batch), lr: 0.045228
2019-12-27 18:37:45.914168: step 106140/136300 (epoch 78/100), loss = 0.221415 (0.209 sec/batch), lr: 0.045228
2019-12-27 18:37:52.181410: step 106160/136300 (epoch 78/100), loss = 0.294716 (0.241 sec/batch), lr: 0.045228
2019-12-27 18:37:57.118131: step 106180/136300 (epoch 78/100), loss = 0.238100 (0.220 sec/batch), lr: 0.045228
2019-12-27 18:38:02.021670: step 106200/136300 (epoch 78/100), loss = 0.218422 (0.239 sec/batch), lr: 0.045228
2019-12-27 18:38:06.696199: step 106220/136300 (epoch 78/100), loss = 0.083645 (0.205 sec/batch), lr: 0.045228
2019-12-27 18:38:11.523973: step 106240/136300 (epoch 78/100), loss = 0.106303 (0.231 sec/batch), lr: 0.045228
2019-12-27 18:38:16.273080: step 106260/136300 (epoch 78/100), loss = 0.223996 (0.244 sec/batch), lr: 0.045228
2019-12-27 18:38:21.202181: step 106280/136300 (epoch 78/100), loss = 0.126663 (0.210 sec/batch), lr: 0.045228
2019-12-27 18:38:26.070603: step 106300/136300 (epoch 78/100), loss = 0.167383 (0.231 sec/batch), lr: 0.045228
Evaluating on dev set...
Precision (micro): 71.414%
   Recall (micro): 62.730%
       F1 (micro): 66.791%
epoch 78: train_loss = 0.219502, dev_loss = 0.453284, dev_f1 = 0.6679
model saved to ./saved_models/01/checkpoint_epoch_78.pt

2019-12-27 18:39:04.514145: step 106320/136300 (epoch 79/100), loss = 0.182166 (0.245 sec/batch), lr: 0.040705
2019-12-27 18:39:10.495596: step 106340/136300 (epoch 79/100), loss = 0.163244 (0.211 sec/batch), lr: 0.040705
2019-12-27 18:39:15.335169: step 106360/136300 (epoch 79/100), loss = 0.129472 (0.235 sec/batch), lr: 0.040705
2019-12-27 18:39:20.089236: step 106380/136300 (epoch 79/100), loss = 0.150768 (0.228 sec/batch), lr: 0.040705
2019-12-27 18:39:25.013410: step 106400/136300 (epoch 79/100), loss = 0.164206 (0.229 sec/batch), lr: 0.040705
2019-12-27 18:39:29.840289: step 106420/136300 (epoch 79/100), loss = 0.169648 (0.247 sec/batch), lr: 0.040705
2019-12-27 18:39:34.581964: step 106440/136300 (epoch 79/100), loss = 0.249798 (0.207 sec/batch), lr: 0.040705
2019-12-27 18:39:39.453170: step 106460/136300 (epoch 79/100), loss = 0.286178 (0.206 sec/batch), lr: 0.040705
2019-12-27 18:39:44.304419: step 106480/136300 (epoch 79/100), loss = 0.276924 (0.235 sec/batch), lr: 0.040705
2019-12-27 18:39:49.198680: step 106500/136300 (epoch 79/100), loss = 0.264193 (0.224 sec/batch), lr: 0.040705
2019-12-27 18:39:54.019218: step 106520/136300 (epoch 79/100), loss = 0.099833 (0.211 sec/batch), lr: 0.040705
2019-12-27 18:40:00.335622: step 106540/136300 (epoch 79/100), loss = 0.165880 (0.206 sec/batch), lr: 0.040705
2019-12-27 18:40:05.200815: step 106560/136300 (epoch 79/100), loss = 0.077335 (0.239 sec/batch), lr: 0.040705
2019-12-27 18:40:09.993595: step 106580/136300 (epoch 79/100), loss = 0.293487 (0.245 sec/batch), lr: 0.040705
2019-12-27 18:40:14.805896: step 106600/136300 (epoch 79/100), loss = 0.159281 (0.236 sec/batch), lr: 0.040705
2019-12-27 18:40:19.661236: step 106620/136300 (epoch 79/100), loss = 0.243191 (0.236 sec/batch), lr: 0.040705
2019-12-27 18:40:24.401088: step 106640/136300 (epoch 79/100), loss = 0.405232 (0.232 sec/batch), lr: 0.040705
2019-12-27 18:40:29.222485: step 106660/136300 (epoch 79/100), loss = 0.121567 (0.241 sec/batch), lr: 0.040705
2019-12-27 18:40:34.156625: step 106680/136300 (epoch 79/100), loss = 0.221731 (0.247 sec/batch), lr: 0.040705
2019-12-27 18:40:38.887831: step 106700/136300 (epoch 79/100), loss = 0.103513 (0.214 sec/batch), lr: 0.040705
2019-12-27 18:40:43.529839: step 106720/136300 (epoch 79/100), loss = 0.101465 (0.223 sec/batch), lr: 0.040705
2019-12-27 18:40:49.714071: step 106740/136300 (epoch 79/100), loss = 0.332377 (0.232 sec/batch), lr: 0.040705
2019-12-27 18:40:54.662036: step 106760/136300 (epoch 79/100), loss = 0.233493 (0.225 sec/batch), lr: 0.040705
2019-12-27 18:40:59.573902: step 106780/136300 (epoch 79/100), loss = 0.172781 (0.231 sec/batch), lr: 0.040705
2019-12-27 18:41:04.350266: step 106800/136300 (epoch 79/100), loss = 0.150151 (0.179 sec/batch), lr: 0.040705
2019-12-27 18:41:09.222003: step 106820/136300 (epoch 79/100), loss = 0.258385 (0.203 sec/batch), lr: 0.040705
2019-12-27 18:41:14.091590: step 106840/136300 (epoch 79/100), loss = 0.130233 (0.237 sec/batch), lr: 0.040705
2019-12-27 18:41:19.062605: step 106860/136300 (epoch 79/100), loss = 0.209466 (0.228 sec/batch), lr: 0.040705
2019-12-27 18:41:23.865887: step 106880/136300 (epoch 79/100), loss = 0.067857 (0.252 sec/batch), lr: 0.040705
2019-12-27 18:41:28.640998: step 106900/136300 (epoch 79/100), loss = 0.320347 (0.230 sec/batch), lr: 0.040705
2019-12-27 18:41:33.410428: step 106920/136300 (epoch 79/100), loss = 0.409062 (0.201 sec/batch), lr: 0.040705
2019-12-27 18:41:39.610214: step 106940/136300 (epoch 79/100), loss = 0.128474 (0.206 sec/batch), lr: 0.040705
2019-12-27 18:41:44.389176: step 106960/136300 (epoch 79/100), loss = 0.170353 (0.238 sec/batch), lr: 0.040705
2019-12-27 18:41:49.179432: step 106980/136300 (epoch 79/100), loss = 0.365880 (0.231 sec/batch), lr: 0.040705
2019-12-27 18:41:54.077119: step 107000/136300 (epoch 79/100), loss = 0.198247 (0.234 sec/batch), lr: 0.040705
2019-12-27 18:41:58.989251: step 107020/136300 (epoch 79/100), loss = 0.208809 (0.215 sec/batch), lr: 0.040705
2019-12-27 18:42:03.881403: step 107040/136300 (epoch 79/100), loss = 0.240579 (0.228 sec/batch), lr: 0.040705
2019-12-27 18:42:08.819543: step 107060/136300 (epoch 79/100), loss = 0.230271 (0.236 sec/batch), lr: 0.040705
2019-12-27 18:42:13.570682: step 107080/136300 (epoch 79/100), loss = 0.268717 (0.235 sec/batch), lr: 0.040705
2019-12-27 18:42:18.382165: step 107100/136300 (epoch 79/100), loss = 0.182825 (0.251 sec/batch), lr: 0.040705
2019-12-27 18:42:24.578661: step 107120/136300 (epoch 79/100), loss = 0.227866 (0.243 sec/batch), lr: 0.040705
2019-12-27 18:42:29.320824: step 107140/136300 (epoch 79/100), loss = 0.396296 (0.207 sec/batch), lr: 0.040705
2019-12-27 18:42:34.304505: step 107160/136300 (epoch 79/100), loss = 0.255438 (0.243 sec/batch), lr: 0.040705
2019-12-27 18:42:39.038481: step 107180/136300 (epoch 79/100), loss = 0.156094 (0.211 sec/batch), lr: 0.040705
2019-12-27 18:42:43.825971: step 107200/136300 (epoch 79/100), loss = 0.074636 (0.233 sec/batch), lr: 0.040705
2019-12-27 18:42:48.747235: step 107220/136300 (epoch 79/100), loss = 0.230130 (0.244 sec/batch), lr: 0.040705
2019-12-27 18:42:53.669948: step 107240/136300 (epoch 79/100), loss = 0.187438 (0.221 sec/batch), lr: 0.040705
2019-12-27 18:42:58.474984: step 107260/136300 (epoch 79/100), loss = 0.273006 (0.221 sec/batch), lr: 0.040705
2019-12-27 18:43:03.462541: step 107280/136300 (epoch 79/100), loss = 0.260985 (0.233 sec/batch), lr: 0.040705
2019-12-27 18:43:08.326422: step 107300/136300 (epoch 79/100), loss = 0.340855 (0.207 sec/batch), lr: 0.040705
2019-12-27 18:43:14.675609: step 107320/136300 (epoch 79/100), loss = 0.322865 (0.236 sec/batch), lr: 0.040705
2019-12-27 18:43:19.566009: step 107340/136300 (epoch 79/100), loss = 0.159318 (0.236 sec/batch), lr: 0.040705
2019-12-27 18:43:24.200417: step 107360/136300 (epoch 79/100), loss = 0.309206 (0.228 sec/batch), lr: 0.040705
2019-12-27 18:43:28.973069: step 107380/136300 (epoch 79/100), loss = 0.242441 (0.237 sec/batch), lr: 0.040705
2019-12-27 18:43:33.798447: step 107400/136300 (epoch 79/100), loss = 0.136969 (0.208 sec/batch), lr: 0.040705
2019-12-27 18:43:38.608037: step 107420/136300 (epoch 79/100), loss = 0.127445 (0.227 sec/batch), lr: 0.040705
2019-12-27 18:43:43.528989: step 107440/136300 (epoch 79/100), loss = 0.296335 (0.225 sec/batch), lr: 0.040705
2019-12-27 18:43:48.272210: step 107460/136300 (epoch 79/100), loss = 0.243465 (0.204 sec/batch), lr: 0.040705
2019-12-27 18:43:53.048800: step 107480/136300 (epoch 79/100), loss = 0.166576 (0.226 sec/batch), lr: 0.040705
2019-12-27 18:43:57.850591: step 107500/136300 (epoch 79/100), loss = 0.100140 (0.247 sec/batch), lr: 0.040705
2019-12-27 18:44:03.933033: step 107520/136300 (epoch 79/100), loss = 0.264799 (0.245 sec/batch), lr: 0.040705
2019-12-27 18:44:08.863723: step 107540/136300 (epoch 79/100), loss = 0.130558 (0.249 sec/batch), lr: 0.040705
2019-12-27 18:44:13.744856: step 107560/136300 (epoch 79/100), loss = 0.199381 (0.237 sec/batch), lr: 0.040705
2019-12-27 18:44:18.538599: step 107580/136300 (epoch 79/100), loss = 0.232181 (0.217 sec/batch), lr: 0.040705
2019-12-27 18:44:23.282667: step 107600/136300 (epoch 79/100), loss = 0.274368 (0.231 sec/batch), lr: 0.040705
2019-12-27 18:44:28.052217: step 107620/136300 (epoch 79/100), loss = 0.281689 (0.243 sec/batch), lr: 0.040705
2019-12-27 18:44:32.949818: step 107640/136300 (epoch 79/100), loss = 0.227986 (0.246 sec/batch), lr: 0.040705
2019-12-27 18:44:37.856064: step 107660/136300 (epoch 79/100), loss = 0.200093 (0.238 sec/batch), lr: 0.040705
Evaluating on dev set...
Precision (micro): 71.686%
   Recall (micro): 62.877%
       F1 (micro): 66.993%
epoch 79: train_loss = 0.217472, dev_loss = 0.460426, dev_f1 = 0.6699
model saved to ./saved_models/01/checkpoint_epoch_79.pt

2019-12-27 18:45:16.214218: step 107680/136300 (epoch 80/100), loss = 0.294296 (0.215 sec/batch), lr: 0.040705
2019-12-27 18:45:22.474356: step 107700/136300 (epoch 80/100), loss = 0.277223 (0.213 sec/batch), lr: 0.040705
2019-12-27 18:45:27.285733: step 107720/136300 (epoch 80/100), loss = 0.319926 (0.241 sec/batch), lr: 0.040705
2019-12-27 18:45:32.079777: step 107740/136300 (epoch 80/100), loss = 0.178440 (0.203 sec/batch), lr: 0.040705
2019-12-27 18:45:36.974323: step 107760/136300 (epoch 80/100), loss = 0.199404 (0.232 sec/batch), lr: 0.040705
2019-12-27 18:45:41.773427: step 107780/136300 (epoch 80/100), loss = 0.153791 (0.213 sec/batch), lr: 0.040705
2019-12-27 18:45:46.574140: step 107800/136300 (epoch 80/100), loss = 0.181157 (0.232 sec/batch), lr: 0.040705
2019-12-27 18:45:51.444275: step 107820/136300 (epoch 80/100), loss = 0.271991 (0.239 sec/batch), lr: 0.040705
2019-12-27 18:45:56.256952: step 107840/136300 (epoch 80/100), loss = 0.266998 (0.224 sec/batch), lr: 0.040705
2019-12-27 18:46:01.152954: step 107860/136300 (epoch 80/100), loss = 0.108688 (0.221 sec/batch), lr: 0.040705
2019-12-27 18:46:06.029979: step 107880/136300 (epoch 80/100), loss = 0.365906 (0.249 sec/batch), lr: 0.040705
2019-12-27 18:46:12.146294: step 107900/136300 (epoch 80/100), loss = 0.321257 (0.215 sec/batch), lr: 0.040705
2019-12-27 18:46:16.942261: step 107920/136300 (epoch 80/100), loss = 0.231666 (0.240 sec/batch), lr: 0.040705
2019-12-27 18:46:21.716156: step 107940/136300 (epoch 80/100), loss = 0.146573 (0.203 sec/batch), lr: 0.040705
2019-12-27 18:46:26.560474: step 107960/136300 (epoch 80/100), loss = 0.180769 (0.240 sec/batch), lr: 0.040705
2019-12-27 18:46:31.414270: step 107980/136300 (epoch 80/100), loss = 0.367887 (0.232 sec/batch), lr: 0.040705
2019-12-27 18:46:36.153543: step 108000/136300 (epoch 80/100), loss = 0.245566 (0.185 sec/batch), lr: 0.040705
2019-12-27 18:46:40.932885: step 108020/136300 (epoch 80/100), loss = 0.163338 (0.216 sec/batch), lr: 0.040705
2019-12-27 18:46:45.835403: step 108040/136300 (epoch 80/100), loss = 0.282858 (0.219 sec/batch), lr: 0.040705
2019-12-27 18:46:50.596248: step 108060/136300 (epoch 80/100), loss = 0.286846 (0.233 sec/batch), lr: 0.040705
2019-12-27 18:46:55.312902: step 108080/136300 (epoch 80/100), loss = 0.392343 (0.216 sec/batch), lr: 0.040705
2019-12-27 18:47:01.397357: step 108100/136300 (epoch 80/100), loss = 0.318463 (0.215 sec/batch), lr: 0.040705
2019-12-27 18:47:06.317319: step 108120/136300 (epoch 80/100), loss = 0.209952 (0.244 sec/batch), lr: 0.040705
2019-12-27 18:47:11.199115: step 108140/136300 (epoch 80/100), loss = 0.273381 (0.222 sec/batch), lr: 0.040705
2019-12-27 18:47:16.018521: step 108160/136300 (epoch 80/100), loss = 0.148683 (0.240 sec/batch), lr: 0.040705
2019-12-27 18:47:20.849708: step 108180/136300 (epoch 80/100), loss = 0.204904 (0.236 sec/batch), lr: 0.040705
2019-12-27 18:47:25.726136: step 108200/136300 (epoch 80/100), loss = 0.297428 (0.236 sec/batch), lr: 0.040705
2019-12-27 18:47:30.696882: step 108220/136300 (epoch 80/100), loss = 0.220629 (0.224 sec/batch), lr: 0.040705
2019-12-27 18:47:35.467100: step 108240/136300 (epoch 80/100), loss = 0.193710 (0.226 sec/batch), lr: 0.040705
2019-12-27 18:47:40.239832: step 108260/136300 (epoch 80/100), loss = 0.175129 (0.178 sec/batch), lr: 0.040705
2019-12-27 18:47:45.005472: step 108280/136300 (epoch 80/100), loss = 0.195929 (0.236 sec/batch), lr: 0.040705
2019-12-27 18:47:51.223550: step 108300/136300 (epoch 80/100), loss = 0.128989 (0.233 sec/batch), lr: 0.040705
2019-12-27 18:47:56.008157: step 108320/136300 (epoch 80/100), loss = 0.253709 (0.221 sec/batch), lr: 0.040705
2019-12-27 18:48:00.799379: step 108340/136300 (epoch 80/100), loss = 0.161347 (0.220 sec/batch), lr: 0.040705
2019-12-27 18:48:05.667371: step 108360/136300 (epoch 80/100), loss = 0.100140 (0.230 sec/batch), lr: 0.040705
2019-12-27 18:48:10.606398: step 108380/136300 (epoch 80/100), loss = 0.213024 (0.241 sec/batch), lr: 0.040705
2019-12-27 18:48:15.537894: step 108400/136300 (epoch 80/100), loss = 0.169261 (0.241 sec/batch), lr: 0.040705
2019-12-27 18:48:20.408085: step 108420/136300 (epoch 80/100), loss = 0.287125 (0.230 sec/batch), lr: 0.040705
2019-12-27 18:48:25.186645: step 108440/136300 (epoch 80/100), loss = 0.158431 (0.239 sec/batch), lr: 0.040705
2019-12-27 18:48:30.023482: step 108460/136300 (epoch 80/100), loss = 0.226907 (0.242 sec/batch), lr: 0.040705
2019-12-27 18:48:36.343734: step 108480/136300 (epoch 80/100), loss = 0.294970 (0.248 sec/batch), lr: 0.040705
2019-12-27 18:48:41.121897: step 108500/136300 (epoch 80/100), loss = 0.288784 (0.249 sec/batch), lr: 0.040705
2019-12-27 18:48:46.038917: step 108520/136300 (epoch 80/100), loss = 0.500700 (0.238 sec/batch), lr: 0.040705
2019-12-27 18:48:50.864987: step 108540/136300 (epoch 80/100), loss = 0.313683 (0.212 sec/batch), lr: 0.040705
2019-12-27 18:48:55.585090: step 108560/136300 (epoch 80/100), loss = 0.122232 (0.218 sec/batch), lr: 0.040705
2019-12-27 18:49:00.480882: step 108580/136300 (epoch 80/100), loss = 0.153937 (0.237 sec/batch), lr: 0.040705
2019-12-27 18:49:05.428446: step 108600/136300 (epoch 80/100), loss = 0.079776 (0.249 sec/batch), lr: 0.040705
2019-12-27 18:49:10.235477: step 108620/136300 (epoch 80/100), loss = 0.182364 (0.242 sec/batch), lr: 0.040705
2019-12-27 18:49:15.171157: step 108640/136300 (epoch 80/100), loss = 0.106420 (0.180 sec/batch), lr: 0.040705
2019-12-27 18:49:20.079190: step 108660/136300 (epoch 80/100), loss = 0.150228 (0.238 sec/batch), lr: 0.040705
2019-12-27 18:49:26.139090: step 108680/136300 (epoch 80/100), loss = 0.251571 (0.198 sec/batch), lr: 0.040705
2019-12-27 18:49:31.008093: step 108700/136300 (epoch 80/100), loss = 0.193021 (0.236 sec/batch), lr: 0.040705
2019-12-27 18:49:35.668854: step 108720/136300 (epoch 80/100), loss = 0.266238 (0.212 sec/batch), lr: 0.040705
2019-12-27 18:49:40.449831: step 108740/136300 (epoch 80/100), loss = 0.321355 (0.238 sec/batch), lr: 0.040705
2019-12-27 18:49:45.314392: step 108760/136300 (epoch 80/100), loss = 0.184950 (0.245 sec/batch), lr: 0.040705
2019-12-27 18:49:50.058101: step 108780/136300 (epoch 80/100), loss = 0.209828 (0.218 sec/batch), lr: 0.040705
2019-12-27 18:49:54.978636: step 108800/136300 (epoch 80/100), loss = 0.182114 (0.220 sec/batch), lr: 0.040705
2019-12-27 18:49:59.806198: step 108820/136300 (epoch 80/100), loss = 0.166336 (0.183 sec/batch), lr: 0.040705
2019-12-27 18:50:04.604098: step 108840/136300 (epoch 80/100), loss = 0.068961 (0.216 sec/batch), lr: 0.040705
2019-12-27 18:50:09.307267: step 108860/136300 (epoch 80/100), loss = 0.454480 (0.165 sec/batch), lr: 0.040705
2019-12-27 18:50:15.564843: step 108880/136300 (epoch 80/100), loss = 0.210559 (0.236 sec/batch), lr: 0.040705
2019-12-27 18:50:20.462436: step 108900/136300 (epoch 80/100), loss = 0.314359 (0.232 sec/batch), lr: 0.040705
2019-12-27 18:50:25.361389: step 108920/136300 (epoch 80/100), loss = 0.303553 (0.206 sec/batch), lr: 0.040705
2019-12-27 18:50:30.194537: step 108940/136300 (epoch 80/100), loss = 0.296076 (0.222 sec/batch), lr: 0.040705
2019-12-27 18:50:34.902613: step 108960/136300 (epoch 80/100), loss = 0.169745 (0.218 sec/batch), lr: 0.040705
2019-12-27 18:50:39.664800: step 108980/136300 (epoch 80/100), loss = 0.248213 (0.206 sec/batch), lr: 0.040705
2019-12-27 18:50:44.537165: step 109000/136300 (epoch 80/100), loss = 0.100192 (0.234 sec/batch), lr: 0.040705
2019-12-27 18:50:49.459819: step 109020/136300 (epoch 80/100), loss = 0.106206 (0.241 sec/batch), lr: 0.040705
2019-12-27 18:50:54.242125: step 109040/136300 (epoch 80/100), loss = 0.037847 (0.132 sec/batch), lr: 0.040705
Evaluating on dev set...
Precision (micro): 71.290%
   Recall (micro): 63.539%
       F1 (micro): 67.192%
epoch 80: train_loss = 0.216360, dev_loss = 0.461878, dev_f1 = 0.6719
model saved to ./saved_models/01/checkpoint_epoch_80.pt

2019-12-27 18:51:33.966048: step 109060/136300 (epoch 81/100), loss = 0.247464 (0.221 sec/batch), lr: 0.040705
2019-12-27 18:51:38.691226: step 109080/136300 (epoch 81/100), loss = 0.058224 (0.235 sec/batch), lr: 0.040705
2019-12-27 18:51:43.528653: step 109100/136300 (epoch 81/100), loss = 0.120498 (0.221 sec/batch), lr: 0.040705
2019-12-27 18:51:48.414227: step 109120/136300 (epoch 81/100), loss = 0.103956 (0.224 sec/batch), lr: 0.040705
2019-12-27 18:51:53.223562: step 109140/136300 (epoch 81/100), loss = 0.077693 (0.233 sec/batch), lr: 0.040705
2019-12-27 18:51:58.011592: step 109160/136300 (epoch 81/100), loss = 0.235739 (0.247 sec/batch), lr: 0.040705
2019-12-27 18:52:02.815644: step 109180/136300 (epoch 81/100), loss = 0.269643 (0.247 sec/batch), lr: 0.040705
2019-12-27 18:52:07.640592: step 109200/136300 (epoch 81/100), loss = 0.130660 (0.244 sec/batch), lr: 0.040705
2019-12-27 18:52:12.551901: step 109220/136300 (epoch 81/100), loss = 0.078744 (0.237 sec/batch), lr: 0.040705
2019-12-27 18:52:17.417695: step 109240/136300 (epoch 81/100), loss = 0.278225 (0.239 sec/batch), lr: 0.040705
2019-12-27 18:52:23.512466: step 109260/136300 (epoch 81/100), loss = 0.242422 (0.204 sec/batch), lr: 0.040705
2019-12-27 18:52:28.270034: step 109280/136300 (epoch 81/100), loss = 0.197410 (0.202 sec/batch), lr: 0.040705
2019-12-27 18:52:33.093042: step 109300/136300 (epoch 81/100), loss = 0.132345 (0.211 sec/batch), lr: 0.040705
2019-12-27 18:52:37.904792: step 109320/136300 (epoch 81/100), loss = 0.107304 (0.184 sec/batch), lr: 0.040705
2019-12-27 18:52:42.753685: step 109340/136300 (epoch 81/100), loss = 0.172160 (0.241 sec/batch), lr: 0.040705
2019-12-27 18:52:47.531196: step 109360/136300 (epoch 81/100), loss = 0.133451 (0.226 sec/batch), lr: 0.040705
2019-12-27 18:52:52.339793: step 109380/136300 (epoch 81/100), loss = 0.334610 (0.211 sec/batch), lr: 0.040705
2019-12-27 18:52:57.224262: step 109400/136300 (epoch 81/100), loss = 0.566002 (0.219 sec/batch), lr: 0.040705
2019-12-27 18:53:02.002636: step 109420/136300 (epoch 81/100), loss = 0.216855 (0.235 sec/batch), lr: 0.040705
2019-12-27 18:53:06.785286: step 109440/136300 (epoch 81/100), loss = 0.322667 (0.247 sec/batch), lr: 0.040705
2019-12-27 18:53:12.992555: step 109460/136300 (epoch 81/100), loss = 0.164631 (0.234 sec/batch), lr: 0.040705
2019-12-27 18:53:17.840903: step 109480/136300 (epoch 81/100), loss = 0.399343 (0.231 sec/batch), lr: 0.040705
2019-12-27 18:53:22.733239: step 109500/136300 (epoch 81/100), loss = 0.305752 (0.248 sec/batch), lr: 0.040705
2019-12-27 18:53:27.589828: step 109520/136300 (epoch 81/100), loss = 0.173553 (0.248 sec/batch), lr: 0.040705
2019-12-27 18:53:32.385712: step 109540/136300 (epoch 81/100), loss = 0.326241 (0.232 sec/batch), lr: 0.040705
2019-12-27 18:53:37.291713: step 109560/136300 (epoch 81/100), loss = 0.410843 (0.244 sec/batch), lr: 0.040705
2019-12-27 18:53:42.217847: step 109580/136300 (epoch 81/100), loss = 0.250777 (0.241 sec/batch), lr: 0.040705
2019-12-27 18:53:47.016118: step 109600/136300 (epoch 81/100), loss = 0.288685 (0.216 sec/batch), lr: 0.040705
2019-12-27 18:53:51.838196: step 109620/136300 (epoch 81/100), loss = 0.171823 (0.213 sec/batch), lr: 0.040705
2019-12-27 18:53:56.546965: step 109640/136300 (epoch 81/100), loss = 0.192134 (0.207 sec/batch), lr: 0.040705
2019-12-27 18:54:02.525993: step 109660/136300 (epoch 81/100), loss = 0.172223 (0.228 sec/batch), lr: 0.040705
2019-12-27 18:54:07.331791: step 109680/136300 (epoch 81/100), loss = 0.194806 (0.182 sec/batch), lr: 0.040705
2019-12-27 18:54:12.109714: step 109700/136300 (epoch 81/100), loss = 0.128927 (0.210 sec/batch), lr: 0.040705
2019-12-27 18:54:16.989277: step 109720/136300 (epoch 81/100), loss = 0.370029 (0.229 sec/batch), lr: 0.040705
2019-12-27 18:54:21.937689: step 109740/136300 (epoch 81/100), loss = 0.202931 (0.242 sec/batch), lr: 0.040705
2019-12-27 18:54:26.839840: step 109760/136300 (epoch 81/100), loss = 0.202168 (0.230 sec/batch), lr: 0.040705
2019-12-27 18:54:31.720021: step 109780/136300 (epoch 81/100), loss = 0.098708 (0.175 sec/batch), lr: 0.040705
2019-12-27 18:54:36.546162: step 109800/136300 (epoch 81/100), loss = 0.299457 (0.246 sec/batch), lr: 0.040705
2019-12-27 18:54:41.360026: step 109820/136300 (epoch 81/100), loss = 0.195827 (0.250 sec/batch), lr: 0.040705
2019-12-27 18:54:47.772734: step 109840/136300 (epoch 81/100), loss = 0.420211 (0.208 sec/batch), lr: 0.040705
2019-12-27 18:54:52.578063: step 109860/136300 (epoch 81/100), loss = 0.312718 (0.247 sec/batch), lr: 0.040705
2019-12-27 18:54:57.448410: step 109880/136300 (epoch 81/100), loss = 0.199925 (0.232 sec/batch), lr: 0.040705
2019-12-27 18:55:02.342717: step 109900/136300 (epoch 81/100), loss = 0.264701 (0.228 sec/batch), lr: 0.040705
2019-12-27 18:55:07.084247: step 109920/136300 (epoch 81/100), loss = 0.147842 (0.247 sec/batch), lr: 0.040705
2019-12-27 18:55:11.916395: step 109940/136300 (epoch 81/100), loss = 0.292194 (0.212 sec/batch), lr: 0.040705
2019-12-27 18:55:16.845360: step 109960/136300 (epoch 81/100), loss = 0.278380 (0.223 sec/batch), lr: 0.040705
2019-12-27 18:55:21.694770: step 109980/136300 (epoch 81/100), loss = 0.096632 (0.234 sec/batch), lr: 0.040705
2019-12-27 18:55:26.636713: step 110000/136300 (epoch 81/100), loss = 0.232106 (0.216 sec/batch), lr: 0.040705
2019-12-27 18:55:31.529847: step 110020/136300 (epoch 81/100), loss = 0.271684 (0.219 sec/batch), lr: 0.040705
2019-12-27 18:55:36.355834: step 110040/136300 (epoch 81/100), loss = 0.306789 (0.241 sec/batch), lr: 0.040705
2019-12-27 18:55:42.435003: step 110060/136300 (epoch 81/100), loss = 0.352120 (0.232 sec/batch), lr: 0.040705
2019-12-27 18:55:47.165755: step 110080/136300 (epoch 81/100), loss = 0.176053 (0.186 sec/batch), lr: 0.040705
2019-12-27 18:55:51.862770: step 110100/136300 (epoch 81/100), loss = 0.314267 (0.170 sec/batch), lr: 0.040705
2019-12-27 18:55:56.722897: step 110120/136300 (epoch 81/100), loss = 0.341493 (0.231 sec/batch), lr: 0.040705
2019-12-27 18:56:01.513237: step 110140/136300 (epoch 81/100), loss = 0.335008 (0.237 sec/batch), lr: 0.040705
2019-12-27 18:56:06.427172: step 110160/136300 (epoch 81/100), loss = 0.263854 (0.216 sec/batch), lr: 0.040705
2019-12-27 18:56:11.292913: step 110180/136300 (epoch 81/100), loss = 0.092875 (0.227 sec/batch), lr: 0.040705
2019-12-27 18:56:16.054696: step 110200/136300 (epoch 81/100), loss = 0.113940 (0.215 sec/batch), lr: 0.040705
2019-12-27 18:56:20.811429: step 110220/136300 (epoch 81/100), loss = 0.242625 (0.235 sec/batch), lr: 0.040705
2019-12-27 18:56:27.280493: step 110240/136300 (epoch 81/100), loss = 0.198635 (1.862 sec/batch), lr: 0.040705
2019-12-27 18:56:32.177198: step 110260/136300 (epoch 81/100), loss = 0.205860 (0.222 sec/batch), lr: 0.040705
2019-12-27 18:56:37.102241: step 110280/136300 (epoch 81/100), loss = 0.238456 (0.232 sec/batch), lr: 0.040705
2019-12-27 18:56:41.967973: step 110300/136300 (epoch 81/100), loss = 0.294192 (0.248 sec/batch), lr: 0.040705
2019-12-27 18:56:46.661684: step 110320/136300 (epoch 81/100), loss = 0.099065 (0.237 sec/batch), lr: 0.040705
2019-12-27 18:56:51.457615: step 110340/136300 (epoch 81/100), loss = 0.239268 (0.180 sec/batch), lr: 0.040705
2019-12-27 18:56:56.250593: step 110360/136300 (epoch 81/100), loss = 0.132410 (0.198 sec/batch), lr: 0.040705
2019-12-27 18:57:01.166552: step 110380/136300 (epoch 81/100), loss = 0.078264 (0.220 sec/batch), lr: 0.040705
2019-12-27 18:57:06.132786: step 110400/136300 (epoch 81/100), loss = 0.124914 (0.231 sec/batch), lr: 0.040705
Evaluating on dev set...
Precision (micro): 71.521%
   Recall (micro): 63.245%
       F1 (micro): 67.129%
epoch 81: train_loss = 0.215709, dev_loss = 0.460267, dev_f1 = 0.6713
model saved to ./saved_models/01/checkpoint_epoch_81.pt

2019-12-27 18:57:44.413580: step 110420/136300 (epoch 82/100), loss = 0.166349 (0.247 sec/batch), lr: 0.036634
2019-12-27 18:57:50.452028: step 110440/136300 (epoch 82/100), loss = 0.108750 (0.250 sec/batch), lr: 0.036634
2019-12-27 18:57:55.284569: step 110460/136300 (epoch 82/100), loss = 0.154687 (0.218 sec/batch), lr: 0.036634
2019-12-27 18:58:00.122320: step 110480/136300 (epoch 82/100), loss = 0.351928 (0.242 sec/batch), lr: 0.036634
2019-12-27 18:58:04.950192: step 110500/136300 (epoch 82/100), loss = 0.187540 (0.219 sec/batch), lr: 0.036634
2019-12-27 18:58:09.713312: step 110520/136300 (epoch 82/100), loss = 0.180974 (0.230 sec/batch), lr: 0.036634
2019-12-27 18:58:14.532244: step 110540/136300 (epoch 82/100), loss = 0.265862 (0.224 sec/batch), lr: 0.036634
2019-12-27 18:58:19.353157: step 110560/136300 (epoch 82/100), loss = 0.334611 (0.230 sec/batch), lr: 0.036634
2019-12-27 18:58:24.234563: step 110580/136300 (epoch 82/100), loss = 0.277590 (0.234 sec/batch), lr: 0.036634
2019-12-27 18:58:29.116697: step 110600/136300 (epoch 82/100), loss = 0.341847 (0.235 sec/batch), lr: 0.036634
2019-12-27 18:58:35.261968: step 110620/136300 (epoch 82/100), loss = 0.144652 (0.241 sec/batch), lr: 0.036634
2019-12-27 18:58:39.996466: step 110640/136300 (epoch 82/100), loss = 0.324202 (0.239 sec/batch), lr: 0.036634
2019-12-27 18:58:44.790265: step 110660/136300 (epoch 82/100), loss = 0.155454 (0.234 sec/batch), lr: 0.036634
2019-12-27 18:58:49.659039: step 110680/136300 (epoch 82/100), loss = 0.194560 (0.229 sec/batch), lr: 0.036634
2019-12-27 18:58:54.462833: step 110700/136300 (epoch 82/100), loss = 0.293085 (0.177 sec/batch), lr: 0.036634
2019-12-27 18:58:59.214226: step 110720/136300 (epoch 82/100), loss = 0.122425 (0.221 sec/batch), lr: 0.036634
2019-12-27 18:59:04.068978: step 110740/136300 (epoch 82/100), loss = 0.136895 (0.238 sec/batch), lr: 0.036634
2019-12-27 18:59:08.919848: step 110760/136300 (epoch 82/100), loss = 0.094194 (0.231 sec/batch), lr: 0.036634
2019-12-27 18:59:13.655179: step 110780/136300 (epoch 82/100), loss = 0.177113 (0.228 sec/batch), lr: 0.036634
2019-12-27 18:59:18.430127: step 110800/136300 (epoch 82/100), loss = 0.294543 (0.228 sec/batch), lr: 0.036634
2019-12-27 18:59:24.528171: step 110820/136300 (epoch 82/100), loss = 0.229262 (0.243 sec/batch), lr: 0.036634
2019-12-27 18:59:29.364987: step 110840/136300 (epoch 82/100), loss = 0.197032 (0.241 sec/batch), lr: 0.036634
2019-12-27 18:59:34.247090: step 110860/136300 (epoch 82/100), loss = 0.217629 (0.243 sec/batch), lr: 0.036634
2019-12-27 18:59:39.078872: step 110880/136300 (epoch 82/100), loss = 0.434878 (0.239 sec/batch), lr: 0.036634
2019-12-27 18:59:43.894281: step 110900/136300 (epoch 82/100), loss = 0.235214 (0.217 sec/batch), lr: 0.036634
2019-12-27 18:59:48.815732: step 110920/136300 (epoch 82/100), loss = 0.321943 (0.235 sec/batch), lr: 0.036634
2019-12-27 18:59:53.739623: step 110940/136300 (epoch 82/100), loss = 0.245800 (0.243 sec/batch), lr: 0.036634
2019-12-27 18:59:58.544657: step 110960/136300 (epoch 82/100), loss = 0.298679 (0.238 sec/batch), lr: 0.036634
2019-12-27 19:00:03.373767: step 110980/136300 (epoch 82/100), loss = 0.288559 (0.240 sec/batch), lr: 0.036634
2019-12-27 19:00:08.060175: step 111000/136300 (epoch 82/100), loss = 0.315662 (0.221 sec/batch), lr: 0.036634
2019-12-27 19:00:14.214518: step 111020/136300 (epoch 82/100), loss = 0.256172 (0.200 sec/batch), lr: 0.036634
2019-12-27 19:00:19.051359: step 111040/136300 (epoch 82/100), loss = 0.142426 (0.232 sec/batch), lr: 0.036634
2019-12-27 19:00:23.786752: step 111060/136300 (epoch 82/100), loss = 0.061133 (0.229 sec/batch), lr: 0.036634
2019-12-27 19:00:28.616691: step 111080/136300 (epoch 82/100), loss = 0.342240 (0.232 sec/batch), lr: 0.036634
2019-12-27 19:00:33.591751: step 111100/136300 (epoch 82/100), loss = 0.383159 (0.234 sec/batch), lr: 0.036634
2019-12-27 19:00:38.459655: step 111120/136300 (epoch 82/100), loss = 0.242861 (0.229 sec/batch), lr: 0.036634
2019-12-27 19:00:43.365394: step 111140/136300 (epoch 82/100), loss = 0.196007 (0.236 sec/batch), lr: 0.036634
2019-12-27 19:00:48.157073: step 111160/136300 (epoch 82/100), loss = 0.258241 (0.232 sec/batch), lr: 0.036634
2019-12-27 19:00:52.982138: step 111180/136300 (epoch 82/100), loss = 0.218557 (0.218 sec/batch), lr: 0.036634
2019-12-27 19:00:57.915219: step 111200/136300 (epoch 82/100), loss = 0.211584 (0.238 sec/batch), lr: 0.036634
2019-12-27 19:01:04.224222: step 111220/136300 (epoch 82/100), loss = 0.177737 (0.207 sec/batch), lr: 0.036634
2019-12-27 19:01:09.012517: step 111240/136300 (epoch 82/100), loss = 0.151662 (0.232 sec/batch), lr: 0.036634
2019-12-27 19:01:13.964247: step 111260/136300 (epoch 82/100), loss = 0.136597 (0.227 sec/batch), lr: 0.036634
2019-12-27 19:01:18.639676: step 111280/136300 (epoch 82/100), loss = 0.193292 (0.206 sec/batch), lr: 0.036634
2019-12-27 19:01:23.531917: step 111300/136300 (epoch 82/100), loss = 0.177315 (0.229 sec/batch), lr: 0.036634
2019-12-27 19:01:28.433729: step 111320/136300 (epoch 82/100), loss = 0.212082 (0.216 sec/batch), lr: 0.036634
2019-12-27 19:01:33.250560: step 111340/136300 (epoch 82/100), loss = 0.137745 (0.237 sec/batch), lr: 0.036634
2019-12-27 19:01:38.215806: step 111360/136300 (epoch 82/100), loss = 0.171778 (0.243 sec/batch), lr: 0.036634
2019-12-27 19:01:43.109745: step 111380/136300 (epoch 82/100), loss = 0.347003 (0.234 sec/batch), lr: 0.036634
2019-12-27 19:01:47.939934: step 111400/136300 (epoch 82/100), loss = 0.144245 (0.243 sec/batch), lr: 0.036634
2019-12-27 19:01:54.086124: step 111420/136300 (epoch 82/100), loss = 0.119589 (0.214 sec/batch), lr: 0.036634
2019-12-27 19:01:58.900694: step 111440/136300 (epoch 82/100), loss = 0.228660 (0.212 sec/batch), lr: 0.036634
2019-12-27 19:02:03.574712: step 111460/136300 (epoch 82/100), loss = 0.152616 (0.215 sec/batch), lr: 0.036634
2019-12-27 19:02:08.367340: step 111480/136300 (epoch 82/100), loss = 0.220592 (0.234 sec/batch), lr: 0.036634
2019-12-27 19:02:13.177577: step 111500/136300 (epoch 82/100), loss = 0.174917 (0.203 sec/batch), lr: 0.036634
2019-12-27 19:02:18.046717: step 111520/136300 (epoch 82/100), loss = 0.083946 (0.240 sec/batch), lr: 0.036634
2019-12-27 19:02:22.897260: step 111540/136300 (epoch 82/100), loss = 0.297098 (0.201 sec/batch), lr: 0.036634
2019-12-27 19:02:27.668000: step 111560/136300 (epoch 82/100), loss = 0.342367 (0.211 sec/batch), lr: 0.036634
2019-12-27 19:02:32.373081: step 111580/136300 (epoch 82/100), loss = 0.114917 (0.240 sec/batch), lr: 0.036634
2019-12-27 19:02:37.213419: step 111600/136300 (epoch 82/100), loss = 0.200468 (0.205 sec/batch), lr: 0.036634
2019-12-27 19:02:43.386471: step 111620/136300 (epoch 82/100), loss = 0.184110 (0.210 sec/batch), lr: 0.036634
2019-12-27 19:02:48.301555: step 111640/136300 (epoch 82/100), loss = 0.204919 (0.216 sec/batch), lr: 0.036634
2019-12-27 19:02:53.166319: step 111660/136300 (epoch 82/100), loss = 0.249435 (0.206 sec/batch), lr: 0.036634
2019-12-27 19:02:57.879970: step 111680/136300 (epoch 82/100), loss = 0.301626 (0.248 sec/batch), lr: 0.036634
2019-12-27 19:03:02.726014: step 111700/136300 (epoch 82/100), loss = 0.112233 (0.243 sec/batch), lr: 0.036634
2019-12-27 19:03:07.484625: step 111720/136300 (epoch 82/100), loss = 0.203357 (0.215 sec/batch), lr: 0.036634
2019-12-27 19:03:12.410175: step 111740/136300 (epoch 82/100), loss = 0.385137 (0.200 sec/batch), lr: 0.036634
2019-12-27 19:03:17.333600: step 111760/136300 (epoch 82/100), loss = 0.260841 (0.241 sec/batch), lr: 0.036634
Evaluating on dev set...
Precision (micro): 72.041%
   Recall (micro): 62.472%
       F1 (micro): 66.916%
epoch 82: train_loss = 0.217134, dev_loss = 0.465356, dev_f1 = 0.6692
model saved to ./saved_models/01/checkpoint_epoch_82.pt

2019-12-27 19:03:55.648741: step 111780/136300 (epoch 83/100), loss = 0.165569 (0.232 sec/batch), lr: 0.032971
2019-12-27 19:04:01.652437: step 111800/136300 (epoch 83/100), loss = 0.204948 (0.183 sec/batch), lr: 0.032971
2019-12-27 19:04:06.470678: step 111820/136300 (epoch 83/100), loss = 0.233238 (0.167 sec/batch), lr: 0.032971
2019-12-27 19:04:11.251184: step 111840/136300 (epoch 83/100), loss = 0.233164 (0.222 sec/batch), lr: 0.032971
2019-12-27 19:04:16.186512: step 111860/136300 (epoch 83/100), loss = 0.204426 (0.227 sec/batch), lr: 0.032971
2019-12-27 19:04:20.943107: step 111880/136300 (epoch 83/100), loss = 0.201701 (0.246 sec/batch), lr: 0.032971
2019-12-27 19:04:25.739990: step 111900/136300 (epoch 83/100), loss = 0.230724 (0.220 sec/batch), lr: 0.032971
2019-12-27 19:04:30.539845: step 111920/136300 (epoch 83/100), loss = 0.179269 (0.244 sec/batch), lr: 0.032971
2019-12-27 19:04:35.433729: step 111940/136300 (epoch 83/100), loss = 0.175369 (0.243 sec/batch), lr: 0.032971
2019-12-27 19:04:40.300994: step 111960/136300 (epoch 83/100), loss = 0.208141 (0.214 sec/batch), lr: 0.032971
2019-12-27 19:04:46.560554: step 111980/136300 (epoch 83/100), loss = 0.109091 (0.242 sec/batch), lr: 0.032971
2019-12-27 19:04:51.255584: step 112000/136300 (epoch 83/100), loss = 0.264646 (0.224 sec/batch), lr: 0.032971
2019-12-27 19:04:56.077157: step 112020/136300 (epoch 83/100), loss = 0.177679 (0.238 sec/batch), lr: 0.032971
2019-12-27 19:05:00.978532: step 112040/136300 (epoch 83/100), loss = 0.185247 (0.225 sec/batch), lr: 0.032971
2019-12-27 19:05:05.830548: step 112060/136300 (epoch 83/100), loss = 0.212474 (0.240 sec/batch), lr: 0.032971
2019-12-27 19:05:10.561338: step 112080/136300 (epoch 83/100), loss = 0.103521 (0.244 sec/batch), lr: 0.032971
2019-12-27 19:05:15.383413: step 112100/136300 (epoch 83/100), loss = 0.342060 (0.237 sec/batch), lr: 0.032971
2019-12-27 19:05:20.212764: step 112120/136300 (epoch 83/100), loss = 0.172293 (0.207 sec/batch), lr: 0.032971
2019-12-27 19:05:24.996104: step 112140/136300 (epoch 83/100), loss = 0.120800 (0.230 sec/batch), lr: 0.032971
2019-12-27 19:05:29.785316: step 112160/136300 (epoch 83/100), loss = 0.172963 (0.248 sec/batch), lr: 0.032971
2019-12-27 19:05:35.820139: step 112180/136300 (epoch 83/100), loss = 0.242253 (0.245 sec/batch), lr: 0.032971
2019-12-27 19:05:40.643521: step 112200/136300 (epoch 83/100), loss = 0.204678 (0.230 sec/batch), lr: 0.032971
2019-12-27 19:05:45.545875: step 112220/136300 (epoch 83/100), loss = 0.327063 (0.248 sec/batch), lr: 0.032971
2019-12-27 19:05:50.365268: step 112240/136300 (epoch 83/100), loss = 0.348955 (0.232 sec/batch), lr: 0.032971
2019-12-27 19:05:55.195326: step 112260/136300 (epoch 83/100), loss = 0.230551 (0.232 sec/batch), lr: 0.032971
2019-12-27 19:06:00.092775: step 112280/136300 (epoch 83/100), loss = 0.113053 (0.233 sec/batch), lr: 0.032971
2019-12-27 19:06:04.973402: step 112300/136300 (epoch 83/100), loss = 0.230885 (0.245 sec/batch), lr: 0.032971
2019-12-27 19:06:09.787554: step 112320/136300 (epoch 83/100), loss = 0.211307 (0.208 sec/batch), lr: 0.032971
2019-12-27 19:06:14.666352: step 112340/136300 (epoch 83/100), loss = 0.161136 (0.242 sec/batch), lr: 0.032971
2019-12-27 19:06:19.470612: step 112360/136300 (epoch 83/100), loss = 0.155306 (0.238 sec/batch), lr: 0.032971
2019-12-27 19:06:25.620107: step 112380/136300 (epoch 83/100), loss = 0.241419 (0.250 sec/batch), lr: 0.032971
2019-12-27 19:06:30.426254: step 112400/136300 (epoch 83/100), loss = 0.270098 (0.251 sec/batch), lr: 0.032971
2019-12-27 19:06:35.193651: step 112420/136300 (epoch 83/100), loss = 0.101563 (0.243 sec/batch), lr: 0.032971
2019-12-27 19:06:40.025302: step 112440/136300 (epoch 83/100), loss = 0.178493 (0.234 sec/batch), lr: 0.032971
2019-12-27 19:06:44.960294: step 112460/136300 (epoch 83/100), loss = 0.271299 (0.241 sec/batch), lr: 0.032971
2019-12-27 19:06:49.842872: step 112480/136300 (epoch 83/100), loss = 0.305954 (0.231 sec/batch), lr: 0.032971
2019-12-27 19:06:54.791777: step 112500/136300 (epoch 83/100), loss = 0.232430 (0.236 sec/batch), lr: 0.032971
2019-12-27 19:06:59.649971: step 112520/136300 (epoch 83/100), loss = 0.254999 (0.243 sec/batch), lr: 0.032971
2019-12-27 19:07:04.409211: step 112540/136300 (epoch 83/100), loss = 0.064641 (0.239 sec/batch), lr: 0.032971
2019-12-27 19:07:09.336878: step 112560/136300 (epoch 83/100), loss = 0.124277 (0.221 sec/batch), lr: 0.032971
2019-12-27 19:07:15.486594: step 112580/136300 (epoch 83/100), loss = 0.376718 (0.222 sec/batch), lr: 0.032971
2019-12-27 19:07:20.278941: step 112600/136300 (epoch 83/100), loss = 0.103909 (0.221 sec/batch), lr: 0.032971
2019-12-27 19:07:25.209959: step 112620/136300 (epoch 83/100), loss = 0.281251 (0.229 sec/batch), lr: 0.032971
2019-12-27 19:07:29.899343: step 112640/136300 (epoch 83/100), loss = 0.266253 (0.243 sec/batch), lr: 0.032971
2019-12-27 19:07:34.789376: step 112660/136300 (epoch 83/100), loss = 0.200124 (0.231 sec/batch), lr: 0.032971
2019-12-27 19:07:39.722111: step 112680/136300 (epoch 83/100), loss = 0.163090 (0.222 sec/batch), lr: 0.032971
2019-12-27 19:07:44.516662: step 112700/136300 (epoch 83/100), loss = 0.178689 (0.212 sec/batch), lr: 0.032971
2019-12-27 19:07:49.476955: step 112720/136300 (epoch 83/100), loss = 0.175103 (0.219 sec/batch), lr: 0.032971
2019-12-27 19:07:54.362268: step 112740/136300 (epoch 83/100), loss = 0.234385 (0.215 sec/batch), lr: 0.032971
2019-12-27 19:07:59.233808: step 112760/136300 (epoch 83/100), loss = 0.312480 (0.221 sec/batch), lr: 0.032971
2019-12-27 19:08:05.434008: step 112780/136300 (epoch 83/100), loss = 0.168433 (0.201 sec/batch), lr: 0.032971
2019-12-27 19:08:10.308475: step 112800/136300 (epoch 83/100), loss = 0.090319 (0.239 sec/batch), lr: 0.032971
2019-12-27 19:08:14.981335: step 112820/136300 (epoch 83/100), loss = 0.089029 (0.214 sec/batch), lr: 0.032971
2019-12-27 19:08:19.754601: step 112840/136300 (epoch 83/100), loss = 0.210998 (0.243 sec/batch), lr: 0.032971
2019-12-27 19:08:24.580578: step 112860/136300 (epoch 83/100), loss = 0.235320 (0.205 sec/batch), lr: 0.032971
2019-12-27 19:08:29.438327: step 112880/136300 (epoch 83/100), loss = 0.234006 (0.242 sec/batch), lr: 0.032971
2019-12-27 19:08:34.337605: step 112900/136300 (epoch 83/100), loss = 0.274949 (0.237 sec/batch), lr: 0.032971
2019-12-27 19:08:39.098287: step 112920/136300 (epoch 83/100), loss = 0.227502 (0.240 sec/batch), lr: 0.032971
2019-12-27 19:08:43.791879: step 112940/136300 (epoch 83/100), loss = 0.202665 (0.206 sec/batch), lr: 0.032971
2019-12-27 19:08:48.633697: step 112960/136300 (epoch 83/100), loss = 0.223680 (0.215 sec/batch), lr: 0.032971
2019-12-27 19:08:54.861216: step 112980/136300 (epoch 83/100), loss = 0.197791 (0.199 sec/batch), lr: 0.032971
2019-12-27 19:08:59.788805: step 113000/136300 (epoch 83/100), loss = 0.185293 (0.214 sec/batch), lr: 0.032971
2019-12-27 19:09:04.686701: step 113020/136300 (epoch 83/100), loss = 0.264264 (0.216 sec/batch), lr: 0.032971
2019-12-27 19:09:09.367189: step 113040/136300 (epoch 83/100), loss = 0.139345 (0.247 sec/batch), lr: 0.032971
2019-12-27 19:09:14.137649: step 113060/136300 (epoch 83/100), loss = 0.193367 (0.230 sec/batch), lr: 0.032971
2019-12-27 19:09:18.955100: step 113080/136300 (epoch 83/100), loss = 0.125620 (0.229 sec/batch), lr: 0.032971
2019-12-27 19:09:23.872178: step 113100/136300 (epoch 83/100), loss = 0.038463 (0.228 sec/batch), lr: 0.032971
2019-12-27 19:09:28.752896: step 113120/136300 (epoch 83/100), loss = 0.170657 (0.232 sec/batch), lr: 0.032971
Evaluating on dev set...
Precision (micro): 71.188%
   Recall (micro): 62.951%
       F1 (micro): 66.816%
epoch 83: train_loss = 0.213407, dev_loss = 0.466088, dev_f1 = 0.6682
model saved to ./saved_models/01/checkpoint_epoch_83.pt

2019-12-27 19:10:07.104209: step 113140/136300 (epoch 84/100), loss = 0.144445 (0.246 sec/batch), lr: 0.029674
2019-12-27 19:10:13.145299: step 113160/136300 (epoch 84/100), loss = 0.256795 (0.210 sec/batch), lr: 0.029674
2019-12-27 19:10:17.983691: step 113180/136300 (epoch 84/100), loss = 0.155807 (0.243 sec/batch), lr: 0.029674
2019-12-27 19:10:22.699575: step 113200/136300 (epoch 84/100), loss = 0.185489 (0.231 sec/batch), lr: 0.029674
2019-12-27 19:10:27.662503: step 113220/136300 (epoch 84/100), loss = 0.173778 (0.235 sec/batch), lr: 0.029674
2019-12-27 19:10:32.397581: step 113240/136300 (epoch 84/100), loss = 0.320456 (0.181 sec/batch), lr: 0.029674
2019-12-27 19:10:37.195310: step 113260/136300 (epoch 84/100), loss = 0.403943 (0.211 sec/batch), lr: 0.029674
2019-12-27 19:10:42.049518: step 113280/136300 (epoch 84/100), loss = 0.183727 (0.250 sec/batch), lr: 0.029674
2019-12-27 19:10:46.884367: step 113300/136300 (epoch 84/100), loss = 0.302845 (0.215 sec/batch), lr: 0.029674
2019-12-27 19:10:51.785752: step 113320/136300 (epoch 84/100), loss = 0.150914 (0.227 sec/batch), lr: 0.029674
2019-12-27 19:10:56.595894: step 113340/136300 (epoch 84/100), loss = 0.305062 (0.240 sec/batch), lr: 0.029674
2019-12-27 19:11:02.723676: step 113360/136300 (epoch 84/100), loss = 0.230695 (0.246 sec/batch), lr: 0.029674
2019-12-27 19:11:07.500736: step 113380/136300 (epoch 84/100), loss = 0.205172 (0.242 sec/batch), lr: 0.029674
2019-12-27 19:11:12.390525: step 113400/136300 (epoch 84/100), loss = 0.191044 (0.234 sec/batch), lr: 0.029674
2019-12-27 19:11:17.235030: step 113420/136300 (epoch 84/100), loss = 0.122630 (0.236 sec/batch), lr: 0.029674
2019-12-27 19:11:21.986402: step 113440/136300 (epoch 84/100), loss = 0.100396 (0.237 sec/batch), lr: 0.029674
2019-12-27 19:11:26.766580: step 113460/136300 (epoch 84/100), loss = 0.287385 (0.222 sec/batch), lr: 0.029674
2019-12-27 19:11:31.620903: step 113480/136300 (epoch 84/100), loss = 0.178408 (0.234 sec/batch), lr: 0.029674
2019-12-27 19:11:36.478485: step 113500/136300 (epoch 84/100), loss = 0.151835 (0.232 sec/batch), lr: 0.029674
2019-12-27 19:11:41.189961: step 113520/136300 (epoch 84/100), loss = 0.224690 (0.219 sec/batch), lr: 0.029674
2019-12-27 19:11:47.227058: step 113540/136300 (epoch 84/100), loss = 0.292909 (0.216 sec/batch), lr: 0.029674
2019-12-27 19:11:52.038376: step 113560/136300 (epoch 84/100), loss = 0.205737 (0.243 sec/batch), lr: 0.029674
2019-12-27 19:11:56.971327: step 113580/136300 (epoch 84/100), loss = 0.190706 (0.231 sec/batch), lr: 0.029674
2019-12-27 19:12:01.819764: step 113600/136300 (epoch 84/100), loss = 0.348697 (0.196 sec/batch), lr: 0.029674
2019-12-27 19:12:06.642866: step 113620/136300 (epoch 84/100), loss = 0.207065 (0.249 sec/batch), lr: 0.029674
2019-12-27 19:12:11.513489: step 113640/136300 (epoch 84/100), loss = 0.176366 (0.224 sec/batch), lr: 0.029674
2019-12-27 19:12:16.402418: step 113660/136300 (epoch 84/100), loss = 0.379181 (0.240 sec/batch), lr: 0.029674
2019-12-27 19:12:21.264616: step 113680/136300 (epoch 84/100), loss = 0.258250 (0.234 sec/batch), lr: 0.029674
2019-12-27 19:12:26.086895: step 113700/136300 (epoch 84/100), loss = 0.046084 (0.230 sec/batch), lr: 0.029674
2019-12-27 19:12:30.957678: step 113720/136300 (epoch 84/100), loss = 0.117319 (0.250 sec/batch), lr: 0.029674
2019-12-27 19:12:36.994419: step 113740/136300 (epoch 84/100), loss = 0.297924 (0.246 sec/batch), lr: 0.029674
2019-12-27 19:12:41.780425: step 113760/136300 (epoch 84/100), loss = 0.132423 (0.215 sec/batch), lr: 0.029674
2019-12-27 19:12:46.549755: step 113780/136300 (epoch 84/100), loss = 0.127065 (0.236 sec/batch), lr: 0.029674
2019-12-27 19:12:51.380210: step 113800/136300 (epoch 84/100), loss = 0.182702 (0.208 sec/batch), lr: 0.029674
2019-12-27 19:12:56.313489: step 113820/136300 (epoch 84/100), loss = 0.103424 (0.231 sec/batch), lr: 0.029674
2019-12-27 19:13:01.215340: step 113840/136300 (epoch 84/100), loss = 0.160867 (0.236 sec/batch), lr: 0.029674
2019-12-27 19:13:06.126223: step 113860/136300 (epoch 84/100), loss = 0.263775 (0.235 sec/batch), lr: 0.029674
2019-12-27 19:13:10.989161: step 113880/136300 (epoch 84/100), loss = 0.212285 (0.207 sec/batch), lr: 0.029674
2019-12-27 19:13:15.767246: step 113900/136300 (epoch 84/100), loss = 0.262824 (0.254 sec/batch), lr: 0.029674
2019-12-27 19:13:20.670141: step 113920/136300 (epoch 84/100), loss = 0.246567 (0.250 sec/batch), lr: 0.029674
2019-12-27 19:13:26.912515: step 113940/136300 (epoch 84/100), loss = 0.500529 (0.237 sec/batch), lr: 0.029674
2019-12-27 19:13:31.670702: step 113960/136300 (epoch 84/100), loss = 0.207348 (0.178 sec/batch), lr: 0.029674
2019-12-27 19:13:36.666077: step 113980/136300 (epoch 84/100), loss = 0.094309 (0.226 sec/batch), lr: 0.029674
2019-12-27 19:13:41.278268: step 114000/136300 (epoch 84/100), loss = 0.180600 (0.170 sec/batch), lr: 0.029674
2019-12-27 19:13:46.180287: step 114020/136300 (epoch 84/100), loss = 0.163216 (0.201 sec/batch), lr: 0.029674
2019-12-27 19:13:51.136379: step 114040/136300 (epoch 84/100), loss = 0.127846 (0.236 sec/batch), lr: 0.029674
2019-12-27 19:13:55.925930: step 114060/136300 (epoch 84/100), loss = 0.253526 (0.203 sec/batch), lr: 0.029674
2019-12-27 19:14:00.878754: step 114080/136300 (epoch 84/100), loss = 0.249427 (0.236 sec/batch), lr: 0.029674
2019-12-27 19:14:05.806252: step 114100/136300 (epoch 84/100), loss = 0.187277 (0.227 sec/batch), lr: 0.029674
2019-12-27 19:14:10.637959: step 114120/136300 (epoch 84/100), loss = 0.131008 (0.246 sec/batch), lr: 0.029674
2019-12-27 19:14:16.853561: step 114140/136300 (epoch 84/100), loss = 0.230281 (0.224 sec/batch), lr: 0.029674
2019-12-27 19:14:21.762501: step 114160/136300 (epoch 84/100), loss = 0.234577 (0.249 sec/batch), lr: 0.029674
2019-12-27 19:14:26.399715: step 114180/136300 (epoch 84/100), loss = 0.218038 (0.239 sec/batch), lr: 0.029674
2019-12-27 19:14:31.129522: step 114200/136300 (epoch 84/100), loss = 0.219244 (0.241 sec/batch), lr: 0.029674
2019-12-27 19:14:35.962564: step 114220/136300 (epoch 84/100), loss = 0.215559 (0.205 sec/batch), lr: 0.029674
2019-12-27 19:14:40.842191: step 114240/136300 (epoch 84/100), loss = 0.269141 (0.245 sec/batch), lr: 0.029674
2019-12-27 19:14:45.721177: step 114260/136300 (epoch 84/100), loss = 0.375295 (0.226 sec/batch), lr: 0.029674
2019-12-27 19:14:50.466399: step 114280/136300 (epoch 84/100), loss = 0.239128 (0.201 sec/batch), lr: 0.029674
2019-12-27 19:14:55.221967: step 114300/136300 (epoch 84/100), loss = 0.183480 (0.242 sec/batch), lr: 0.029674
2019-12-27 19:14:59.997905: step 114320/136300 (epoch 84/100), loss = 0.295373 (0.244 sec/batch), lr: 0.029674
2019-12-27 19:15:06.178029: step 114340/136300 (epoch 84/100), loss = 0.279084 (0.214 sec/batch), lr: 0.029674
2019-12-27 19:15:11.095248: step 114360/136300 (epoch 84/100), loss = 0.308097 (0.199 sec/batch), lr: 0.029674
2019-12-27 19:15:16.046069: step 114380/136300 (epoch 84/100), loss = 0.280586 (0.233 sec/batch), lr: 0.029674
2019-12-27 19:15:20.657383: step 114400/136300 (epoch 84/100), loss = 0.169143 (0.196 sec/batch), lr: 0.029674
2019-12-27 19:15:25.480371: step 114420/136300 (epoch 84/100), loss = 0.213654 (0.242 sec/batch), lr: 0.029674
2019-12-27 19:15:30.261273: step 114440/136300 (epoch 84/100), loss = 0.152157 (0.236 sec/batch), lr: 0.029674
2019-12-27 19:15:35.215905: step 114460/136300 (epoch 84/100), loss = 0.194099 (0.240 sec/batch), lr: 0.029674
2019-12-27 19:15:40.051065: step 114480/136300 (epoch 84/100), loss = 0.239788 (0.241 sec/batch), lr: 0.029674
Evaluating on dev set...
Precision (micro): 71.663%
   Recall (micro): 62.712%
       F1 (micro): 66.889%
epoch 84: train_loss = 0.215063, dev_loss = 0.465204, dev_f1 = 0.6689
model saved to ./saved_models/01/checkpoint_epoch_84.pt

2019-12-27 19:16:18.474338: step 114500/136300 (epoch 85/100), loss = 0.288634 (0.204 sec/batch), lr: 0.029674
2019-12-27 19:16:24.462265: step 114520/136300 (epoch 85/100), loss = 0.147027 (0.243 sec/batch), lr: 0.029674
2019-12-27 19:16:29.328633: step 114540/136300 (epoch 85/100), loss = 0.245331 (0.247 sec/batch), lr: 0.029674
2019-12-27 19:16:34.022477: step 114560/136300 (epoch 85/100), loss = 0.238766 (0.201 sec/batch), lr: 0.029674
2019-12-27 19:16:38.967167: step 114580/136300 (epoch 85/100), loss = 0.208459 (0.238 sec/batch), lr: 0.029674
2019-12-27 19:16:43.802603: step 114600/136300 (epoch 85/100), loss = 0.179874 (0.233 sec/batch), lr: 0.029674
2019-12-27 19:16:48.554592: step 114620/136300 (epoch 85/100), loss = 0.118828 (0.230 sec/batch), lr: 0.029674
2019-12-27 19:16:53.447195: step 114640/136300 (epoch 85/100), loss = 0.177442 (0.231 sec/batch), lr: 0.029674
2019-12-27 19:16:58.248500: step 114660/136300 (epoch 85/100), loss = 0.109863 (0.211 sec/batch), lr: 0.029674
2019-12-27 19:17:03.146671: step 114680/136300 (epoch 85/100), loss = 0.202541 (0.204 sec/batch), lr: 0.029674
2019-12-27 19:17:08.011404: step 114700/136300 (epoch 85/100), loss = 0.191599 (0.235 sec/batch), lr: 0.029674
2019-12-27 19:17:14.145138: step 114720/136300 (epoch 85/100), loss = 0.310356 (0.239 sec/batch), lr: 0.029674
2019-12-27 19:17:19.002400: step 114740/136300 (epoch 85/100), loss = 0.272802 (0.240 sec/batch), lr: 0.029674
2019-12-27 19:17:23.825556: step 114760/136300 (epoch 85/100), loss = 0.140877 (0.244 sec/batch), lr: 0.029674
2019-12-27 19:17:28.626333: step 114780/136300 (epoch 85/100), loss = 0.168593 (0.239 sec/batch), lr: 0.029674
2019-12-27 19:17:33.444107: step 114800/136300 (epoch 85/100), loss = 0.251066 (0.231 sec/batch), lr: 0.029674
2019-12-27 19:17:38.186337: step 114820/136300 (epoch 85/100), loss = 0.111190 (0.216 sec/batch), lr: 0.029674
2019-12-27 19:17:42.985457: step 114840/136300 (epoch 85/100), loss = 0.084649 (0.209 sec/batch), lr: 0.029674
2019-12-27 19:17:47.894281: step 114860/136300 (epoch 85/100), loss = 0.225376 (0.250 sec/batch), lr: 0.029674
2019-12-27 19:17:52.618071: step 114880/136300 (epoch 85/100), loss = 0.207688 (0.180 sec/batch), lr: 0.029674
2019-12-27 19:17:57.323664: step 114900/136300 (epoch 85/100), loss = 0.312810 (0.251 sec/batch), lr: 0.029674
2019-12-27 19:18:03.379279: step 114920/136300 (epoch 85/100), loss = 0.103349 (0.218 sec/batch), lr: 0.029674
2019-12-27 19:18:08.335628: step 114940/136300 (epoch 85/100), loss = 0.266067 (0.221 sec/batch), lr: 0.029674
2019-12-27 19:18:13.202626: step 114960/136300 (epoch 85/100), loss = 0.205679 (0.174 sec/batch), lr: 0.029674
2019-12-27 19:18:18.034362: step 114980/136300 (epoch 85/100), loss = 0.335043 (0.232 sec/batch), lr: 0.029674
2019-12-27 19:18:22.904836: step 115000/136300 (epoch 85/100), loss = 0.217615 (0.240 sec/batch), lr: 0.029674
2019-12-27 19:18:27.755514: step 115020/136300 (epoch 85/100), loss = 0.450608 (0.218 sec/batch), lr: 0.029674
2019-12-27 19:18:32.693331: step 115040/136300 (epoch 85/100), loss = 0.172107 (0.180 sec/batch), lr: 0.029674
2019-12-27 19:18:37.495898: step 115060/136300 (epoch 85/100), loss = 0.374141 (0.168 sec/batch), lr: 0.029674
2019-12-27 19:18:42.321231: step 115080/136300 (epoch 85/100), loss = 0.183104 (0.228 sec/batch), lr: 0.029674
2019-12-27 19:18:48.481711: step 115100/136300 (epoch 85/100), loss = 0.227105 (0.178 sec/batch), lr: 0.029674
2019-12-27 19:18:53.269830: step 115120/136300 (epoch 85/100), loss = 0.250400 (0.222 sec/batch), lr: 0.029674
2019-12-27 19:18:58.016319: step 115140/136300 (epoch 85/100), loss = 0.122783 (0.205 sec/batch), lr: 0.029674
2019-12-27 19:19:02.838503: step 115160/136300 (epoch 85/100), loss = 0.290077 (0.218 sec/batch), lr: 0.029674
2019-12-27 19:19:07.767922: step 115180/136300 (epoch 85/100), loss = 0.261259 (0.251 sec/batch), lr: 0.029674
2019-12-27 19:19:12.667202: step 115200/136300 (epoch 85/100), loss = 0.213866 (0.231 sec/batch), lr: 0.029674
2019-12-27 19:19:17.576512: step 115220/136300 (epoch 85/100), loss = 0.213716 (0.231 sec/batch), lr: 0.029674
2019-12-27 19:19:22.454621: step 115240/136300 (epoch 85/100), loss = 0.319500 (0.217 sec/batch), lr: 0.029674
2019-12-27 19:19:27.166428: step 115260/136300 (epoch 85/100), loss = 0.195434 (0.172 sec/batch), lr: 0.029674
2019-12-27 19:19:32.095289: step 115280/136300 (epoch 85/100), loss = 0.273232 (0.241 sec/batch), lr: 0.029674
2019-12-27 19:19:38.336669: step 115300/136300 (epoch 85/100), loss = 0.175230 (0.217 sec/batch), lr: 0.029674
2019-12-27 19:19:43.092684: step 115320/136300 (epoch 85/100), loss = 0.306403 (0.234 sec/batch), lr: 0.029674
2019-12-27 19:19:48.065081: step 115340/136300 (epoch 85/100), loss = 0.323209 (0.226 sec/batch), lr: 0.029674
2019-12-27 19:19:52.807392: step 115360/136300 (epoch 85/100), loss = 0.199704 (0.236 sec/batch), lr: 0.029674
2019-12-27 19:19:57.609260: step 115380/136300 (epoch 85/100), loss = 0.165580 (0.239 sec/batch), lr: 0.029674
2019-12-27 19:20:02.537833: step 115400/136300 (epoch 85/100), loss = 0.288312 (0.240 sec/batch), lr: 0.029674
2019-12-27 19:20:07.373365: step 115420/136300 (epoch 85/100), loss = 0.343002 (0.235 sec/batch), lr: 0.029674
2019-12-27 19:20:12.261573: step 115440/136300 (epoch 85/100), loss = 0.301240 (0.240 sec/batch), lr: 0.029674
2019-12-27 19:20:17.233725: step 115460/136300 (epoch 85/100), loss = 0.118630 (0.246 sec/batch), lr: 0.029674
2019-12-27 19:20:22.025498: step 115480/136300 (epoch 85/100), loss = 0.225791 (0.176 sec/batch), lr: 0.029674
2019-12-27 19:20:28.336747: step 115500/136300 (epoch 85/100), loss = 0.309768 (0.215 sec/batch), lr: 0.029674
2019-12-27 19:20:33.266903: step 115520/136300 (epoch 85/100), loss = 0.246522 (0.237 sec/batch), lr: 0.029674
2019-12-27 19:20:37.877115: step 115540/136300 (epoch 85/100), loss = 0.174295 (0.235 sec/batch), lr: 0.029674
2019-12-27 19:20:42.601679: step 115560/136300 (epoch 85/100), loss = 0.250238 (0.181 sec/batch), lr: 0.029674
2019-12-27 19:20:47.483899: step 115580/136300 (epoch 85/100), loss = 0.277045 (0.238 sec/batch), lr: 0.029674
2019-12-27 19:20:52.269415: step 115600/136300 (epoch 85/100), loss = 0.261356 (0.204 sec/batch), lr: 0.029674
2019-12-27 19:20:57.175660: step 115620/136300 (epoch 85/100), loss = 0.314647 (0.239 sec/batch), lr: 0.029674
2019-12-27 19:21:01.933663: step 115640/136300 (epoch 85/100), loss = 0.222390 (0.231 sec/batch), lr: 0.029674
2019-12-27 19:21:06.669080: step 115660/136300 (epoch 85/100), loss = 0.117011 (0.212 sec/batch), lr: 0.029674
2019-12-27 19:21:11.444965: step 115680/136300 (epoch 85/100), loss = 0.270876 (0.234 sec/batch), lr: 0.029674
2019-12-27 19:21:17.708377: step 115700/136300 (epoch 85/100), loss = 0.122225 (0.202 sec/batch), lr: 0.029674
2019-12-27 19:21:22.663951: step 115720/136300 (epoch 85/100), loss = 0.113054 (0.233 sec/batch), lr: 0.029674
2019-12-27 19:21:27.552891: step 115740/136300 (epoch 85/100), loss = 0.149513 (0.239 sec/batch), lr: 0.029674
2019-12-27 19:21:32.272937: step 115760/136300 (epoch 85/100), loss = 0.197840 (0.219 sec/batch), lr: 0.029674
2019-12-27 19:21:37.034318: step 115780/136300 (epoch 85/100), loss = 0.430313 (0.250 sec/batch), lr: 0.029674
2019-12-27 19:21:41.769764: step 115800/136300 (epoch 85/100), loss = 0.218135 (0.218 sec/batch), lr: 0.029674
2019-12-27 19:21:46.732154: step 115820/136300 (epoch 85/100), loss = 0.211800 (0.245 sec/batch), lr: 0.029674
2019-12-27 19:21:51.578354: step 115840/136300 (epoch 85/100), loss = 0.300523 (0.197 sec/batch), lr: 0.029674
Evaluating on dev set...
Precision (micro): 71.940%
   Recall (micro): 62.160%
       F1 (micro): 66.693%
epoch 85: train_loss = 0.213720, dev_loss = 0.460904, dev_f1 = 0.6669
model saved to ./saved_models/01/checkpoint_epoch_85.pt

2019-12-27 19:22:29.917368: step 115860/136300 (epoch 86/100), loss = 0.183192 (0.245 sec/batch), lr: 0.026706
2019-12-27 19:22:36.023849: step 115880/136300 (epoch 86/100), loss = 0.246544 (0.228 sec/batch), lr: 0.026706
2019-12-27 19:22:40.833785: step 115900/136300 (epoch 86/100), loss = 0.110011 (0.238 sec/batch), lr: 0.026706
2019-12-27 19:22:45.579963: step 115920/136300 (epoch 86/100), loss = 0.233111 (0.218 sec/batch), lr: 0.026706
2019-12-27 19:22:50.502578: step 115940/136300 (epoch 86/100), loss = 0.116732 (0.218 sec/batch), lr: 0.026706
2019-12-27 19:22:55.302513: step 115960/136300 (epoch 86/100), loss = 0.149617 (0.241 sec/batch), lr: 0.026706
2019-12-27 19:23:00.074456: step 115980/136300 (epoch 86/100), loss = 0.262882 (0.237 sec/batch), lr: 0.026706
2019-12-27 19:23:04.943372: step 116000/136300 (epoch 86/100), loss = 0.229405 (0.234 sec/batch), lr: 0.026706
2019-12-27 19:23:09.746229: step 116020/136300 (epoch 86/100), loss = 0.119333 (0.213 sec/batch), lr: 0.026706
2019-12-27 19:23:14.647189: step 116040/136300 (epoch 86/100), loss = 0.274882 (0.232 sec/batch), lr: 0.026706
2019-12-27 19:23:19.471651: step 116060/136300 (epoch 86/100), loss = 0.180891 (0.247 sec/batch), lr: 0.026706
2019-12-27 19:23:25.747398: step 116080/136300 (epoch 86/100), loss = 0.122377 (0.239 sec/batch), lr: 0.026706
2019-12-27 19:23:30.574409: step 116100/136300 (epoch 86/100), loss = 0.302530 (0.217 sec/batch), lr: 0.026706
2019-12-27 19:23:35.357505: step 116120/136300 (epoch 86/100), loss = 0.103721 (0.227 sec/batch), lr: 0.026706
2019-12-27 19:23:40.162941: step 116140/136300 (epoch 86/100), loss = 0.414319 (0.221 sec/batch), lr: 0.026706
2019-12-27 19:23:45.007496: step 116160/136300 (epoch 86/100), loss = 0.196265 (0.195 sec/batch), lr: 0.026706
2019-12-27 19:23:49.750570: step 116180/136300 (epoch 86/100), loss = 0.216036 (0.218 sec/batch), lr: 0.026706
2019-12-27 19:23:54.550360: step 116200/136300 (epoch 86/100), loss = 0.169623 (0.219 sec/batch), lr: 0.026706
2019-12-27 19:23:59.463329: step 116220/136300 (epoch 86/100), loss = 0.108397 (0.223 sec/batch), lr: 0.026706
2019-12-27 19:24:04.212689: step 116240/136300 (epoch 86/100), loss = 0.075458 (0.244 sec/batch), lr: 0.026706
2019-12-27 19:24:08.841410: step 116260/136300 (epoch 86/100), loss = 0.267406 (0.216 sec/batch), lr: 0.026706
2019-12-27 19:24:14.941998: step 116280/136300 (epoch 86/100), loss = 0.109505 (0.206 sec/batch), lr: 0.026706
2019-12-27 19:24:19.898199: step 116300/136300 (epoch 86/100), loss = 0.437704 (0.234 sec/batch), lr: 0.026706
2019-12-27 19:24:24.798538: step 116320/136300 (epoch 86/100), loss = 0.117668 (0.237 sec/batch), lr: 0.026706
2019-12-27 19:24:29.617869: step 116340/136300 (epoch 86/100), loss = 0.219450 (0.244 sec/batch), lr: 0.026706
2019-12-27 19:24:34.458694: step 116360/136300 (epoch 86/100), loss = 0.236502 (0.245 sec/batch), lr: 0.026706
2019-12-27 19:24:39.289178: step 116380/136300 (epoch 86/100), loss = 0.435870 (0.200 sec/batch), lr: 0.026706
2019-12-27 19:24:44.253374: step 116400/136300 (epoch 86/100), loss = 0.181504 (0.245 sec/batch), lr: 0.026706
2019-12-27 19:24:49.032323: step 116420/136300 (epoch 86/100), loss = 0.152373 (0.249 sec/batch), lr: 0.026706
2019-12-27 19:24:53.819600: step 116440/136300 (epoch 86/100), loss = 0.331439 (0.211 sec/batch), lr: 0.026706
2019-12-27 19:24:58.606684: step 116460/136300 (epoch 86/100), loss = 0.328869 (0.246 sec/batch), lr: 0.026706
2019-12-27 19:25:04.706157: step 116480/136300 (epoch 86/100), loss = 0.182480 (0.235 sec/batch), lr: 0.026706
2019-12-27 19:25:09.449118: step 116500/136300 (epoch 86/100), loss = 0.197510 (0.221 sec/batch), lr: 0.026706
2019-12-27 19:25:14.237958: step 116520/136300 (epoch 86/100), loss = 0.164305 (0.230 sec/batch), lr: 0.026706
2019-12-27 19:25:19.122321: step 116540/136300 (epoch 86/100), loss = 0.329771 (0.218 sec/batch), lr: 0.026706
2019-12-27 19:25:24.066652: step 116560/136300 (epoch 86/100), loss = 0.158036 (0.247 sec/batch), lr: 0.026706
2019-12-27 19:25:28.958813: step 116580/136300 (epoch 86/100), loss = 0.253579 (0.223 sec/batch), lr: 0.026706
2019-12-27 19:25:33.887654: step 116600/136300 (epoch 86/100), loss = 0.225946 (0.249 sec/batch), lr: 0.026706
2019-12-27 19:25:38.637732: step 116620/136300 (epoch 86/100), loss = 0.224658 (0.227 sec/batch), lr: 0.026706
2019-12-27 19:25:43.428429: step 116640/136300 (epoch 86/100), loss = 0.171020 (0.182 sec/batch), lr: 0.026706
2019-12-27 19:25:49.788814: step 116660/136300 (epoch 86/100), loss = 0.166267 (0.221 sec/batch), lr: 0.026706
2019-12-27 19:25:54.565639: step 116680/136300 (epoch 86/100), loss = 0.172689 (0.217 sec/batch), lr: 0.026706
2019-12-27 19:25:59.497622: step 116700/136300 (epoch 86/100), loss = 0.241393 (0.248 sec/batch), lr: 0.026706
2019-12-27 19:26:04.249899: step 116720/136300 (epoch 86/100), loss = 0.205132 (0.207 sec/batch), lr: 0.026706
2019-12-27 19:26:09.015163: step 116740/136300 (epoch 86/100), loss = 0.176145 (0.230 sec/batch), lr: 0.026706
2019-12-27 19:26:13.920715: step 116760/136300 (epoch 86/100), loss = 0.155765 (0.244 sec/batch), lr: 0.026706
2019-12-27 19:26:18.842615: step 116780/136300 (epoch 86/100), loss = 0.072257 (0.219 sec/batch), lr: 0.026706
2019-12-27 19:26:23.638911: step 116800/136300 (epoch 86/100), loss = 0.281990 (0.235 sec/batch), lr: 0.026706
2019-12-27 19:26:28.610680: step 116820/136300 (epoch 86/100), loss = 0.257637 (0.251 sec/batch), lr: 0.026706
2019-12-27 19:26:33.495854: step 116840/136300 (epoch 86/100), loss = 0.308930 (0.219 sec/batch), lr: 0.026706
2019-12-27 19:26:39.841200: step 116860/136300 (epoch 86/100), loss = 0.194219 (0.240 sec/batch), lr: 0.026706
2019-12-27 19:26:44.726107: step 116880/136300 (epoch 86/100), loss = 0.133397 (0.236 sec/batch), lr: 0.026706
2019-12-27 19:26:49.358884: step 116900/136300 (epoch 86/100), loss = 0.221684 (0.230 sec/batch), lr: 0.026706
2019-12-27 19:26:54.118818: step 116920/136300 (epoch 86/100), loss = 0.363868 (0.218 sec/batch), lr: 0.026706
2019-12-27 19:26:58.967693: step 116940/136300 (epoch 86/100), loss = 0.211444 (0.206 sec/batch), lr: 0.026706
2019-12-27 19:27:03.762381: step 116960/136300 (epoch 86/100), loss = 0.356942 (0.237 sec/batch), lr: 0.026706
2019-12-27 19:27:08.686000: step 116980/136300 (epoch 86/100), loss = 0.255580 (0.238 sec/batch), lr: 0.026706
2019-12-27 19:27:13.453667: step 117000/136300 (epoch 86/100), loss = 0.099952 (0.225 sec/batch), lr: 0.026706
2019-12-27 19:27:18.201346: step 117020/136300 (epoch 86/100), loss = 0.299758 (0.219 sec/batch), lr: 0.026706
2019-12-27 19:27:22.984132: step 117040/136300 (epoch 86/100), loss = 0.325095 (0.236 sec/batch), lr: 0.026706
2019-12-27 19:27:29.172061: step 117060/136300 (epoch 86/100), loss = 0.136254 (0.217 sec/batch), lr: 0.026706
2019-12-27 19:27:34.096602: step 117080/136300 (epoch 86/100), loss = 0.494323 (0.248 sec/batch), lr: 0.026706
2019-12-27 19:27:38.981393: step 117100/136300 (epoch 86/100), loss = 0.235196 (0.231 sec/batch), lr: 0.026706
2019-12-27 19:27:43.795668: step 117120/136300 (epoch 86/100), loss = 0.193506 (0.243 sec/batch), lr: 0.026706
2019-12-27 19:27:48.506686: step 117140/136300 (epoch 86/100), loss = 0.144326 (0.227 sec/batch), lr: 0.026706
2019-12-27 19:27:53.252461: step 117160/136300 (epoch 86/100), loss = 0.097868 (0.236 sec/batch), lr: 0.026706
2019-12-27 19:27:58.145514: step 117180/136300 (epoch 86/100), loss = 0.081936 (0.236 sec/batch), lr: 0.026706
2019-12-27 19:28:03.050602: step 117200/136300 (epoch 86/100), loss = 0.201425 (0.231 sec/batch), lr: 0.026706
Evaluating on dev set...
Precision (micro): 72.166%
   Recall (micro): 62.528%
       F1 (micro): 67.002%
epoch 86: train_loss = 0.209822, dev_loss = 0.467438, dev_f1 = 0.6700
model saved to ./saved_models/01/checkpoint_epoch_86.pt

2019-12-27 19:28:41.350848: step 117220/136300 (epoch 87/100), loss = 0.243672 (0.230 sec/batch), lr: 0.026706
2019-12-27 19:28:47.519167: step 117240/136300 (epoch 87/100), loss = 0.133108 (0.215 sec/batch), lr: 0.026706
2019-12-27 19:28:52.311231: step 117260/136300 (epoch 87/100), loss = 0.348509 (0.218 sec/batch), lr: 0.026706
2019-12-27 19:28:57.146815: step 117280/136300 (epoch 87/100), loss = 0.200439 (0.217 sec/batch), lr: 0.026706
2019-12-27 19:29:02.016209: step 117300/136300 (epoch 87/100), loss = 0.171654 (0.204 sec/batch), lr: 0.026706
2019-12-27 19:29:06.830237: step 117320/136300 (epoch 87/100), loss = 0.073161 (0.226 sec/batch), lr: 0.026706
2019-12-27 19:29:11.612014: step 117340/136300 (epoch 87/100), loss = 0.219086 (0.242 sec/batch), lr: 0.026706
2019-12-27 19:29:16.471157: step 117360/136300 (epoch 87/100), loss = 0.287718 (0.235 sec/batch), lr: 0.026706
2019-12-27 19:29:21.281871: step 117380/136300 (epoch 87/100), loss = 0.126895 (0.224 sec/batch), lr: 0.026706
2019-12-27 19:29:26.173306: step 117400/136300 (epoch 87/100), loss = 0.126304 (0.218 sec/batch), lr: 0.026706
2019-12-27 19:29:31.021323: step 117420/136300 (epoch 87/100), loss = 0.199830 (0.231 sec/batch), lr: 0.026706
2019-12-27 19:29:37.103666: step 117440/136300 (epoch 87/100), loss = 0.253992 (0.237 sec/batch), lr: 0.026706
2019-12-27 19:29:41.876928: step 117460/136300 (epoch 87/100), loss = 0.243376 (0.243 sec/batch), lr: 0.026706
2019-12-27 19:29:46.692381: step 117480/136300 (epoch 87/100), loss = 0.168766 (0.216 sec/batch), lr: 0.026706
2019-12-27 19:29:51.503958: step 117500/136300 (epoch 87/100), loss = 0.244214 (0.215 sec/batch), lr: 0.026706
2019-12-27 19:29:56.367080: step 117520/136300 (epoch 87/100), loss = 0.174321 (0.243 sec/batch), lr: 0.026706
2019-12-27 19:30:01.153638: step 117540/136300 (epoch 87/100), loss = 0.217076 (0.241 sec/batch), lr: 0.026706
2019-12-27 19:30:05.920184: step 117560/136300 (epoch 87/100), loss = 0.188404 (0.231 sec/batch), lr: 0.026706
2019-12-27 19:30:10.834694: step 117580/136300 (epoch 87/100), loss = 0.186612 (0.240 sec/batch), lr: 0.026706
2019-12-27 19:30:15.589778: step 117600/136300 (epoch 87/100), loss = 0.101341 (0.219 sec/batch), lr: 0.026706
2019-12-27 19:30:20.330368: step 117620/136300 (epoch 87/100), loss = 0.158593 (0.227 sec/batch), lr: 0.026706
2019-12-27 19:30:26.523227: step 117640/136300 (epoch 87/100), loss = 0.093965 (0.217 sec/batch), lr: 0.026706
2019-12-27 19:30:31.420180: step 117660/136300 (epoch 87/100), loss = 0.256913 (0.240 sec/batch), lr: 0.026706
2019-12-27 19:30:36.334555: step 117680/136300 (epoch 87/100), loss = 0.179289 (0.234 sec/batch), lr: 0.026706
2019-12-27 19:30:41.138289: step 117700/136300 (epoch 87/100), loss = 0.169831 (0.216 sec/batch), lr: 0.026706
2019-12-27 19:30:45.970457: step 117720/136300 (epoch 87/100), loss = 0.479110 (0.225 sec/batch), lr: 0.026706
2019-12-27 19:30:50.842572: step 117740/136300 (epoch 87/100), loss = 0.155073 (0.248 sec/batch), lr: 0.026706
2019-12-27 19:30:55.827581: step 117760/136300 (epoch 87/100), loss = 0.246764 (0.237 sec/batch), lr: 0.026706
2019-12-27 19:31:00.592969: step 117780/136300 (epoch 87/100), loss = 0.163777 (0.242 sec/batch), lr: 0.026706
2019-12-27 19:31:05.410714: step 117800/136300 (epoch 87/100), loss = 0.244019 (0.218 sec/batch), lr: 0.026706
2019-12-27 19:31:10.120003: step 117820/136300 (epoch 87/100), loss = 0.148868 (0.215 sec/batch), lr: 0.026706
2019-12-27 19:31:16.161293: step 117840/136300 (epoch 87/100), loss = 0.137297 (0.224 sec/batch), lr: 0.026706
2019-12-27 19:31:20.951237: step 117860/136300 (epoch 87/100), loss = 0.283724 (0.219 sec/batch), lr: 0.026706
2019-12-27 19:31:25.744022: step 117880/136300 (epoch 87/100), loss = 0.087783 (0.225 sec/batch), lr: 0.026706
2019-12-27 19:31:30.608878: step 117900/136300 (epoch 87/100), loss = 0.358351 (0.207 sec/batch), lr: 0.026706
2019-12-27 19:31:35.540116: step 117920/136300 (epoch 87/100), loss = 0.310204 (0.210 sec/batch), lr: 0.026706
2019-12-27 19:31:40.473710: step 117940/136300 (epoch 87/100), loss = 0.298456 (0.249 sec/batch), lr: 0.026706
2019-12-27 19:31:45.369390: step 117960/136300 (epoch 87/100), loss = 0.218809 (0.250 sec/batch), lr: 0.026706
2019-12-27 19:31:50.142351: step 117980/136300 (epoch 87/100), loss = 0.114245 (0.213 sec/batch), lr: 0.026706
2019-12-27 19:31:54.978616: step 118000/136300 (epoch 87/100), loss = 0.129988 (0.245 sec/batch), lr: 0.026706
2019-12-27 19:32:01.559935: step 118020/136300 (epoch 87/100), loss = 0.178674 (0.221 sec/batch), lr: 0.026706
2019-12-27 19:32:06.338206: step 118040/136300 (epoch 87/100), loss = 0.295661 (0.202 sec/batch), lr: 0.026706
2019-12-27 19:32:11.271445: step 118060/136300 (epoch 87/100), loss = 0.122120 (0.252 sec/batch), lr: 0.026706
2019-12-27 19:32:16.108207: step 118080/136300 (epoch 87/100), loss = 0.106141 (0.216 sec/batch), lr: 0.026706
2019-12-27 19:32:20.827161: step 118100/136300 (epoch 87/100), loss = 0.280133 (0.211 sec/batch), lr: 0.026706
2019-12-27 19:32:25.693977: step 118120/136300 (epoch 87/100), loss = 0.137505 (0.231 sec/batch), lr: 0.026706
2019-12-27 19:32:30.636469: step 118140/136300 (epoch 87/100), loss = 0.135415 (0.224 sec/batch), lr: 0.026706
2019-12-27 19:32:35.451130: step 118160/136300 (epoch 87/100), loss = 0.081093 (0.249 sec/batch), lr: 0.026706
2019-12-27 19:32:40.454459: step 118180/136300 (epoch 87/100), loss = 0.210897 (0.248 sec/batch), lr: 0.026706
2019-12-27 19:32:45.327123: step 118200/136300 (epoch 87/100), loss = 0.310491 (0.243 sec/batch), lr: 0.026706
2019-12-27 19:32:51.549486: step 118220/136300 (epoch 87/100), loss = 0.213557 (1.638 sec/batch), lr: 0.026706
2019-12-27 19:32:56.383935: step 118240/136300 (epoch 87/100), loss = 0.085935 (0.223 sec/batch), lr: 0.026706
2019-12-27 19:33:01.066737: step 118260/136300 (epoch 87/100), loss = 0.302151 (0.200 sec/batch), lr: 0.026706
2019-12-27 19:33:05.830567: step 118280/136300 (epoch 87/100), loss = 0.336987 (0.234 sec/batch), lr: 0.026706
2019-12-27 19:33:10.703881: step 118300/136300 (epoch 87/100), loss = 0.107400 (0.231 sec/batch), lr: 0.026706
2019-12-27 19:33:15.485563: step 118320/136300 (epoch 87/100), loss = 0.092530 (0.234 sec/batch), lr: 0.026706
2019-12-27 19:33:20.416703: step 118340/136300 (epoch 87/100), loss = 0.265738 (0.245 sec/batch), lr: 0.026706
2019-12-27 19:33:25.294004: step 118360/136300 (epoch 87/100), loss = 0.164634 (0.235 sec/batch), lr: 0.026706
2019-12-27 19:33:30.064057: step 118380/136300 (epoch 87/100), loss = 0.125295 (0.248 sec/batch), lr: 0.026706
2019-12-27 19:33:34.830581: step 118400/136300 (epoch 87/100), loss = 0.227423 (0.246 sec/batch), lr: 0.026706
2019-12-27 19:33:41.073326: step 118420/136300 (epoch 87/100), loss = 0.163351 (0.240 sec/batch), lr: 0.026706
2019-12-27 19:33:45.976596: step 118440/136300 (epoch 87/100), loss = 0.235537 (0.237 sec/batch), lr: 0.026706
2019-12-27 19:33:50.903718: step 118460/136300 (epoch 87/100), loss = 0.101025 (0.240 sec/batch), lr: 0.026706
2019-12-27 19:33:55.723032: step 118480/136300 (epoch 87/100), loss = 0.289608 (0.243 sec/batch), lr: 0.026706
2019-12-27 19:34:00.438520: step 118500/136300 (epoch 87/100), loss = 0.157140 (0.230 sec/batch), lr: 0.026706
2019-12-27 19:34:05.212137: step 118520/136300 (epoch 87/100), loss = 0.138355 (0.211 sec/batch), lr: 0.026706
2019-12-27 19:34:10.058400: step 118540/136300 (epoch 87/100), loss = 0.171494 (0.243 sec/batch), lr: 0.026706
2019-12-27 19:34:14.982877: step 118560/136300 (epoch 87/100), loss = 0.156778 (0.245 sec/batch), lr: 0.026706
2019-12-27 19:34:19.882571: step 118580/136300 (epoch 87/100), loss = 0.228433 (0.202 sec/batch), lr: 0.026706
Evaluating on dev set...
Precision (micro): 71.739%
   Recall (micro): 63.227%
       F1 (micro): 67.214%
epoch 87: train_loss = 0.211570, dev_loss = 0.457732, dev_f1 = 0.6721
model saved to ./saved_models/01/checkpoint_epoch_87.pt
new best model saved.

2019-12-27 19:34:59.875033: step 118600/136300 (epoch 88/100), loss = 0.169471 (0.229 sec/batch), lr: 0.026706
2019-12-27 19:35:04.594612: step 118620/136300 (epoch 88/100), loss = 0.156356 (0.215 sec/batch), lr: 0.026706
2019-12-27 19:35:09.449760: step 118640/136300 (epoch 88/100), loss = 0.193322 (0.216 sec/batch), lr: 0.026706
2019-12-27 19:35:14.322466: step 118660/136300 (epoch 88/100), loss = 0.156583 (0.238 sec/batch), lr: 0.026706
2019-12-27 19:35:19.137363: step 118680/136300 (epoch 88/100), loss = 0.102586 (0.241 sec/batch), lr: 0.026706
2019-12-27 19:35:23.905738: step 118700/136300 (epoch 88/100), loss = 0.168600 (0.225 sec/batch), lr: 0.026706
2019-12-27 19:35:28.702636: step 118720/136300 (epoch 88/100), loss = 0.311065 (0.218 sec/batch), lr: 0.026706
2019-12-27 19:35:33.530081: step 118740/136300 (epoch 88/100), loss = 0.125863 (0.232 sec/batch), lr: 0.026706
2019-12-27 19:35:38.444700: step 118760/136300 (epoch 88/100), loss = 0.135310 (0.241 sec/batch), lr: 0.026706
2019-12-27 19:35:43.305064: step 118780/136300 (epoch 88/100), loss = 0.302438 (0.223 sec/batch), lr: 0.026706
2019-12-27 19:35:49.468398: step 118800/136300 (epoch 88/100), loss = 0.312137 (0.227 sec/batch), lr: 0.026706
2019-12-27 19:35:54.229642: step 118820/136300 (epoch 88/100), loss = 0.162048 (0.245 sec/batch), lr: 0.026706
2019-12-27 19:35:59.038289: step 118840/136300 (epoch 88/100), loss = 0.202285 (0.239 sec/batch), lr: 0.026706
2019-12-27 19:36:03.903933: step 118860/136300 (epoch 88/100), loss = 0.229646 (0.232 sec/batch), lr: 0.026706
2019-12-27 19:36:08.696992: step 118880/136300 (epoch 88/100), loss = 0.393529 (0.213 sec/batch), lr: 0.026706
2019-12-27 19:36:13.473308: step 118900/136300 (epoch 88/100), loss = 0.284298 (0.242 sec/batch), lr: 0.026706
2019-12-27 19:36:18.298585: step 118920/136300 (epoch 88/100), loss = 0.266071 (0.199 sec/batch), lr: 0.026706
2019-12-27 19:36:23.178102: step 118940/136300 (epoch 88/100), loss = 0.154630 (0.244 sec/batch), lr: 0.026706
2019-12-27 19:36:27.930845: step 118960/136300 (epoch 88/100), loss = 0.146012 (0.245 sec/batch), lr: 0.026706
2019-12-27 19:36:32.697937: step 118980/136300 (epoch 88/100), loss = 0.220970 (0.236 sec/batch), lr: 0.026706
2019-12-27 19:36:38.876021: step 119000/136300 (epoch 88/100), loss = 0.197141 (0.241 sec/batch), lr: 0.026706
2019-12-27 19:36:43.728248: step 119020/136300 (epoch 88/100), loss = 0.315307 (0.236 sec/batch), lr: 0.026706
2019-12-27 19:36:48.590363: step 119040/136300 (epoch 88/100), loss = 0.106998 (0.207 sec/batch), lr: 0.026706
2019-12-27 19:36:53.441761: step 119060/136300 (epoch 88/100), loss = 0.283332 (0.237 sec/batch), lr: 0.026706
2019-12-27 19:36:58.247933: step 119080/136300 (epoch 88/100), loss = 0.082105 (0.227 sec/batch), lr: 0.026706
2019-12-27 19:37:03.137606: step 119100/136300 (epoch 88/100), loss = 0.144584 (0.232 sec/batch), lr: 0.026706
2019-12-27 19:37:08.065635: step 119120/136300 (epoch 88/100), loss = 0.216855 (0.214 sec/batch), lr: 0.026706
2019-12-27 19:37:12.890323: step 119140/136300 (epoch 88/100), loss = 0.180955 (0.243 sec/batch), lr: 0.026706
2019-12-27 19:37:17.714203: step 119160/136300 (epoch 88/100), loss = 0.180078 (0.237 sec/batch), lr: 0.026706
2019-12-27 19:37:22.431655: step 119180/136300 (epoch 88/100), loss = 0.247817 (0.224 sec/batch), lr: 0.026706
2019-12-27 19:37:28.417666: step 119200/136300 (epoch 88/100), loss = 0.224405 (0.238 sec/batch), lr: 0.026706
2019-12-27 19:37:33.270337: step 119220/136300 (epoch 88/100), loss = 0.351845 (0.227 sec/batch), lr: 0.026706
2019-12-27 19:37:38.020509: step 119240/136300 (epoch 88/100), loss = 0.245155 (0.224 sec/batch), lr: 0.026706
2019-12-27 19:37:42.882449: step 119260/136300 (epoch 88/100), loss = 0.187478 (0.242 sec/batch), lr: 0.026706
2019-12-27 19:37:47.806517: step 119280/136300 (epoch 88/100), loss = 0.109978 (0.231 sec/batch), lr: 0.026706
2019-12-27 19:37:52.721091: step 119300/136300 (epoch 88/100), loss = 0.164024 (0.239 sec/batch), lr: 0.026706
2019-12-27 19:37:57.646281: step 119320/136300 (epoch 88/100), loss = 0.232047 (0.248 sec/batch), lr: 0.026706
2019-12-27 19:38:02.396381: step 119340/136300 (epoch 88/100), loss = 0.103238 (0.213 sec/batch), lr: 0.026706
2019-12-27 19:38:07.193764: step 119360/136300 (epoch 88/100), loss = 0.195526 (0.173 sec/batch), lr: 0.026706
2019-12-27 19:38:13.741081: step 119380/136300 (epoch 88/100), loss = 0.263743 (0.243 sec/batch), lr: 0.026706
2019-12-27 19:38:18.501428: step 119400/136300 (epoch 88/100), loss = 0.280546 (0.220 sec/batch), lr: 0.026706
2019-12-27 19:38:23.376751: step 119420/136300 (epoch 88/100), loss = 0.244957 (0.237 sec/batch), lr: 0.026706
2019-12-27 19:38:28.274592: step 119440/136300 (epoch 88/100), loss = 0.448816 (0.211 sec/batch), lr: 0.026706
2019-12-27 19:38:32.995255: step 119460/136300 (epoch 88/100), loss = 0.204420 (0.237 sec/batch), lr: 0.026706
2019-12-27 19:38:37.857512: step 119480/136300 (epoch 88/100), loss = 0.137730 (0.211 sec/batch), lr: 0.026706
2019-12-27 19:38:42.781147: step 119500/136300 (epoch 88/100), loss = 0.178370 (0.219 sec/batch), lr: 0.026706
2019-12-27 19:38:47.614361: step 119520/136300 (epoch 88/100), loss = 0.257070 (0.245 sec/batch), lr: 0.026706
2019-12-27 19:38:52.574583: step 119540/136300 (epoch 88/100), loss = 0.242322 (0.232 sec/batch), lr: 0.026706
2019-12-27 19:38:57.472255: step 119560/136300 (epoch 88/100), loss = 0.088850 (0.247 sec/batch), lr: 0.026706
2019-12-27 19:39:02.274387: step 119580/136300 (epoch 88/100), loss = 0.314123 (0.236 sec/batch), lr: 0.026706
2019-12-27 19:39:08.463989: step 119600/136300 (epoch 88/100), loss = 0.120655 (0.225 sec/batch), lr: 0.026706
2019-12-27 19:39:13.233265: step 119620/136300 (epoch 88/100), loss = 0.291020 (0.203 sec/batch), lr: 0.026706
2019-12-27 19:39:17.948050: step 119640/136300 (epoch 88/100), loss = 0.357448 (0.216 sec/batch), lr: 0.026706
2019-12-27 19:39:22.737992: step 119660/136300 (epoch 88/100), loss = 0.270565 (0.237 sec/batch), lr: 0.026706
2019-12-27 19:39:27.517058: step 119680/136300 (epoch 88/100), loss = 0.335722 (0.240 sec/batch), lr: 0.026706
2019-12-27 19:39:32.445405: step 119700/136300 (epoch 88/100), loss = 0.207132 (0.240 sec/batch), lr: 0.026706
2019-12-27 19:39:37.283415: step 119720/136300 (epoch 88/100), loss = 0.094466 (0.239 sec/batch), lr: 0.026706
2019-12-27 19:39:42.046247: step 119740/136300 (epoch 88/100), loss = 0.217671 (0.235 sec/batch), lr: 0.026706
2019-12-27 19:39:46.765234: step 119760/136300 (epoch 88/100), loss = 0.183517 (0.234 sec/batch), lr: 0.026706
2019-12-27 19:39:51.599185: step 119780/136300 (epoch 88/100), loss = 0.258693 (0.231 sec/batch), lr: 0.026706
2019-12-27 19:39:57.897345: step 119800/136300 (epoch 88/100), loss = 0.205914 (0.239 sec/batch), lr: 0.026706
2019-12-27 19:40:02.801650: step 119820/136300 (epoch 88/100), loss = 0.152127 (0.243 sec/batch), lr: 0.026706
2019-12-27 19:40:07.643238: step 119840/136300 (epoch 88/100), loss = 0.266026 (0.220 sec/batch), lr: 0.026706
2019-12-27 19:40:12.328102: step 119860/136300 (epoch 88/100), loss = 0.182709 (0.212 sec/batch), lr: 0.026706
2019-12-27 19:40:17.181540: step 119880/136300 (epoch 88/100), loss = 0.210408 (0.237 sec/batch), lr: 0.026706
2019-12-27 19:40:21.940034: step 119900/136300 (epoch 88/100), loss = 0.354072 (0.239 sec/batch), lr: 0.026706
2019-12-27 19:40:26.837561: step 119920/136300 (epoch 88/100), loss = 0.127023 (0.200 sec/batch), lr: 0.026706
2019-12-27 19:40:31.795148: step 119940/136300 (epoch 88/100), loss = 0.135009 (0.240 sec/batch), lr: 0.026706
Evaluating on dev set...
Precision (micro): 71.802%
   Recall (micro): 63.098%
       F1 (micro): 67.169%
epoch 88: train_loss = 0.209873, dev_loss = 0.464280, dev_f1 = 0.6717
model saved to ./saved_models/01/checkpoint_epoch_88.pt

2019-12-27 19:41:10.058663: step 119960/136300 (epoch 89/100), loss = 0.211215 (0.223 sec/batch), lr: 0.024036
2019-12-27 19:41:16.204901: step 119980/136300 (epoch 89/100), loss = 0.256542 (0.205 sec/batch), lr: 0.024036
2019-12-27 19:41:21.073933: step 120000/136300 (epoch 89/100), loss = 0.070605 (0.220 sec/batch), lr: 0.024036
2019-12-27 19:41:25.881431: step 120020/136300 (epoch 89/100), loss = 0.306972 (0.243 sec/batch), lr: 0.024036
2019-12-27 19:41:30.725936: step 120040/136300 (epoch 89/100), loss = 0.187734 (0.203 sec/batch), lr: 0.024036
2019-12-27 19:41:35.486160: step 120060/136300 (epoch 89/100), loss = 0.136970 (0.217 sec/batch), lr: 0.024036
2019-12-27 19:41:40.308482: step 120080/136300 (epoch 89/100), loss = 0.243688 (0.234 sec/batch), lr: 0.024036
2019-12-27 19:41:45.122618: step 120100/136300 (epoch 89/100), loss = 0.229709 (0.225 sec/batch), lr: 0.024036
2019-12-27 19:41:49.990354: step 120120/136300 (epoch 89/100), loss = 0.213429 (0.235 sec/batch), lr: 0.024036
2019-12-27 19:41:54.860488: step 120140/136300 (epoch 89/100), loss = 0.142246 (0.247 sec/batch), lr: 0.024036
2019-12-27 19:42:01.246159: step 120160/136300 (epoch 89/100), loss = 0.071300 (0.226 sec/batch), lr: 0.024036
2019-12-27 19:42:05.974655: step 120180/136300 (epoch 89/100), loss = 0.334666 (0.226 sec/batch), lr: 0.024036
2019-12-27 19:42:10.775444: step 120200/136300 (epoch 89/100), loss = 0.209153 (0.201 sec/batch), lr: 0.024036
2019-12-27 19:42:15.650390: step 120220/136300 (epoch 89/100), loss = 0.212870 (0.205 sec/batch), lr: 0.024036
2019-12-27 19:42:20.512823: step 120240/136300 (epoch 89/100), loss = 0.221392 (0.234 sec/batch), lr: 0.024036
2019-12-27 19:42:25.224126: step 120260/136300 (epoch 89/100), loss = 0.231291 (0.217 sec/batch), lr: 0.024036
2019-12-27 19:42:30.060942: step 120280/136300 (epoch 89/100), loss = 0.234010 (0.202 sec/batch), lr: 0.024036
2019-12-27 19:42:34.916119: step 120300/136300 (epoch 89/100), loss = 0.129918 (0.232 sec/batch), lr: 0.024036
2019-12-27 19:42:39.661935: step 120320/136300 (epoch 89/100), loss = 0.143957 (0.236 sec/batch), lr: 0.024036
2019-12-27 19:42:44.452104: step 120340/136300 (epoch 89/100), loss = 0.223437 (0.235 sec/batch), lr: 0.024036
2019-12-27 19:42:50.693331: step 120360/136300 (epoch 89/100), loss = 0.175933 (0.230 sec/batch), lr: 0.024036
2019-12-27 19:42:55.534968: step 120380/136300 (epoch 89/100), loss = 0.275353 (0.248 sec/batch), lr: 0.024036
2019-12-27 19:43:00.418162: step 120400/136300 (epoch 89/100), loss = 0.095539 (0.205 sec/batch), lr: 0.024036
2019-12-27 19:43:05.258358: step 120420/136300 (epoch 89/100), loss = 0.145897 (0.232 sec/batch), lr: 0.024036
2019-12-27 19:43:10.088419: step 120440/136300 (epoch 89/100), loss = 0.143230 (0.215 sec/batch), lr: 0.024036
2019-12-27 19:43:14.960201: step 120460/136300 (epoch 89/100), loss = 0.128295 (0.211 sec/batch), lr: 0.024036
2019-12-27 19:43:19.879234: step 120480/136300 (epoch 89/100), loss = 0.202325 (0.243 sec/batch), lr: 0.024036
2019-12-27 19:43:24.691872: step 120500/136300 (epoch 89/100), loss = 0.476590 (0.231 sec/batch), lr: 0.024036
2019-12-27 19:43:29.529854: step 120520/136300 (epoch 89/100), loss = 0.321116 (0.206 sec/batch), lr: 0.024036
2019-12-27 19:43:34.276557: step 120540/136300 (epoch 89/100), loss = 0.227927 (0.206 sec/batch), lr: 0.024036
2019-12-27 19:43:40.431783: step 120560/136300 (epoch 89/100), loss = 0.168052 (0.228 sec/batch), lr: 0.024036
2019-12-27 19:43:45.238539: step 120580/136300 (epoch 89/100), loss = 0.108855 (0.214 sec/batch), lr: 0.024036
2019-12-27 19:43:49.982396: step 120600/136300 (epoch 89/100), loss = 0.183394 (0.211 sec/batch), lr: 0.024036
2019-12-27 19:43:54.814527: step 120620/136300 (epoch 89/100), loss = 0.225695 (0.212 sec/batch), lr: 0.024036
2019-12-27 19:43:59.793543: step 120640/136300 (epoch 89/100), loss = 0.223738 (0.239 sec/batch), lr: 0.024036
2019-12-27 19:44:04.669239: step 120660/136300 (epoch 89/100), loss = 0.229405 (0.243 sec/batch), lr: 0.024036
2019-12-27 19:44:09.569748: step 120680/136300 (epoch 89/100), loss = 0.187260 (0.206 sec/batch), lr: 0.024036
2019-12-27 19:44:14.361629: step 120700/136300 (epoch 89/100), loss = 0.246702 (0.182 sec/batch), lr: 0.024036
2019-12-27 19:44:19.197892: step 120720/136300 (epoch 89/100), loss = 0.111885 (0.227 sec/batch), lr: 0.024036
2019-12-27 19:44:24.101044: step 120740/136300 (epoch 89/100), loss = 0.185400 (0.228 sec/batch), lr: 0.024036
2019-12-27 19:44:30.332200: step 120760/136300 (epoch 89/100), loss = 0.246696 (0.244 sec/batch), lr: 0.024036
2019-12-27 19:44:35.104485: step 120780/136300 (epoch 89/100), loss = 0.176065 (0.217 sec/batch), lr: 0.024036
2019-12-27 19:44:40.062807: step 120800/136300 (epoch 89/100), loss = 0.201748 (0.246 sec/batch), lr: 0.024036
2019-12-27 19:44:44.759453: step 120820/136300 (epoch 89/100), loss = 0.168881 (0.241 sec/batch), lr: 0.024036
2019-12-27 19:44:49.626499: step 120840/136300 (epoch 89/100), loss = 0.158243 (0.230 sec/batch), lr: 0.024036
2019-12-27 19:44:54.540958: step 120860/136300 (epoch 89/100), loss = 0.243994 (0.202 sec/batch), lr: 0.024036
2019-12-27 19:44:59.330869: step 120880/136300 (epoch 89/100), loss = 0.153602 (0.236 sec/batch), lr: 0.024036
2019-12-27 19:45:04.290130: step 120900/136300 (epoch 89/100), loss = 0.221888 (0.224 sec/batch), lr: 0.024036
2019-12-27 19:45:09.198878: step 120920/136300 (epoch 89/100), loss = 0.286296 (0.239 sec/batch), lr: 0.024036
2019-12-27 19:45:14.022571: step 120940/136300 (epoch 89/100), loss = 0.222994 (0.229 sec/batch), lr: 0.024036
2019-12-27 19:45:20.512442: step 120960/136300 (epoch 89/100), loss = 0.104888 (0.232 sec/batch), lr: 0.024036
2019-12-27 19:45:25.347377: step 120980/136300 (epoch 89/100), loss = 0.141840 (0.201 sec/batch), lr: 0.024036
2019-12-27 19:45:30.012147: step 121000/136300 (epoch 89/100), loss = 0.156778 (0.217 sec/batch), lr: 0.024036
2019-12-27 19:45:34.787225: step 121020/136300 (epoch 89/100), loss = 0.296386 (0.235 sec/batch), lr: 0.024036
2019-12-27 19:45:39.621845: step 121040/136300 (epoch 89/100), loss = 0.241697 (0.233 sec/batch), lr: 0.024036
2019-12-27 19:45:44.451590: step 121060/136300 (epoch 89/100), loss = 0.090482 (0.236 sec/batch), lr: 0.024036
2019-12-27 19:45:49.354253: step 121080/136300 (epoch 89/100), loss = 0.115123 (0.230 sec/batch), lr: 0.024036
2019-12-27 19:45:54.114825: step 121100/136300 (epoch 89/100), loss = 0.278250 (0.232 sec/batch), lr: 0.024036
2019-12-27 19:45:58.786172: step 121120/136300 (epoch 89/100), loss = 0.211831 (0.215 sec/batch), lr: 0.024036
2019-12-27 19:46:03.660682: step 121140/136300 (epoch 89/100), loss = 0.160530 (0.232 sec/batch), lr: 0.024036
2019-12-27 19:46:09.840686: step 121160/136300 (epoch 89/100), loss = 0.210758 (0.228 sec/batch), lr: 0.024036
2019-12-27 19:46:14.749873: step 121180/136300 (epoch 89/100), loss = 0.403093 (0.213 sec/batch), lr: 0.024036
2019-12-27 19:46:19.629682: step 121200/136300 (epoch 89/100), loss = 0.208500 (0.210 sec/batch), lr: 0.024036
2019-12-27 19:46:24.297120: step 121220/136300 (epoch 89/100), loss = 0.139995 (0.245 sec/batch), lr: 0.024036
2019-12-27 19:46:29.150381: step 121240/136300 (epoch 89/100), loss = 0.347472 (0.244 sec/batch), lr: 0.024036
2019-12-27 19:46:33.933033: step 121260/136300 (epoch 89/100), loss = 0.129562 (0.203 sec/batch), lr: 0.024036
2019-12-27 19:46:38.880460: step 121280/136300 (epoch 89/100), loss = 0.239427 (0.232 sec/batch), lr: 0.024036
2019-12-27 19:46:43.760741: step 121300/136300 (epoch 89/100), loss = 0.217015 (0.242 sec/batch), lr: 0.024036
Evaluating on dev set...
Precision (micro): 71.288%
   Recall (micro): 63.852%
       F1 (micro): 67.365%
epoch 89: train_loss = 0.210670, dev_loss = 0.459231, dev_f1 = 0.6737
model saved to ./saved_models/01/checkpoint_epoch_89.pt
new best model saved.

2019-12-27 19:47:22.369050: step 121320/136300 (epoch 90/100), loss = 0.264353 (0.236 sec/batch), lr: 0.024036
2019-12-27 19:47:28.609860: step 121340/136300 (epoch 90/100), loss = 0.198026 (0.250 sec/batch), lr: 0.024036
2019-12-27 19:47:33.444679: step 121360/136300 (epoch 90/100), loss = 0.080726 (0.243 sec/batch), lr: 0.024036
2019-12-27 19:47:38.167663: step 121380/136300 (epoch 90/100), loss = 0.208963 (0.221 sec/batch), lr: 0.024036
2019-12-27 19:47:43.105880: step 121400/136300 (epoch 90/100), loss = 0.187622 (0.238 sec/batch), lr: 0.024036
2019-12-27 19:47:47.852684: step 121420/136300 (epoch 90/100), loss = 0.138905 (0.219 sec/batch), lr: 0.024036
2019-12-27 19:47:52.680508: step 121440/136300 (epoch 90/100), loss = 0.211638 (0.236 sec/batch), lr: 0.024036
2019-12-27 19:47:57.447846: step 121460/136300 (epoch 90/100), loss = 0.205828 (0.182 sec/batch), lr: 0.024036
2019-12-27 19:48:02.327920: step 121480/136300 (epoch 90/100), loss = 0.102718 (0.232 sec/batch), lr: 0.024036
2019-12-27 19:48:07.222629: step 121500/136300 (epoch 90/100), loss = 0.233132 (0.205 sec/batch), lr: 0.024036
2019-12-27 19:48:13.629588: step 121520/136300 (epoch 90/100), loss = 0.087884 (1.789 sec/batch), lr: 0.024036
2019-12-27 19:48:18.329544: step 121540/136300 (epoch 90/100), loss = 0.202976 (0.180 sec/batch), lr: 0.024036
2019-12-27 19:48:23.114514: step 121560/136300 (epoch 90/100), loss = 0.156872 (0.240 sec/batch), lr: 0.024036
2019-12-27 19:48:28.020863: step 121580/136300 (epoch 90/100), loss = 0.263583 (0.232 sec/batch), lr: 0.024036
2019-12-27 19:48:32.844419: step 121600/136300 (epoch 90/100), loss = 0.145655 (0.240 sec/batch), lr: 0.024036
2019-12-27 19:48:37.554135: step 121620/136300 (epoch 90/100), loss = 0.233558 (0.201 sec/batch), lr: 0.024036
2019-12-27 19:48:42.363161: step 121640/136300 (epoch 90/100), loss = 0.181388 (0.234 sec/batch), lr: 0.024036
2019-12-27 19:48:47.202540: step 121660/136300 (epoch 90/100), loss = 0.108223 (0.246 sec/batch), lr: 0.024036
2019-12-27 19:48:51.948959: step 121680/136300 (epoch 90/100), loss = 0.216805 (0.180 sec/batch), lr: 0.024036
2019-12-27 19:48:56.710420: step 121700/136300 (epoch 90/100), loss = 0.155661 (0.211 sec/batch), lr: 0.024036
2019-12-27 19:49:02.784876: step 121720/136300 (epoch 90/100), loss = 0.142398 (0.233 sec/batch), lr: 0.024036
2019-12-27 19:49:07.617587: step 121740/136300 (epoch 90/100), loss = 0.291866 (0.240 sec/batch), lr: 0.024036
2019-12-27 19:49:12.497314: step 121760/136300 (epoch 90/100), loss = 0.197744 (0.213 sec/batch), lr: 0.024036
2019-12-27 19:49:17.323375: step 121780/136300 (epoch 90/100), loss = 0.279212 (0.201 sec/batch), lr: 0.024036
2019-12-27 19:49:22.168321: step 121800/136300 (epoch 90/100), loss = 0.269597 (0.228 sec/batch), lr: 0.024036
2019-12-27 19:49:27.070519: step 121820/136300 (epoch 90/100), loss = 0.327299 (0.246 sec/batch), lr: 0.024036
2019-12-27 19:49:31.933054: step 121840/136300 (epoch 90/100), loss = 0.320589 (0.227 sec/batch), lr: 0.024036
2019-12-27 19:49:36.787453: step 121860/136300 (epoch 90/100), loss = 0.228308 (0.216 sec/batch), lr: 0.024036
2019-12-27 19:49:41.617761: step 121880/136300 (epoch 90/100), loss = 0.210610 (0.238 sec/batch), lr: 0.024036
2019-12-27 19:49:46.417449: step 121900/136300 (epoch 90/100), loss = 0.294199 (0.230 sec/batch), lr: 0.024036
2019-12-27 19:49:52.717843: step 121920/136300 (epoch 90/100), loss = 0.214599 (0.217 sec/batch), lr: 0.024036
2019-12-27 19:49:57.515554: step 121940/136300 (epoch 90/100), loss = 0.090973 (0.218 sec/batch), lr: 0.024036
2019-12-27 19:50:02.278201: step 121960/136300 (epoch 90/100), loss = 0.238981 (0.237 sec/batch), lr: 0.024036
2019-12-27 19:50:07.109965: step 121980/136300 (epoch 90/100), loss = 0.125302 (0.240 sec/batch), lr: 0.024036
2019-12-27 19:50:12.021969: step 122000/136300 (epoch 90/100), loss = 0.167850 (0.221 sec/batch), lr: 0.024036
2019-12-27 19:50:16.905143: step 122020/136300 (epoch 90/100), loss = 0.251167 (0.232 sec/batch), lr: 0.024036
2019-12-27 19:50:21.817144: step 122040/136300 (epoch 90/100), loss = 0.226160 (0.238 sec/batch), lr: 0.024036
2019-12-27 19:50:26.653181: step 122060/136300 (epoch 90/100), loss = 0.320854 (0.229 sec/batch), lr: 0.024036
2019-12-27 19:50:31.403225: step 122080/136300 (epoch 90/100), loss = 0.358964 (0.214 sec/batch), lr: 0.024036
2019-12-27 19:50:36.336258: step 122100/136300 (epoch 90/100), loss = 0.125286 (0.247 sec/batch), lr: 0.024036
2019-12-27 19:50:42.626246: step 122120/136300 (epoch 90/100), loss = 0.119069 (0.219 sec/batch), lr: 0.024036
2019-12-27 19:50:47.415106: step 122140/136300 (epoch 90/100), loss = 0.353108 (0.221 sec/batch), lr: 0.024036
2019-12-27 19:50:52.334229: step 122160/136300 (epoch 90/100), loss = 0.340329 (0.213 sec/batch), lr: 0.024036
2019-12-27 19:50:57.011640: step 122180/136300 (epoch 90/100), loss = 0.247243 (0.229 sec/batch), lr: 0.024036
2019-12-27 19:51:01.917648: step 122200/136300 (epoch 90/100), loss = 0.163724 (0.219 sec/batch), lr: 0.024036
2019-12-27 19:51:06.855411: step 122220/136300 (epoch 90/100), loss = 0.161389 (0.231 sec/batch), lr: 0.024036
2019-12-27 19:51:11.662740: step 122240/136300 (epoch 90/100), loss = 0.159534 (0.227 sec/batch), lr: 0.024036
2019-12-27 19:51:16.618490: step 122260/136300 (epoch 90/100), loss = 0.258567 (0.233 sec/batch), lr: 0.024036
2019-12-27 19:51:21.497404: step 122280/136300 (epoch 90/100), loss = 0.288687 (0.207 sec/batch), lr: 0.024036
2019-12-27 19:51:26.343850: step 122300/136300 (epoch 90/100), loss = 0.244209 (0.228 sec/batch), lr: 0.024036
2019-12-27 19:51:32.669553: step 122320/136300 (epoch 90/100), loss = 0.089445 (0.238 sec/batch), lr: 0.024036
2019-12-27 19:51:37.492135: step 122340/136300 (epoch 90/100), loss = 0.119641 (0.180 sec/batch), lr: 0.024036
2019-12-27 19:51:42.178993: step 122360/136300 (epoch 90/100), loss = 0.126765 (0.236 sec/batch), lr: 0.024036
2019-12-27 19:51:46.910370: step 122380/136300 (epoch 90/100), loss = 0.168656 (0.219 sec/batch), lr: 0.024036
2019-12-27 19:51:51.762328: step 122400/136300 (epoch 90/100), loss = 0.186531 (0.241 sec/batch), lr: 0.024036
2019-12-27 19:51:56.567559: step 122420/136300 (epoch 90/100), loss = 0.261669 (0.215 sec/batch), lr: 0.024036
2019-12-27 19:52:01.462731: step 122440/136300 (epoch 90/100), loss = 0.219850 (0.204 sec/batch), lr: 0.024036
2019-12-27 19:52:06.202364: step 122460/136300 (epoch 90/100), loss = 0.216721 (0.213 sec/batch), lr: 0.024036
2019-12-27 19:52:10.929016: step 122480/136300 (epoch 90/100), loss = 0.202923 (0.247 sec/batch), lr: 0.024036
2019-12-27 19:52:15.750904: step 122500/136300 (epoch 90/100), loss = 0.318557 (0.233 sec/batch), lr: 0.024036
2019-12-27 19:52:22.066334: step 122520/136300 (epoch 90/100), loss = 0.221524 (0.242 sec/batch), lr: 0.024036
2019-12-27 19:52:26.975378: step 122540/136300 (epoch 90/100), loss = 0.310867 (0.247 sec/batch), lr: 0.024036
2019-12-27 19:52:31.876969: step 122560/136300 (epoch 90/100), loss = 0.101373 (0.206 sec/batch), lr: 0.024036
2019-12-27 19:52:36.525725: step 122580/136300 (epoch 90/100), loss = 0.221926 (0.243 sec/batch), lr: 0.024036
2019-12-27 19:52:41.339359: step 122600/136300 (epoch 90/100), loss = 0.264357 (0.207 sec/batch), lr: 0.024036
2019-12-27 19:52:46.170464: step 122620/136300 (epoch 90/100), loss = 0.135936 (0.242 sec/batch), lr: 0.024036
2019-12-27 19:52:51.092381: step 122640/136300 (epoch 90/100), loss = 0.082511 (0.238 sec/batch), lr: 0.024036
2019-12-27 19:52:55.958569: step 122660/136300 (epoch 90/100), loss = 0.187118 (0.245 sec/batch), lr: 0.024036
Evaluating on dev set...
Precision (micro): 72.536%
   Recall (micro): 62.141%
       F1 (micro): 66.937%
epoch 90: train_loss = 0.208822, dev_loss = 0.468005, dev_f1 = 0.6694
model saved to ./saved_models/01/checkpoint_epoch_90.pt

2019-12-27 19:53:34.328634: step 122680/136300 (epoch 91/100), loss = 0.148077 (0.175 sec/batch), lr: 0.021632
2019-12-27 19:53:40.572644: step 122700/136300 (epoch 91/100), loss = 0.218074 (0.244 sec/batch), lr: 0.021632
2019-12-27 19:53:45.385596: step 122720/136300 (epoch 91/100), loss = 0.248035 (0.216 sec/batch), lr: 0.021632
2019-12-27 19:53:50.121159: step 122740/136300 (epoch 91/100), loss = 0.224116 (0.233 sec/batch), lr: 0.021632
2019-12-27 19:53:55.077982: step 122760/136300 (epoch 91/100), loss = 0.159897 (0.237 sec/batch), lr: 0.021632
2019-12-27 19:53:59.871030: step 122780/136300 (epoch 91/100), loss = 0.245945 (0.197 sec/batch), lr: 0.021632
2019-12-27 19:54:04.645291: step 122800/136300 (epoch 91/100), loss = 0.176462 (0.250 sec/batch), lr: 0.021632
2019-12-27 19:54:09.461452: step 122820/136300 (epoch 91/100), loss = 0.214824 (0.164 sec/batch), lr: 0.021632
2019-12-27 19:54:14.336471: step 122840/136300 (epoch 91/100), loss = 0.250110 (0.241 sec/batch), lr: 0.021632
2019-12-27 19:54:19.238969: step 122860/136300 (epoch 91/100), loss = 0.226067 (0.223 sec/batch), lr: 0.021632
2019-12-27 19:54:24.042877: step 122880/136300 (epoch 91/100), loss = 0.189826 (0.238 sec/batch), lr: 0.021632
2019-12-27 19:54:30.200693: step 122900/136300 (epoch 91/100), loss = 0.175171 (0.241 sec/batch), lr: 0.021632
2019-12-27 19:54:34.976787: step 122920/136300 (epoch 91/100), loss = 0.106177 (0.177 sec/batch), lr: 0.021632
2019-12-27 19:54:39.873282: step 122940/136300 (epoch 91/100), loss = 0.197102 (0.210 sec/batch), lr: 0.021632
2019-12-27 19:54:44.702787: step 122960/136300 (epoch 91/100), loss = 0.270292 (0.246 sec/batch), lr: 0.021632
2019-12-27 19:54:49.441742: step 122980/136300 (epoch 91/100), loss = 0.149613 (0.238 sec/batch), lr: 0.021632
2019-12-27 19:54:54.216294: step 123000/136300 (epoch 91/100), loss = 0.157785 (0.205 sec/batch), lr: 0.021632
2019-12-27 19:54:59.063070: step 123020/136300 (epoch 91/100), loss = 0.137993 (0.228 sec/batch), lr: 0.021632
2019-12-27 19:55:03.904593: step 123040/136300 (epoch 91/100), loss = 0.107630 (0.182 sec/batch), lr: 0.021632
2019-12-27 19:55:08.622750: step 123060/136300 (epoch 91/100), loss = 0.264935 (0.203 sec/batch), lr: 0.021632
2019-12-27 19:55:14.617759: step 123080/136300 (epoch 91/100), loss = 0.081394 (0.212 sec/batch), lr: 0.021632
2019-12-27 19:55:19.410642: step 123100/136300 (epoch 91/100), loss = 0.168443 (0.233 sec/batch), lr: 0.021632
2019-12-27 19:55:24.360215: step 123120/136300 (epoch 91/100), loss = 0.109022 (0.229 sec/batch), lr: 0.021632
2019-12-27 19:55:29.247204: step 123140/136300 (epoch 91/100), loss = 0.071193 (0.245 sec/batch), lr: 0.021632
2019-12-27 19:55:34.017410: step 123160/136300 (epoch 91/100), loss = 0.204712 (0.227 sec/batch), lr: 0.021632
2019-12-27 19:55:38.913380: step 123180/136300 (epoch 91/100), loss = 0.243026 (0.240 sec/batch), lr: 0.021632
2019-12-27 19:55:43.796307: step 123200/136300 (epoch 91/100), loss = 0.188139 (0.242 sec/batch), lr: 0.021632
2019-12-27 19:55:48.662717: step 123220/136300 (epoch 91/100), loss = 0.219342 (0.240 sec/batch), lr: 0.021632
2019-12-27 19:55:53.485477: step 123240/136300 (epoch 91/100), loss = 0.290635 (0.206 sec/batch), lr: 0.021632
2019-12-27 19:55:58.326564: step 123260/136300 (epoch 91/100), loss = 0.217351 (0.243 sec/batch), lr: 0.021632
2019-12-27 19:56:04.577920: step 123280/136300 (epoch 91/100), loss = 0.292331 (0.248 sec/batch), lr: 0.021632
2019-12-27 19:56:09.396329: step 123300/136300 (epoch 91/100), loss = 0.176445 (0.241 sec/batch), lr: 0.021632
2019-12-27 19:56:14.130220: step 123320/136300 (epoch 91/100), loss = 0.244187 (0.220 sec/batch), lr: 0.021632
2019-12-27 19:56:18.980367: step 123340/136300 (epoch 91/100), loss = 0.142302 (0.235 sec/batch), lr: 0.021632
2019-12-27 19:56:23.887217: step 123360/136300 (epoch 91/100), loss = 0.111458 (0.231 sec/batch), lr: 0.021632
2019-12-27 19:56:28.773567: step 123380/136300 (epoch 91/100), loss = 0.195305 (0.215 sec/batch), lr: 0.021632
2019-12-27 19:56:33.679591: step 123400/136300 (epoch 91/100), loss = 0.140581 (0.211 sec/batch), lr: 0.021632
2019-12-27 19:56:38.571097: step 123420/136300 (epoch 91/100), loss = 0.123379 (0.201 sec/batch), lr: 0.021632
2019-12-27 19:56:43.302282: step 123440/136300 (epoch 91/100), loss = 0.099710 (0.221 sec/batch), lr: 0.021632
2019-12-27 19:56:48.210361: step 123460/136300 (epoch 91/100), loss = 0.265631 (0.228 sec/batch), lr: 0.021632
2019-12-27 19:56:54.710123: step 123480/136300 (epoch 91/100), loss = 0.091676 (0.237 sec/batch), lr: 0.021632
2019-12-27 19:56:59.529546: step 123500/136300 (epoch 91/100), loss = 0.116455 (0.236 sec/batch), lr: 0.021632
2019-12-27 19:57:04.481723: step 123520/136300 (epoch 91/100), loss = 0.209498 (0.232 sec/batch), lr: 0.021632
2019-12-27 19:57:09.145818: step 123540/136300 (epoch 91/100), loss = 0.188013 (0.222 sec/batch), lr: 0.021632
2019-12-27 19:57:14.010218: step 123560/136300 (epoch 91/100), loss = 0.184175 (0.239 sec/batch), lr: 0.021632
2019-12-27 19:57:18.930168: step 123580/136300 (epoch 91/100), loss = 0.171450 (0.227 sec/batch), lr: 0.021632
2019-12-27 19:57:23.747415: step 123600/136300 (epoch 91/100), loss = 0.165874 (0.238 sec/batch), lr: 0.021632
2019-12-27 19:57:28.658992: step 123620/136300 (epoch 91/100), loss = 0.138953 (0.242 sec/batch), lr: 0.021632
2019-12-27 19:57:33.586691: step 123640/136300 (epoch 91/100), loss = 0.367794 (0.219 sec/batch), lr: 0.021632
2019-12-27 19:57:38.399717: step 123660/136300 (epoch 91/100), loss = 0.165123 (0.239 sec/batch), lr: 0.021632
2019-12-27 19:57:44.757524: step 123680/136300 (epoch 91/100), loss = 0.195476 (0.242 sec/batch), lr: 0.021632
2019-12-27 19:57:49.636284: step 123700/136300 (epoch 91/100), loss = 0.088599 (0.193 sec/batch), lr: 0.021632
2019-12-27 19:57:54.274522: step 123720/136300 (epoch 91/100), loss = 0.256249 (0.218 sec/batch), lr: 0.021632
2019-12-27 19:57:58.994201: step 123740/136300 (epoch 91/100), loss = 0.180586 (0.210 sec/batch), lr: 0.021632
2019-12-27 19:58:03.856676: step 123760/136300 (epoch 91/100), loss = 0.196170 (0.216 sec/batch), lr: 0.021632
2019-12-27 19:58:08.698881: step 123780/136300 (epoch 91/100), loss = 0.163660 (0.252 sec/batch), lr: 0.021632
2019-12-27 19:58:13.596003: step 123800/136300 (epoch 91/100), loss = 0.230640 (0.233 sec/batch), lr: 0.021632
2019-12-27 19:58:18.372692: step 123820/136300 (epoch 91/100), loss = 0.216014 (0.227 sec/batch), lr: 0.021632
2019-12-27 19:58:23.084731: step 123840/136300 (epoch 91/100), loss = 0.173881 (0.233 sec/batch), lr: 0.021632
2019-12-27 19:58:27.867982: step 123860/136300 (epoch 91/100), loss = 0.194909 (0.232 sec/batch), lr: 0.021632
2019-12-27 19:58:34.375317: step 123880/136300 (epoch 91/100), loss = 0.143293 (0.239 sec/batch), lr: 0.021632
2019-12-27 19:58:39.318758: step 123900/136300 (epoch 91/100), loss = 0.244497 (0.232 sec/batch), lr: 0.021632
2019-12-27 19:58:44.238661: step 123920/136300 (epoch 91/100), loss = 0.208924 (0.241 sec/batch), lr: 0.021632
2019-12-27 19:58:48.900762: step 123940/136300 (epoch 91/100), loss = 0.176932 (0.215 sec/batch), lr: 0.021632
2019-12-27 19:58:53.696340: step 123960/136300 (epoch 91/100), loss = 0.166987 (0.180 sec/batch), lr: 0.021632
2019-12-27 19:58:58.488045: step 123980/136300 (epoch 91/100), loss = 0.134092 (0.218 sec/batch), lr: 0.021632
2019-12-27 19:59:03.433408: step 124000/136300 (epoch 91/100), loss = 0.129067 (0.240 sec/batch), lr: 0.021632
2019-12-27 19:59:08.265413: step 124020/136300 (epoch 91/100), loss = 0.090014 (0.208 sec/batch), lr: 0.021632
Evaluating on dev set...
Precision (micro): 71.977%
   Recall (micro): 62.417%
       F1 (micro): 66.857%
epoch 91: train_loss = 0.207372, dev_loss = 0.463702, dev_f1 = 0.6686
model saved to ./saved_models/01/checkpoint_epoch_91.pt

2019-12-27 19:59:46.601756: step 124040/136300 (epoch 92/100), loss = 0.251215 (0.231 sec/batch), lr: 0.019469
2019-12-27 19:59:52.682721: step 124060/136300 (epoch 92/100), loss = 0.120068 (0.228 sec/batch), lr: 0.019469
2019-12-27 19:59:57.530634: step 124080/136300 (epoch 92/100), loss = 0.298037 (0.245 sec/batch), lr: 0.019469
2019-12-27 20:00:02.270231: step 124100/136300 (epoch 92/100), loss = 0.109476 (0.241 sec/batch), lr: 0.019469
2019-12-27 20:00:07.176512: step 124120/136300 (epoch 92/100), loss = 0.311043 (0.216 sec/batch), lr: 0.019469
2019-12-27 20:00:12.010290: step 124140/136300 (epoch 92/100), loss = 0.158035 (0.231 sec/batch), lr: 0.019469
2019-12-27 20:00:16.746294: step 124160/136300 (epoch 92/100), loss = 0.141697 (0.233 sec/batch), lr: 0.019469
2019-12-27 20:00:21.624469: step 124180/136300 (epoch 92/100), loss = 0.139103 (0.245 sec/batch), lr: 0.019469
2019-12-27 20:00:26.434526: step 124200/136300 (epoch 92/100), loss = 0.154037 (0.222 sec/batch), lr: 0.019469
2019-12-27 20:00:31.328000: step 124220/136300 (epoch 92/100), loss = 0.128836 (0.231 sec/batch), lr: 0.019469
2019-12-27 20:00:36.148419: step 124240/136300 (epoch 92/100), loss = 0.157096 (0.238 sec/batch), lr: 0.019469
2019-12-27 20:00:42.105209: step 124260/136300 (epoch 92/100), loss = 0.221214 (0.215 sec/batch), lr: 0.019469
2019-12-27 20:00:46.963508: step 124280/136300 (epoch 92/100), loss = 0.103293 (0.212 sec/batch), lr: 0.019469
2019-12-27 20:00:51.784867: step 124300/136300 (epoch 92/100), loss = 0.066694 (0.241 sec/batch), lr: 0.019469
2019-12-27 20:00:56.596225: step 124320/136300 (epoch 92/100), loss = 0.264256 (0.241 sec/batch), lr: 0.019469
2019-12-27 20:01:01.426362: step 124340/136300 (epoch 92/100), loss = 0.179990 (0.216 sec/batch), lr: 0.019469
2019-12-27 20:01:06.203861: step 124360/136300 (epoch 92/100), loss = 0.173780 (0.247 sec/batch), lr: 0.019469
2019-12-27 20:01:11.029478: step 124380/136300 (epoch 92/100), loss = 0.177329 (0.242 sec/batch), lr: 0.019469
2019-12-27 20:01:15.906504: step 124400/136300 (epoch 92/100), loss = 0.327021 (0.178 sec/batch), lr: 0.019469
2019-12-27 20:01:20.703868: step 124420/136300 (epoch 92/100), loss = 0.133961 (0.242 sec/batch), lr: 0.019469
2019-12-27 20:01:25.338216: step 124440/136300 (epoch 92/100), loss = 0.162262 (0.233 sec/batch), lr: 0.019469
2019-12-27 20:01:31.573534: step 124460/136300 (epoch 92/100), loss = 0.126439 (0.244 sec/batch), lr: 0.019469
2019-12-27 20:01:36.522003: step 124480/136300 (epoch 92/100), loss = 0.159242 (0.239 sec/batch), lr: 0.019469
2019-12-27 20:01:41.427914: step 124500/136300 (epoch 92/100), loss = 0.143112 (0.233 sec/batch), lr: 0.019469
2019-12-27 20:01:46.191018: step 124520/136300 (epoch 92/100), loss = 0.108400 (0.229 sec/batch), lr: 0.019469
2019-12-27 20:01:51.049707: step 124540/136300 (epoch 92/100), loss = 0.133178 (0.228 sec/batch), lr: 0.019469
2019-12-27 20:01:55.910102: step 124560/136300 (epoch 92/100), loss = 0.193311 (0.232 sec/batch), lr: 0.019469
2019-12-27 20:02:00.871823: step 124580/136300 (epoch 92/100), loss = 0.188091 (0.242 sec/batch), lr: 0.019469
2019-12-27 20:02:05.669984: step 124600/136300 (epoch 92/100), loss = 0.211711 (0.243 sec/batch), lr: 0.019469
2019-12-27 20:02:10.424647: step 124620/136300 (epoch 92/100), loss = 0.075583 (0.236 sec/batch), lr: 0.019469
2019-12-27 20:02:16.430147: step 124640/136300 (epoch 92/100), loss = 0.273030 (1.483 sec/batch), lr: 0.019469
2019-12-27 20:02:21.177045: step 124660/136300 (epoch 92/100), loss = 0.118763 (0.218 sec/batch), lr: 0.019469
2019-12-27 20:02:25.952464: step 124680/136300 (epoch 92/100), loss = 0.166715 (0.208 sec/batch), lr: 0.019469
2019-12-27 20:02:30.770628: step 124700/136300 (epoch 92/100), loss = 0.254909 (0.240 sec/batch), lr: 0.019469
2019-12-27 20:02:35.670546: step 124720/136300 (epoch 92/100), loss = 0.146601 (0.240 sec/batch), lr: 0.019469
2019-12-27 20:02:40.592633: step 124740/136300 (epoch 92/100), loss = 0.090010 (0.235 sec/batch), lr: 0.019469
2019-12-27 20:02:45.512846: step 124760/136300 (epoch 92/100), loss = 0.245265 (0.241 sec/batch), lr: 0.019469
2019-12-27 20:02:50.413683: step 124780/136300 (epoch 92/100), loss = 0.243280 (0.200 sec/batch), lr: 0.019469
2019-12-27 20:02:55.175496: step 124800/136300 (epoch 92/100), loss = 0.196128 (0.201 sec/batch), lr: 0.019469
2019-12-27 20:03:00.043485: step 124820/136300 (epoch 92/100), loss = 0.217288 (0.249 sec/batch), lr: 0.019469
2019-12-27 20:03:06.437913: step 124840/136300 (epoch 92/100), loss = 0.124694 (0.246 sec/batch), lr: 0.019469
2019-12-27 20:03:11.169000: step 124860/136300 (epoch 92/100), loss = 0.152419 (0.239 sec/batch), lr: 0.019469
2019-12-27 20:03:16.142794: step 124880/136300 (epoch 92/100), loss = 0.218358 (0.235 sec/batch), lr: 0.019469
2019-12-27 20:03:20.863554: step 124900/136300 (epoch 92/100), loss = 0.274717 (0.235 sec/batch), lr: 0.019469
2019-12-27 20:03:25.658179: step 124920/136300 (epoch 92/100), loss = 0.174816 (0.243 sec/batch), lr: 0.019469
2019-12-27 20:03:30.570024: step 124940/136300 (epoch 92/100), loss = 0.221401 (0.244 sec/batch), lr: 0.019469
2019-12-27 20:03:35.395782: step 124960/136300 (epoch 92/100), loss = 0.238941 (0.161 sec/batch), lr: 0.019469
2019-12-27 20:03:40.273706: step 124980/136300 (epoch 92/100), loss = 0.355100 (0.241 sec/batch), lr: 0.019469
2019-12-27 20:03:45.230463: step 125000/136300 (epoch 92/100), loss = 0.127116 (0.222 sec/batch), lr: 0.019469
2019-12-27 20:03:50.098237: step 125020/136300 (epoch 92/100), loss = 0.292603 (0.230 sec/batch), lr: 0.019469
2019-12-27 20:03:56.156212: step 125040/136300 (epoch 92/100), loss = 0.335235 (0.223 sec/batch), lr: 0.019469
2019-12-27 20:04:01.066856: step 125060/136300 (epoch 92/100), loss = 0.213865 (0.237 sec/batch), lr: 0.019469
2019-12-27 20:04:05.693076: step 125080/136300 (epoch 92/100), loss = 0.282092 (0.230 sec/batch), lr: 0.019469
2019-12-27 20:04:10.476850: step 125100/136300 (epoch 92/100), loss = 0.101537 (0.235 sec/batch), lr: 0.019469
2019-12-27 20:04:15.311199: step 125120/136300 (epoch 92/100), loss = 0.259694 (0.238 sec/batch), lr: 0.019469
2019-12-27 20:04:20.143556: step 125140/136300 (epoch 92/100), loss = 0.260998 (0.244 sec/batch), lr: 0.019469
2019-12-27 20:04:25.024103: step 125160/136300 (epoch 92/100), loss = 0.201941 (0.199 sec/batch), lr: 0.019469
2019-12-27 20:04:29.814573: step 125180/136300 (epoch 92/100), loss = 0.313038 (0.234 sec/batch), lr: 0.019469
2019-12-27 20:04:34.588820: step 125200/136300 (epoch 92/100), loss = 0.223416 (0.217 sec/batch), lr: 0.019469
2019-12-27 20:04:39.352058: step 125220/136300 (epoch 92/100), loss = 0.174837 (0.164 sec/batch), lr: 0.019469
2019-12-27 20:04:45.726175: step 125240/136300 (epoch 92/100), loss = 0.218627 (0.234 sec/batch), lr: 0.019469
2019-12-27 20:04:50.658142: step 125260/136300 (epoch 92/100), loss = 0.217017 (0.231 sec/batch), lr: 0.019469
2019-12-27 20:04:55.533144: step 125280/136300 (epoch 92/100), loss = 0.417678 (0.229 sec/batch), lr: 0.019469
2019-12-27 20:05:00.266606: step 125300/136300 (epoch 92/100), loss = 0.085744 (0.172 sec/batch), lr: 0.019469
2019-12-27 20:05:05.031726: step 125320/136300 (epoch 92/100), loss = 0.148061 (0.197 sec/batch), lr: 0.019469
2019-12-27 20:05:09.794959: step 125340/136300 (epoch 92/100), loss = 0.103410 (0.198 sec/batch), lr: 0.019469
2019-12-27 20:05:14.725864: step 125360/136300 (epoch 92/100), loss = 0.168459 (0.235 sec/batch), lr: 0.019469
2019-12-27 20:05:19.610057: step 125380/136300 (epoch 92/100), loss = 0.156366 (0.224 sec/batch), lr: 0.019469
Evaluating on dev set...
Precision (micro): 71.982%
   Recall (micro): 62.528%
       F1 (micro): 66.923%
epoch 92: train_loss = 0.207531, dev_loss = 0.467062, dev_f1 = 0.6692
model saved to ./saved_models/01/checkpoint_epoch_92.pt

2019-12-27 20:05:57.871899: step 125400/136300 (epoch 93/100), loss = 0.160373 (0.207 sec/batch), lr: 0.019469
2019-12-27 20:06:04.013351: step 125420/136300 (epoch 93/100), loss = 0.406543 (0.220 sec/batch), lr: 0.019469
2019-12-27 20:06:08.806592: step 125440/136300 (epoch 93/100), loss = 0.270248 (0.215 sec/batch), lr: 0.019469
2019-12-27 20:06:13.541618: step 125460/136300 (epoch 93/100), loss = 0.186453 (0.199 sec/batch), lr: 0.019469
2019-12-27 20:06:18.469741: step 125480/136300 (epoch 93/100), loss = 0.251253 (0.233 sec/batch), lr: 0.019469
2019-12-27 20:06:23.251209: step 125500/136300 (epoch 93/100), loss = 0.203072 (0.219 sec/batch), lr: 0.019469
2019-12-27 20:06:28.037218: step 125520/136300 (epoch 93/100), loss = 0.281771 (0.206 sec/batch), lr: 0.019469
2019-12-27 20:06:32.917319: step 125540/136300 (epoch 93/100), loss = 0.528503 (0.218 sec/batch), lr: 0.019469
2019-12-27 20:06:37.745302: step 125560/136300 (epoch 93/100), loss = 0.139712 (0.243 sec/batch), lr: 0.019469
2019-12-27 20:06:42.636918: step 125580/136300 (epoch 93/100), loss = 0.447549 (0.234 sec/batch), lr: 0.019469
2019-12-27 20:06:47.452652: step 125600/136300 (epoch 93/100), loss = 0.141682 (0.175 sec/batch), lr: 0.019469
2019-12-27 20:06:53.445441: step 125620/136300 (epoch 93/100), loss = 0.345858 (0.186 sec/batch), lr: 0.019469
2019-12-27 20:06:58.297512: step 125640/136300 (epoch 93/100), loss = 0.176598 (0.236 sec/batch), lr: 0.019469
2019-12-27 20:07:03.088916: step 125660/136300 (epoch 93/100), loss = 0.110913 (0.234 sec/batch), lr: 0.019469
2019-12-27 20:07:07.919676: step 125680/136300 (epoch 93/100), loss = 0.099409 (0.203 sec/batch), lr: 0.019469
2019-12-27 20:07:12.808141: step 125700/136300 (epoch 93/100), loss = 0.267693 (0.222 sec/batch), lr: 0.019469
2019-12-27 20:07:17.540732: step 125720/136300 (epoch 93/100), loss = 0.148295 (0.205 sec/batch), lr: 0.019469
2019-12-27 20:07:22.360723: step 125740/136300 (epoch 93/100), loss = 0.199183 (0.233 sec/batch), lr: 0.019469
2019-12-27 20:07:27.288078: step 125760/136300 (epoch 93/100), loss = 0.171328 (0.249 sec/batch), lr: 0.019469
2019-12-27 20:07:32.043191: step 125780/136300 (epoch 93/100), loss = 0.239153 (0.231 sec/batch), lr: 0.019469
2019-12-27 20:07:36.709393: step 125800/136300 (epoch 93/100), loss = 0.070878 (0.170 sec/batch), lr: 0.019469
2019-12-27 20:07:42.946225: step 125820/136300 (epoch 93/100), loss = 0.092276 (0.203 sec/batch), lr: 0.019469
2019-12-27 20:07:47.886282: step 125840/136300 (epoch 93/100), loss = 0.292106 (0.213 sec/batch), lr: 0.019469
2019-12-27 20:07:52.792518: step 125860/136300 (epoch 93/100), loss = 0.344577 (0.226 sec/batch), lr: 0.019469
2019-12-27 20:07:57.606571: step 125880/136300 (epoch 93/100), loss = 0.199578 (0.219 sec/batch), lr: 0.019469
2019-12-27 20:08:02.447360: step 125900/136300 (epoch 93/100), loss = 0.129539 (0.231 sec/batch), lr: 0.019469
2019-12-27 20:08:07.320721: step 125920/136300 (epoch 93/100), loss = 0.198794 (0.230 sec/batch), lr: 0.019469
2019-12-27 20:08:12.237688: step 125940/136300 (epoch 93/100), loss = 0.264799 (0.177 sec/batch), lr: 0.019469
2019-12-27 20:08:17.014256: step 125960/136300 (epoch 93/100), loss = 0.100414 (0.182 sec/batch), lr: 0.019469
2019-12-27 20:08:21.842338: step 125980/136300 (epoch 93/100), loss = 0.131069 (0.233 sec/batch), lr: 0.019469
2019-12-27 20:08:26.603084: step 126000/136300 (epoch 93/100), loss = 0.192620 (0.221 sec/batch), lr: 0.019469
2019-12-27 20:08:32.666762: step 126020/136300 (epoch 93/100), loss = 0.176577 (0.228 sec/batch), lr: 0.019469
2019-12-27 20:08:37.431291: step 126040/136300 (epoch 93/100), loss = 0.273292 (0.203 sec/batch), lr: 0.019469
2019-12-27 20:08:42.225727: step 126060/136300 (epoch 93/100), loss = 0.275022 (0.202 sec/batch), lr: 0.019469
2019-12-27 20:08:47.139639: step 126080/136300 (epoch 93/100), loss = 0.129569 (0.236 sec/batch), lr: 0.019469
2019-12-27 20:08:52.062499: step 126100/136300 (epoch 93/100), loss = 0.194554 (0.218 sec/batch), lr: 0.019469
2019-12-27 20:08:56.989320: step 126120/136300 (epoch 93/100), loss = 0.244250 (0.206 sec/batch), lr: 0.019469
2019-12-27 20:09:01.909975: step 126140/136300 (epoch 93/100), loss = 0.131144 (0.239 sec/batch), lr: 0.019469
2019-12-27 20:09:06.694497: step 126160/136300 (epoch 93/100), loss = 0.156558 (0.243 sec/batch), lr: 0.019469
2019-12-27 20:09:11.544322: step 126180/136300 (epoch 93/100), loss = 0.202228 (0.246 sec/batch), lr: 0.019469
2019-12-27 20:09:17.893423: step 126200/136300 (epoch 93/100), loss = 0.307287 (0.234 sec/batch), lr: 0.019469
2019-12-27 20:09:22.680716: step 126220/136300 (epoch 93/100), loss = 0.166779 (0.240 sec/batch), lr: 0.019469
2019-12-27 20:09:27.592709: step 126240/136300 (epoch 93/100), loss = 0.150016 (0.234 sec/batch), lr: 0.019469
2019-12-27 20:09:32.388777: step 126260/136300 (epoch 93/100), loss = 0.203725 (0.211 sec/batch), lr: 0.019469
2019-12-27 20:09:37.133326: step 126280/136300 (epoch 93/100), loss = 0.238903 (0.236 sec/batch), lr: 0.019469
2019-12-27 20:09:42.018241: step 126300/136300 (epoch 93/100), loss = 0.163514 (0.232 sec/batch), lr: 0.019469
2019-12-27 20:09:46.976441: step 126320/136300 (epoch 93/100), loss = 0.139701 (0.242 sec/batch), lr: 0.019469
2019-12-27 20:09:51.770482: step 126340/136300 (epoch 93/100), loss = 0.228669 (0.219 sec/batch), lr: 0.019469
2019-12-27 20:09:56.725630: step 126360/136300 (epoch 93/100), loss = 0.205150 (0.234 sec/batch), lr: 0.019469
2019-12-27 20:10:01.638639: step 126380/136300 (epoch 93/100), loss = 0.159694 (0.217 sec/batch), lr: 0.019469
2019-12-27 20:10:07.692181: step 126400/136300 (epoch 93/100), loss = 0.223494 (0.215 sec/batch), lr: 0.019469
2019-12-27 20:10:12.598754: step 126420/136300 (epoch 93/100), loss = 0.228152 (0.243 sec/batch), lr: 0.019469
2019-12-27 20:10:17.247840: step 126440/136300 (epoch 93/100), loss = 0.210891 (0.229 sec/batch), lr: 0.019469
2019-12-27 20:10:22.035944: step 126460/136300 (epoch 93/100), loss = 0.066103 (0.229 sec/batch), lr: 0.019469
2019-12-27 20:10:26.905476: step 126480/136300 (epoch 93/100), loss = 0.365071 (0.228 sec/batch), lr: 0.019469
2019-12-27 20:10:31.677729: step 126500/136300 (epoch 93/100), loss = 0.142298 (0.245 sec/batch), lr: 0.019469
2019-12-27 20:10:36.605587: step 126520/136300 (epoch 93/100), loss = 0.285932 (0.244 sec/batch), lr: 0.019469
2019-12-27 20:10:41.397731: step 126540/136300 (epoch 93/100), loss = 0.294911 (0.200 sec/batch), lr: 0.019469
2019-12-27 20:10:46.164819: step 126560/136300 (epoch 93/100), loss = 0.261864 (0.171 sec/batch), lr: 0.019469
2019-12-27 20:10:50.941158: step 126580/136300 (epoch 93/100), loss = 0.252620 (0.235 sec/batch), lr: 0.019469
2019-12-27 20:10:57.222544: step 126600/136300 (epoch 93/100), loss = 0.241423 (0.231 sec/batch), lr: 0.019469
2019-12-27 20:11:02.118273: step 126620/136300 (epoch 93/100), loss = 0.232185 (0.233 sec/batch), lr: 0.019469
2019-12-27 20:11:07.026464: step 126640/136300 (epoch 93/100), loss = 0.289128 (0.247 sec/batch), lr: 0.019469
2019-12-27 20:11:11.828379: step 126660/136300 (epoch 93/100), loss = 0.079169 (0.218 sec/batch), lr: 0.019469
2019-12-27 20:11:16.564608: step 126680/136300 (epoch 93/100), loss = 0.108913 (0.241 sec/batch), lr: 0.019469
2019-12-27 20:11:21.319911: step 126700/136300 (epoch 93/100), loss = 0.103009 (0.227 sec/batch), lr: 0.019469
2019-12-27 20:11:26.214626: step 126720/136300 (epoch 93/100), loss = 0.390941 (0.244 sec/batch), lr: 0.019469
2019-12-27 20:11:31.129634: step 126740/136300 (epoch 93/100), loss = 0.152785 (0.226 sec/batch), lr: 0.019469
Evaluating on dev set...
Precision (micro): 72.043%
   Recall (micro): 62.528%
       F1 (micro): 66.949%
epoch 93: train_loss = 0.206822, dev_loss = 0.463381, dev_f1 = 0.6695
model saved to ./saved_models/01/checkpoint_epoch_93.pt

2019-12-27 20:12:09.544116: step 126760/136300 (epoch 94/100), loss = 0.338253 (0.174 sec/batch), lr: 0.019469
2019-12-27 20:12:15.581241: step 126780/136300 (epoch 94/100), loss = 0.249480 (0.197 sec/batch), lr: 0.019469
2019-12-27 20:12:20.354357: step 126800/136300 (epoch 94/100), loss = 0.122322 (0.243 sec/batch), lr: 0.019469
2019-12-27 20:12:25.181899: step 126820/136300 (epoch 94/100), loss = 0.249231 (0.240 sec/batch), lr: 0.019469
2019-12-27 20:12:30.050147: step 126840/136300 (epoch 94/100), loss = 0.111218 (0.235 sec/batch), lr: 0.019469
2019-12-27 20:12:34.837208: step 126860/136300 (epoch 94/100), loss = 0.227645 (0.203 sec/batch), lr: 0.019469
2019-12-27 20:12:39.591973: step 126880/136300 (epoch 94/100), loss = 0.303698 (0.177 sec/batch), lr: 0.019469
2019-12-27 20:12:44.458110: step 126900/136300 (epoch 94/100), loss = 0.183646 (0.239 sec/batch), lr: 0.019469
2019-12-27 20:12:49.286431: step 126920/136300 (epoch 94/100), loss = 0.092270 (0.245 sec/batch), lr: 0.019469
2019-12-27 20:12:54.187142: step 126940/136300 (epoch 94/100), loss = 0.177979 (0.240 sec/batch), lr: 0.019469
2019-12-27 20:12:59.024669: step 126960/136300 (epoch 94/100), loss = 0.204477 (0.205 sec/batch), lr: 0.019469
2019-12-27 20:13:05.118236: step 126980/136300 (epoch 94/100), loss = 0.190749 (0.203 sec/batch), lr: 0.019469
2019-12-27 20:13:09.882498: step 127000/136300 (epoch 94/100), loss = 0.220993 (0.222 sec/batch), lr: 0.019469
2019-12-27 20:13:14.718581: step 127020/136300 (epoch 94/100), loss = 0.432767 (0.246 sec/batch), lr: 0.019469
2019-12-27 20:13:19.532379: step 127040/136300 (epoch 94/100), loss = 0.153810 (0.222 sec/batch), lr: 0.019469
2019-12-27 20:13:24.373642: step 127060/136300 (epoch 94/100), loss = 0.248794 (0.217 sec/batch), lr: 0.019469
2019-12-27 20:13:29.159392: step 127080/136300 (epoch 94/100), loss = 0.130238 (0.239 sec/batch), lr: 0.019469
2019-12-27 20:13:33.918118: step 127100/136300 (epoch 94/100), loss = 0.135866 (0.205 sec/batch), lr: 0.019469
2019-12-27 20:13:38.814870: step 127120/136300 (epoch 94/100), loss = 0.390792 (0.219 sec/batch), lr: 0.019469
2019-12-27 20:13:43.579982: step 127140/136300 (epoch 94/100), loss = 0.143684 (0.222 sec/batch), lr: 0.019469
2019-12-27 20:13:48.302697: step 127160/136300 (epoch 94/100), loss = 0.193619 (0.177 sec/batch), lr: 0.019469
2019-12-27 20:13:54.411472: step 127180/136300 (epoch 94/100), loss = 0.190491 (0.177 sec/batch), lr: 0.019469
2019-12-27 20:13:59.209373: step 127200/136300 (epoch 94/100), loss = 0.158329 (0.205 sec/batch), lr: 0.019469
2019-12-27 20:14:04.123102: step 127220/136300 (epoch 94/100), loss = 0.227976 (0.238 sec/batch), lr: 0.019469
2019-12-27 20:14:08.938430: step 127240/136300 (epoch 94/100), loss = 0.168624 (0.207 sec/batch), lr: 0.019469
2019-12-27 20:14:13.754457: step 127260/136300 (epoch 94/100), loss = 0.183744 (0.233 sec/batch), lr: 0.019469
2019-12-27 20:14:18.602762: step 127280/136300 (epoch 94/100), loss = 0.107564 (0.173 sec/batch), lr: 0.019469
2019-12-27 20:14:23.595111: step 127300/136300 (epoch 94/100), loss = 0.273495 (0.239 sec/batch), lr: 0.019469
2019-12-27 20:14:28.353195: step 127320/136300 (epoch 94/100), loss = 0.238635 (0.202 sec/batch), lr: 0.019469
2019-12-27 20:14:33.198986: step 127340/136300 (epoch 94/100), loss = 0.252328 (0.222 sec/batch), lr: 0.019469
2019-12-27 20:14:37.908897: step 127360/136300 (epoch 94/100), loss = 0.087609 (0.223 sec/batch), lr: 0.019469
2019-12-27 20:14:44.017818: step 127380/136300 (epoch 94/100), loss = 0.286870 (0.228 sec/batch), lr: 0.019469
2019-12-27 20:14:48.817900: step 127400/136300 (epoch 94/100), loss = 0.231105 (0.233 sec/batch), lr: 0.019469
2019-12-27 20:14:53.595353: step 127420/136300 (epoch 94/100), loss = 0.143452 (0.239 sec/batch), lr: 0.019469
2019-12-27 20:14:58.473794: step 127440/136300 (epoch 94/100), loss = 0.305637 (0.242 sec/batch), lr: 0.019469
2019-12-27 20:15:03.396242: step 127460/136300 (epoch 94/100), loss = 0.183555 (0.228 sec/batch), lr: 0.019469
2019-12-27 20:15:08.291704: step 127480/136300 (epoch 94/100), loss = 0.070475 (0.229 sec/batch), lr: 0.019469
2019-12-27 20:15:13.173778: step 127500/136300 (epoch 94/100), loss = 0.191589 (0.242 sec/batch), lr: 0.019469
2019-12-27 20:15:17.970276: step 127520/136300 (epoch 94/100), loss = 0.177161 (0.231 sec/batch), lr: 0.019469
2019-12-27 20:15:22.769687: step 127540/136300 (epoch 94/100), loss = 0.357432 (0.231 sec/batch), lr: 0.019469
2019-12-27 20:15:29.061488: step 127560/136300 (epoch 94/100), loss = 0.299356 (0.238 sec/batch), lr: 0.019469
2019-12-27 20:15:33.849642: step 127580/136300 (epoch 94/100), loss = 0.245157 (0.227 sec/batch), lr: 0.019469
2019-12-27 20:15:38.718776: step 127600/136300 (epoch 94/100), loss = 0.276717 (0.239 sec/batch), lr: 0.019469
2019-12-27 20:15:43.597327: step 127620/136300 (epoch 94/100), loss = 0.331532 (0.228 sec/batch), lr: 0.019469
2019-12-27 20:15:48.313930: step 127640/136300 (epoch 94/100), loss = 0.233381 (0.207 sec/batch), lr: 0.019469
2019-12-27 20:15:53.162445: step 127660/136300 (epoch 94/100), loss = 0.194840 (0.229 sec/batch), lr: 0.019469
2019-12-27 20:15:58.103339: step 127680/136300 (epoch 94/100), loss = 0.181772 (0.246 sec/batch), lr: 0.019469
2019-12-27 20:16:02.885440: step 127700/136300 (epoch 94/100), loss = 0.081402 (0.183 sec/batch), lr: 0.019469
2019-12-27 20:16:07.895836: step 127720/136300 (epoch 94/100), loss = 0.170738 (0.247 sec/batch), lr: 0.019469
2019-12-27 20:16:12.778800: step 127740/136300 (epoch 94/100), loss = 0.126412 (0.232 sec/batch), lr: 0.019469
2019-12-27 20:16:17.605211: step 127760/136300 (epoch 94/100), loss = 0.138378 (0.235 sec/batch), lr: 0.019469
2019-12-27 20:16:23.820397: step 127780/136300 (epoch 94/100), loss = 0.158969 (0.232 sec/batch), lr: 0.019469
2019-12-27 20:16:28.531688: step 127800/136300 (epoch 94/100), loss = 0.278066 (0.209 sec/batch), lr: 0.019469
2019-12-27 20:16:33.253426: step 127820/136300 (epoch 94/100), loss = 0.166286 (0.230 sec/batch), lr: 0.019469
2019-12-27 20:16:38.116739: step 127840/136300 (epoch 94/100), loss = 0.178636 (0.235 sec/batch), lr: 0.019469
2019-12-27 20:16:42.889952: step 127860/136300 (epoch 94/100), loss = 0.312046 (0.226 sec/batch), lr: 0.019469
2019-12-27 20:16:47.798062: step 127880/136300 (epoch 94/100), loss = 0.181142 (0.222 sec/batch), lr: 0.019469
2019-12-27 20:16:52.666672: step 127900/136300 (epoch 94/100), loss = 0.119932 (0.240 sec/batch), lr: 0.019469
2019-12-27 20:16:57.409401: step 127920/136300 (epoch 94/100), loss = 0.211223 (0.237 sec/batch), lr: 0.019469
2019-12-27 20:17:02.161736: step 127940/136300 (epoch 94/100), loss = 0.176492 (0.245 sec/batch), lr: 0.019469
2019-12-27 20:17:08.266934: step 127960/136300 (epoch 94/100), loss = 0.095210 (0.233 sec/batch), lr: 0.019469
2019-12-27 20:17:13.162691: step 127980/136300 (epoch 94/100), loss = 0.245201 (0.241 sec/batch), lr: 0.019469
2019-12-27 20:17:18.077559: step 128000/136300 (epoch 94/100), loss = 0.241608 (0.238 sec/batch), lr: 0.019469
2019-12-27 20:17:22.885166: step 128020/136300 (epoch 94/100), loss = 0.456375 (0.186 sec/batch), lr: 0.019469
2019-12-27 20:17:27.611852: step 128040/136300 (epoch 94/100), loss = 0.156676 (0.220 sec/batch), lr: 0.019469
2019-12-27 20:17:32.405066: step 128060/136300 (epoch 94/100), loss = 0.193310 (0.216 sec/batch), lr: 0.019469
2019-12-27 20:17:37.222892: step 128080/136300 (epoch 94/100), loss = 0.193623 (0.240 sec/batch), lr: 0.019469
2019-12-27 20:17:42.143554: step 128100/136300 (epoch 94/100), loss = 0.179763 (0.242 sec/batch), lr: 0.019469
2019-12-27 20:17:47.095095: step 128120/136300 (epoch 94/100), loss = 0.193808 (0.218 sec/batch), lr: 0.019469
Evaluating on dev set...
Precision (micro): 72.877%
   Recall (micro): 61.884%
       F1 (micro): 66.932%
epoch 94: train_loss = 0.208061, dev_loss = 0.464515, dev_f1 = 0.6693
model saved to ./saved_models/01/checkpoint_epoch_94.pt

2019-12-27 20:18:26.629151: step 128140/136300 (epoch 95/100), loss = 0.152185 (1.476 sec/batch), lr: 0.017522
2019-12-27 20:18:31.355829: step 128160/136300 (epoch 95/100), loss = 0.242302 (0.221 sec/batch), lr: 0.017522
2019-12-27 20:18:36.205801: step 128180/136300 (epoch 95/100), loss = 0.291954 (0.235 sec/batch), lr: 0.017522
2019-12-27 20:18:41.055187: step 128200/136300 (epoch 95/100), loss = 0.127539 (0.244 sec/batch), lr: 0.017522
2019-12-27 20:18:45.864379: step 128220/136300 (epoch 95/100), loss = 0.200619 (0.220 sec/batch), lr: 0.017522
2019-12-27 20:18:50.655042: step 128240/136300 (epoch 95/100), loss = 0.363147 (0.238 sec/batch), lr: 0.017522
2019-12-27 20:18:55.462536: step 128260/136300 (epoch 95/100), loss = 0.282063 (0.231 sec/batch), lr: 0.017522
2019-12-27 20:19:00.278294: step 128280/136300 (epoch 95/100), loss = 0.176074 (0.218 sec/batch), lr: 0.017522
2019-12-27 20:19:05.182613: step 128300/136300 (epoch 95/100), loss = 0.110679 (0.240 sec/batch), lr: 0.017522
2019-12-27 20:19:10.069185: step 128320/136300 (epoch 95/100), loss = 0.214423 (0.242 sec/batch), lr: 0.017522
2019-12-27 20:19:16.193954: step 128340/136300 (epoch 95/100), loss = 0.110254 (0.224 sec/batch), lr: 0.017522
2019-12-27 20:19:20.929121: step 128360/136300 (epoch 95/100), loss = 0.089397 (0.228 sec/batch), lr: 0.017522
2019-12-27 20:19:25.739980: step 128380/136300 (epoch 95/100), loss = 0.267537 (0.243 sec/batch), lr: 0.017522
2019-12-27 20:19:30.614195: step 128400/136300 (epoch 95/100), loss = 0.161696 (0.241 sec/batch), lr: 0.017522
2019-12-27 20:19:35.423152: step 128420/136300 (epoch 95/100), loss = 0.145677 (0.240 sec/batch), lr: 0.017522
2019-12-27 20:19:40.169211: step 128440/136300 (epoch 95/100), loss = 0.365269 (0.237 sec/batch), lr: 0.017522
2019-12-27 20:19:45.033824: step 128460/136300 (epoch 95/100), loss = 0.346710 (0.239 sec/batch), lr: 0.017522
2019-12-27 20:19:49.860517: step 128480/136300 (epoch 95/100), loss = 0.184884 (0.215 sec/batch), lr: 0.017522
2019-12-27 20:19:54.603195: step 128500/136300 (epoch 95/100), loss = 0.230009 (0.222 sec/batch), lr: 0.017522
2019-12-27 20:19:59.376069: step 128520/136300 (epoch 95/100), loss = 0.331863 (0.206 sec/batch), lr: 0.017522
2019-12-27 20:20:05.502481: step 128540/136300 (epoch 95/100), loss = 0.119304 (0.212 sec/batch), lr: 0.017522
2019-12-27 20:20:10.356676: step 128560/136300 (epoch 95/100), loss = 0.294072 (0.233 sec/batch), lr: 0.017522
2019-12-27 20:20:15.251121: step 128580/136300 (epoch 95/100), loss = 0.125995 (0.243 sec/batch), lr: 0.017522
2019-12-27 20:20:20.074695: step 128600/136300 (epoch 95/100), loss = 0.103609 (0.234 sec/batch), lr: 0.017522
2019-12-27 20:20:24.891809: step 128620/136300 (epoch 95/100), loss = 0.171289 (0.236 sec/batch), lr: 0.017522
2019-12-27 20:20:29.780764: step 128640/136300 (epoch 95/100), loss = 0.401115 (0.217 sec/batch), lr: 0.017522
2019-12-27 20:20:34.730241: step 128660/136300 (epoch 95/100), loss = 0.168853 (0.243 sec/batch), lr: 0.017522
2019-12-27 20:20:39.528687: step 128680/136300 (epoch 95/100), loss = 0.180488 (0.237 sec/batch), lr: 0.017522
2019-12-27 20:20:44.369954: step 128700/136300 (epoch 95/100), loss = 0.097884 (0.243 sec/batch), lr: 0.017522
2019-12-27 20:20:49.104407: step 128720/136300 (epoch 95/100), loss = 0.087917 (0.240 sec/batch), lr: 0.017522
2019-12-27 20:20:55.190712: step 128740/136300 (epoch 95/100), loss = 0.195140 (0.216 sec/batch), lr: 0.017522
2019-12-27 20:21:00.042311: step 128760/136300 (epoch 95/100), loss = 0.148774 (0.233 sec/batch), lr: 0.017522
2019-12-27 20:21:04.792641: step 128780/136300 (epoch 95/100), loss = 0.070761 (0.239 sec/batch), lr: 0.017522
2019-12-27 20:21:09.631729: step 128800/136300 (epoch 95/100), loss = 0.108615 (0.245 sec/batch), lr: 0.017522
2019-12-27 20:21:14.570597: step 128820/136300 (epoch 95/100), loss = 0.274001 (0.202 sec/batch), lr: 0.017522
2019-12-27 20:21:19.471872: step 128840/136300 (epoch 95/100), loss = 0.178781 (0.227 sec/batch), lr: 0.017522
2019-12-27 20:21:24.387297: step 128860/136300 (epoch 95/100), loss = 0.272975 (0.235 sec/batch), lr: 0.017522
2019-12-27 20:21:29.164651: step 128880/136300 (epoch 95/100), loss = 0.293257 (0.222 sec/batch), lr: 0.017522
2019-12-27 20:21:34.001538: step 128900/136300 (epoch 95/100), loss = 0.281769 (0.234 sec/batch), lr: 0.017522
2019-12-27 20:21:40.240557: step 128920/136300 (epoch 95/100), loss = 0.281567 (1.537 sec/batch), lr: 0.017522
2019-12-27 20:21:45.029041: step 128940/136300 (epoch 95/100), loss = 0.234356 (0.175 sec/batch), lr: 0.017522
2019-12-27 20:21:49.889622: step 128960/136300 (epoch 95/100), loss = 0.192046 (0.247 sec/batch), lr: 0.017522
2019-12-27 20:21:54.811626: step 128980/136300 (epoch 95/100), loss = 0.246423 (0.216 sec/batch), lr: 0.017522
2019-12-27 20:21:59.514313: step 129000/136300 (epoch 95/100), loss = 0.290420 (0.233 sec/batch), lr: 0.017522
2019-12-27 20:22:04.403572: step 129020/136300 (epoch 95/100), loss = 0.256908 (0.236 sec/batch), lr: 0.017522
2019-12-27 20:22:09.314560: step 129040/136300 (epoch 95/100), loss = 0.143548 (0.243 sec/batch), lr: 0.017522
2019-12-27 20:22:14.127133: step 129060/136300 (epoch 95/100), loss = 0.139336 (0.237 sec/batch), lr: 0.017522
2019-12-27 20:22:19.110201: step 129080/136300 (epoch 95/100), loss = 0.208475 (0.247 sec/batch), lr: 0.017522
2019-12-27 20:22:23.991607: step 129100/136300 (epoch 95/100), loss = 0.275373 (0.235 sec/batch), lr: 0.017522
2019-12-27 20:22:28.803432: step 129120/136300 (epoch 95/100), loss = 0.237579 (0.217 sec/batch), lr: 0.017522
2019-12-27 20:22:35.047342: step 129140/136300 (epoch 95/100), loss = 0.217112 (0.246 sec/batch), lr: 0.017522
2019-12-27 20:22:39.846299: step 129160/136300 (epoch 95/100), loss = 0.160075 (0.203 sec/batch), lr: 0.017522
2019-12-27 20:22:44.551424: step 129180/136300 (epoch 95/100), loss = 0.298487 (0.231 sec/batch), lr: 0.017522
2019-12-27 20:22:49.329933: step 129200/136300 (epoch 95/100), loss = 0.215236 (0.214 sec/batch), lr: 0.017522
2019-12-27 20:22:54.107054: step 129220/136300 (epoch 95/100), loss = 0.188306 (0.182 sec/batch), lr: 0.017522
2019-12-27 20:22:59.041932: step 129240/136300 (epoch 95/100), loss = 0.156918 (0.244 sec/batch), lr: 0.017522
2019-12-27 20:23:03.885903: step 129260/136300 (epoch 95/100), loss = 0.236818 (0.225 sec/batch), lr: 0.017522
2019-12-27 20:23:08.650072: step 129280/136300 (epoch 95/100), loss = 0.186218 (0.219 sec/batch), lr: 0.017522
2019-12-27 20:23:13.377013: step 129300/136300 (epoch 95/100), loss = 0.219625 (0.241 sec/batch), lr: 0.017522
2019-12-27 20:23:18.214454: step 129320/136300 (epoch 95/100), loss = 0.243583 (0.228 sec/batch), lr: 0.017522
2019-12-27 20:23:24.401611: step 129340/136300 (epoch 95/100), loss = 0.189959 (0.249 sec/batch), lr: 0.017522
2019-12-27 20:23:29.309325: step 129360/136300 (epoch 95/100), loss = 0.089543 (0.237 sec/batch), lr: 0.017522
2019-12-27 20:23:34.184149: step 129380/136300 (epoch 95/100), loss = 0.266679 (0.237 sec/batch), lr: 0.017522
2019-12-27 20:23:38.900017: step 129400/136300 (epoch 95/100), loss = 0.146018 (0.228 sec/batch), lr: 0.017522
2019-12-27 20:23:43.734646: step 129420/136300 (epoch 95/100), loss = 0.164852 (0.212 sec/batch), lr: 0.017522
2019-12-27 20:23:48.510158: step 129440/136300 (epoch 95/100), loss = 0.270468 (0.233 sec/batch), lr: 0.017522
2019-12-27 20:23:53.449085: step 129460/136300 (epoch 95/100), loss = 0.161551 (0.243 sec/batch), lr: 0.017522
2019-12-27 20:23:58.373743: step 129480/136300 (epoch 95/100), loss = 0.234101 (0.241 sec/batch), lr: 0.017522
Evaluating on dev set...
Precision (micro): 72.897%
   Recall (micro): 61.700%
       F1 (micro): 66.833%
epoch 95: train_loss = 0.208200, dev_loss = 0.465187, dev_f1 = 0.6683
model saved to ./saved_models/01/checkpoint_epoch_95.pt

2019-12-27 20:24:36.709146: step 129500/136300 (epoch 96/100), loss = 0.170581 (0.234 sec/batch), lr: 0.015770
2019-12-27 20:24:42.670890: step 129520/136300 (epoch 96/100), loss = 0.163366 (0.202 sec/batch), lr: 0.015770
2019-12-27 20:24:47.524479: step 129540/136300 (epoch 96/100), loss = 0.244730 (0.238 sec/batch), lr: 0.015770
2019-12-27 20:24:52.317428: step 129560/136300 (epoch 96/100), loss = 0.267435 (0.251 sec/batch), lr: 0.015770
2019-12-27 20:24:57.217909: step 129580/136300 (epoch 96/100), loss = 0.152457 (0.216 sec/batch), lr: 0.015770
2019-12-27 20:25:01.962576: step 129600/136300 (epoch 96/100), loss = 0.196912 (0.206 sec/batch), lr: 0.015770
2019-12-27 20:25:06.766348: step 129620/136300 (epoch 96/100), loss = 0.213164 (0.215 sec/batch), lr: 0.015770
2019-12-27 20:25:11.600269: step 129640/136300 (epoch 96/100), loss = 0.219230 (0.245 sec/batch), lr: 0.015770
2019-12-27 20:25:16.470910: step 129660/136300 (epoch 96/100), loss = 0.164306 (0.229 sec/batch), lr: 0.015770
2019-12-27 20:25:21.346297: step 129680/136300 (epoch 96/100), loss = 0.174634 (0.239 sec/batch), lr: 0.015770
2019-12-27 20:25:27.546331: step 129700/136300 (epoch 96/100), loss = 0.152686 (0.182 sec/batch), lr: 0.015770
2019-12-27 20:25:32.281430: step 129720/136300 (epoch 96/100), loss = 0.386236 (0.227 sec/batch), lr: 0.015770
2019-12-27 20:25:37.110032: step 129740/136300 (epoch 96/100), loss = 0.132305 (0.240 sec/batch), lr: 0.015770
2019-12-27 20:25:41.981261: step 129760/136300 (epoch 96/100), loss = 0.177236 (0.215 sec/batch), lr: 0.015770
2019-12-27 20:25:46.815117: step 129780/136300 (epoch 96/100), loss = 0.401967 (0.200 sec/batch), lr: 0.015770
2019-12-27 20:25:51.543628: step 129800/136300 (epoch 96/100), loss = 0.216600 (0.205 sec/batch), lr: 0.015770
2019-12-27 20:25:56.394270: step 129820/136300 (epoch 96/100), loss = 0.170852 (0.241 sec/batch), lr: 0.015770
2019-12-27 20:26:01.220764: step 129840/136300 (epoch 96/100), loss = 0.144331 (0.246 sec/batch), lr: 0.015770
2019-12-27 20:26:05.958217: step 129860/136300 (epoch 96/100), loss = 0.157859 (0.209 sec/batch), lr: 0.015770
2019-12-27 20:26:10.741824: step 129880/136300 (epoch 96/100), loss = 0.152370 (0.213 sec/batch), lr: 0.015770
2019-12-27 20:26:16.798640: step 129900/136300 (epoch 96/100), loss = 0.119403 (0.222 sec/batch), lr: 0.015770
2019-12-27 20:26:21.620970: step 129920/136300 (epoch 96/100), loss = 0.182236 (0.218 sec/batch), lr: 0.015770
2019-12-27 20:26:26.549311: step 129940/136300 (epoch 96/100), loss = 0.073389 (0.235 sec/batch), lr: 0.015770
2019-12-27 20:26:31.365380: step 129960/136300 (epoch 96/100), loss = 0.220522 (0.225 sec/batch), lr: 0.015770
2019-12-27 20:26:36.226439: step 129980/136300 (epoch 96/100), loss = 0.236863 (0.231 sec/batch), lr: 0.015770
2019-12-27 20:26:41.142732: step 130000/136300 (epoch 96/100), loss = 0.184560 (0.235 sec/batch), lr: 0.015770
2019-12-27 20:26:46.035140: step 130020/136300 (epoch 96/100), loss = 0.192997 (0.240 sec/batch), lr: 0.015770
2019-12-27 20:26:50.861832: step 130040/136300 (epoch 96/100), loss = 0.182312 (0.244 sec/batch), lr: 0.015770
2019-12-27 20:26:55.719837: step 130060/136300 (epoch 96/100), loss = 0.177236 (0.228 sec/batch), lr: 0.015770
2019-12-27 20:27:00.471499: step 130080/136300 (epoch 96/100), loss = 0.331192 (0.175 sec/batch), lr: 0.015770
2019-12-27 20:27:06.642505: step 130100/136300 (epoch 96/100), loss = 0.196739 (0.228 sec/batch), lr: 0.015770
2019-12-27 20:27:11.465263: step 130120/136300 (epoch 96/100), loss = 0.154031 (0.242 sec/batch), lr: 0.015770
2019-12-27 20:27:16.214120: step 130140/136300 (epoch 96/100), loss = 0.085857 (0.232 sec/batch), lr: 0.015770
2019-12-27 20:27:21.041048: step 130160/136300 (epoch 96/100), loss = 0.196654 (0.232 sec/batch), lr: 0.015770
2019-12-27 20:27:25.985577: step 130180/136300 (epoch 96/100), loss = 0.163589 (0.252 sec/batch), lr: 0.015770
2019-12-27 20:27:30.850229: step 130200/136300 (epoch 96/100), loss = 0.096094 (0.241 sec/batch), lr: 0.015770
2019-12-27 20:27:35.785738: step 130220/136300 (epoch 96/100), loss = 0.088421 (0.241 sec/batch), lr: 0.015770
2019-12-27 20:27:40.598449: step 130240/136300 (epoch 96/100), loss = 0.080721 (0.205 sec/batch), lr: 0.015770
2019-12-27 20:27:45.390392: step 130260/136300 (epoch 96/100), loss = 0.329367 (0.247 sec/batch), lr: 0.015770
2019-12-27 20:27:50.296949: step 130280/136300 (epoch 96/100), loss = 0.170836 (0.236 sec/batch), lr: 0.015770
2019-12-27 20:27:56.420378: step 130300/136300 (epoch 96/100), loss = 0.213436 (0.229 sec/batch), lr: 0.015770
2019-12-27 20:28:01.089961: step 130320/136300 (epoch 96/100), loss = 0.209570 (0.239 sec/batch), lr: 0.015770
2019-12-27 20:28:05.927392: step 130340/136300 (epoch 96/100), loss = 0.180780 (0.234 sec/batch), lr: 0.015770
2019-12-27 20:28:10.630242: step 130360/136300 (epoch 96/100), loss = 0.287457 (0.249 sec/batch), lr: 0.015770
2019-12-27 20:28:15.515575: step 130380/136300 (epoch 96/100), loss = 0.205637 (0.242 sec/batch), lr: 0.015770
2019-12-27 20:28:20.468659: step 130400/136300 (epoch 96/100), loss = 0.313974 (0.249 sec/batch), lr: 0.015770
2019-12-27 20:28:25.234275: step 130420/136300 (epoch 96/100), loss = 0.114905 (0.218 sec/batch), lr: 0.015770
2019-12-27 20:28:30.208638: step 130440/136300 (epoch 96/100), loss = 0.210540 (0.230 sec/batch), lr: 0.015770
2019-12-27 20:28:35.096641: step 130460/136300 (epoch 96/100), loss = 0.158314 (0.235 sec/batch), lr: 0.015770
2019-12-27 20:28:39.933586: step 130480/136300 (epoch 96/100), loss = 0.134831 (0.207 sec/batch), lr: 0.015770
2019-12-27 20:28:46.203019: step 130500/136300 (epoch 96/100), loss = 0.192202 (0.237 sec/batch), lr: 0.015770
2019-12-27 20:28:51.070280: step 130520/136300 (epoch 96/100), loss = 0.236577 (0.241 sec/batch), lr: 0.015770
2019-12-27 20:28:55.720665: step 130540/136300 (epoch 96/100), loss = 0.189076 (0.230 sec/batch), lr: 0.015770
2019-12-27 20:29:00.476736: step 130560/136300 (epoch 96/100), loss = 0.250061 (0.223 sec/batch), lr: 0.015770
2019-12-27 20:29:05.313473: step 130580/136300 (epoch 96/100), loss = 0.182735 (0.236 sec/batch), lr: 0.015770
2019-12-27 20:29:10.135203: step 130600/136300 (epoch 96/100), loss = 0.183955 (0.216 sec/batch), lr: 0.015770
2019-12-27 20:29:15.038466: step 130620/136300 (epoch 96/100), loss = 0.109395 (0.225 sec/batch), lr: 0.015770
2019-12-27 20:29:19.797909: step 130640/136300 (epoch 96/100), loss = 0.327680 (0.227 sec/batch), lr: 0.015770
2019-12-27 20:29:24.484275: step 130660/136300 (epoch 96/100), loss = 0.145972 (0.228 sec/batch), lr: 0.015770
2019-12-27 20:29:29.341005: step 130680/136300 (epoch 96/100), loss = 0.257612 (0.249 sec/batch), lr: 0.015770
2019-12-27 20:29:35.488869: step 130700/136300 (epoch 96/100), loss = 0.115859 (0.227 sec/batch), lr: 0.015770
2019-12-27 20:29:40.424639: step 130720/136300 (epoch 96/100), loss = 0.182349 (0.229 sec/batch), lr: 0.015770
2019-12-27 20:29:45.310526: step 130740/136300 (epoch 96/100), loss = 0.351202 (0.215 sec/batch), lr: 0.015770
2019-12-27 20:29:49.955131: step 130760/136300 (epoch 96/100), loss = 0.205154 (0.178 sec/batch), lr: 0.015770
2019-12-27 20:29:54.811194: step 130780/136300 (epoch 96/100), loss = 0.398187 (0.236 sec/batch), lr: 0.015770
2019-12-27 20:29:59.637449: step 130800/136300 (epoch 96/100), loss = 0.351607 (0.246 sec/batch), lr: 0.015770
2019-12-27 20:30:04.545368: step 130820/136300 (epoch 96/100), loss = 0.220018 (0.240 sec/batch), lr: 0.015770
2019-12-27 20:30:09.412007: step 130840/136300 (epoch 96/100), loss = 0.289316 (0.238 sec/batch), lr: 0.015770
Evaluating on dev set...
Precision (micro): 70.969%
   Recall (micro): 63.723%
       F1 (micro): 67.151%
epoch 96: train_loss = 0.204867, dev_loss = 0.462794, dev_f1 = 0.6715
model saved to ./saved_models/01/checkpoint_epoch_96.pt

2019-12-27 20:30:47.758618: step 130860/136300 (epoch 97/100), loss = 0.213937 (0.233 sec/batch), lr: 0.015770
2019-12-27 20:30:53.805350: step 130880/136300 (epoch 97/100), loss = 0.125854 (0.218 sec/batch), lr: 0.015770
2019-12-27 20:30:58.646942: step 130900/136300 (epoch 97/100), loss = 0.250805 (0.218 sec/batch), lr: 0.015770
2019-12-27 20:31:03.401067: step 130920/136300 (epoch 97/100), loss = 0.284967 (0.247 sec/batch), lr: 0.015770
2019-12-27 20:31:08.322678: step 130940/136300 (epoch 97/100), loss = 0.206500 (0.204 sec/batch), lr: 0.015770
2019-12-27 20:31:13.090643: step 130960/136300 (epoch 97/100), loss = 0.151244 (0.222 sec/batch), lr: 0.015770
2019-12-27 20:31:17.908950: step 130980/136300 (epoch 97/100), loss = 0.185338 (0.232 sec/batch), lr: 0.015770
2019-12-27 20:31:22.744055: step 131000/136300 (epoch 97/100), loss = 0.164605 (0.207 sec/batch), lr: 0.015770
2019-12-27 20:31:27.594616: step 131020/136300 (epoch 97/100), loss = 0.324094 (0.214 sec/batch), lr: 0.015770
2019-12-27 20:31:32.522610: step 131040/136300 (epoch 97/100), loss = 0.141495 (0.222 sec/batch), lr: 0.015770
2019-12-27 20:31:37.353565: step 131060/136300 (epoch 97/100), loss = 0.096967 (0.233 sec/batch), lr: 0.015770
2019-12-27 20:31:43.569970: step 131080/136300 (epoch 97/100), loss = 0.225607 (0.241 sec/batch), lr: 0.015770
2019-12-27 20:31:48.312913: step 131100/136300 (epoch 97/100), loss = 0.257001 (0.204 sec/batch), lr: 0.015770
2019-12-27 20:31:53.230022: step 131120/136300 (epoch 97/100), loss = 0.175799 (0.237 sec/batch), lr: 0.015770
2019-12-27 20:31:58.046429: step 131140/136300 (epoch 97/100), loss = 0.173545 (0.218 sec/batch), lr: 0.015770
2019-12-27 20:32:02.805248: step 131160/136300 (epoch 97/100), loss = 0.279057 (0.231 sec/batch), lr: 0.015770
2019-12-27 20:32:07.590173: step 131180/136300 (epoch 97/100), loss = 0.188065 (0.246 sec/batch), lr: 0.015770
2019-12-27 20:32:12.429941: step 131200/136300 (epoch 97/100), loss = 0.111101 (0.233 sec/batch), lr: 0.015770
2019-12-27 20:32:17.245636: step 131220/136300 (epoch 97/100), loss = 0.303806 (0.198 sec/batch), lr: 0.015770
2019-12-27 20:32:21.981208: step 131240/136300 (epoch 97/100), loss = 0.126490 (0.225 sec/batch), lr: 0.015770
2019-12-27 20:32:27.946688: step 131260/136300 (epoch 97/100), loss = 0.087840 (0.226 sec/batch), lr: 0.015770
2019-12-27 20:32:32.787738: step 131280/136300 (epoch 97/100), loss = 0.237638 (0.248 sec/batch), lr: 0.015770
2019-12-27 20:32:37.705705: step 131300/136300 (epoch 97/100), loss = 0.078768 (0.228 sec/batch), lr: 0.015770
2019-12-27 20:32:42.558149: step 131320/136300 (epoch 97/100), loss = 0.337890 (0.229 sec/batch), lr: 0.015770
2019-12-27 20:32:47.386291: step 131340/136300 (epoch 97/100), loss = 0.070273 (0.227 sec/batch), lr: 0.015770
2019-12-27 20:32:52.282456: step 131360/136300 (epoch 97/100), loss = 0.175075 (0.240 sec/batch), lr: 0.015770
2019-12-27 20:32:57.174273: step 131380/136300 (epoch 97/100), loss = 0.293275 (0.237 sec/batch), lr: 0.015770
2019-12-27 20:33:02.045117: step 131400/136300 (epoch 97/100), loss = 0.133825 (0.240 sec/batch), lr: 0.015770
2019-12-27 20:33:06.862857: step 131420/136300 (epoch 97/100), loss = 0.142526 (0.235 sec/batch), lr: 0.015770
2019-12-27 20:33:11.681808: step 131440/136300 (epoch 97/100), loss = 0.263351 (0.180 sec/batch), lr: 0.015770
2019-12-27 20:33:17.824385: step 131460/136300 (epoch 97/100), loss = 0.174989 (0.221 sec/batch), lr: 0.015770
2019-12-27 20:33:22.630570: step 131480/136300 (epoch 97/100), loss = 0.246935 (0.237 sec/batch), lr: 0.015770
2019-12-27 20:33:27.378397: step 131500/136300 (epoch 97/100), loss = 0.206115 (0.223 sec/batch), lr: 0.015770
2019-12-27 20:33:32.208894: step 131520/136300 (epoch 97/100), loss = 0.094991 (0.232 sec/batch), lr: 0.015770
2019-12-27 20:33:37.140434: step 131540/136300 (epoch 97/100), loss = 0.204164 (0.237 sec/batch), lr: 0.015770
2019-12-27 20:33:42.017509: step 131560/136300 (epoch 97/100), loss = 0.331191 (0.219 sec/batch), lr: 0.015770
2019-12-27 20:33:46.942622: step 131580/136300 (epoch 97/100), loss = 0.219768 (0.241 sec/batch), lr: 0.015770
2019-12-27 20:33:51.793369: step 131600/136300 (epoch 97/100), loss = 0.171188 (0.233 sec/batch), lr: 0.015770
2019-12-27 20:33:56.558608: step 131620/136300 (epoch 97/100), loss = 0.249859 (0.229 sec/batch), lr: 0.015770
2019-12-27 20:34:01.462245: step 131640/136300 (epoch 97/100), loss = 0.271806 (0.234 sec/batch), lr: 0.015770
2019-12-27 20:34:07.787965: step 131660/136300 (epoch 97/100), loss = 0.231593 (0.217 sec/batch), lr: 0.015770
2019-12-27 20:34:12.564466: step 131680/136300 (epoch 97/100), loss = 0.219689 (0.245 sec/batch), lr: 0.015770
2019-12-27 20:34:17.482109: step 131700/136300 (epoch 97/100), loss = 0.192111 (0.178 sec/batch), lr: 0.015770
2019-12-27 20:34:22.134050: step 131720/136300 (epoch 97/100), loss = 0.209106 (0.228 sec/batch), lr: 0.015770
2019-12-27 20:34:27.034205: step 131740/136300 (epoch 97/100), loss = 0.193629 (0.234 sec/batch), lr: 0.015770
2019-12-27 20:34:31.962200: step 131760/136300 (epoch 97/100), loss = 0.195808 (0.218 sec/batch), lr: 0.015770
2019-12-27 20:34:36.760105: step 131780/136300 (epoch 97/100), loss = 0.173160 (0.239 sec/batch), lr: 0.015770
2019-12-27 20:34:41.704991: step 131800/136300 (epoch 97/100), loss = 0.141874 (0.241 sec/batch), lr: 0.015770
2019-12-27 20:34:46.609434: step 131820/136300 (epoch 97/100), loss = 0.276690 (0.226 sec/batch), lr: 0.015770
2019-12-27 20:34:51.441470: step 131840/136300 (epoch 97/100), loss = 0.246853 (0.227 sec/batch), lr: 0.015770
2019-12-27 20:34:57.897053: step 131860/136300 (epoch 97/100), loss = 0.161623 (0.216 sec/batch), lr: 0.015770
2019-12-27 20:35:02.771275: step 131880/136300 (epoch 97/100), loss = 0.101654 (0.201 sec/batch), lr: 0.015770
2019-12-27 20:35:07.395708: step 131900/136300 (epoch 97/100), loss = 0.179556 (0.203 sec/batch), lr: 0.015770
2019-12-27 20:35:12.132013: step 131920/136300 (epoch 97/100), loss = 0.149654 (0.229 sec/batch), lr: 0.015770
2019-12-27 20:35:16.956243: step 131940/136300 (epoch 97/100), loss = 0.192421 (0.238 sec/batch), lr: 0.015770
2019-12-27 20:35:21.784093: step 131960/136300 (epoch 97/100), loss = 0.118425 (0.202 sec/batch), lr: 0.015770
2019-12-27 20:35:26.682568: step 131980/136300 (epoch 97/100), loss = 0.218642 (0.236 sec/batch), lr: 0.015770
2019-12-27 20:35:31.412657: step 132000/136300 (epoch 97/100), loss = 0.159242 (0.237 sec/batch), lr: 0.015770
2019-12-27 20:35:36.089815: step 132020/136300 (epoch 97/100), loss = 0.182292 (0.178 sec/batch), lr: 0.015770
2019-12-27 20:35:40.918366: step 132040/136300 (epoch 97/100), loss = 0.135571 (0.239 sec/batch), lr: 0.015770
2019-12-27 20:35:47.241968: step 132060/136300 (epoch 97/100), loss = 0.139195 (0.245 sec/batch), lr: 0.015770
2019-12-27 20:35:52.141108: step 132080/136300 (epoch 97/100), loss = 0.229454 (0.234 sec/batch), lr: 0.015770
2019-12-27 20:35:57.065822: step 132100/136300 (epoch 97/100), loss = 0.095283 (0.226 sec/batch), lr: 0.015770
2019-12-27 20:36:01.669832: step 132120/136300 (epoch 97/100), loss = 0.250797 (0.221 sec/batch), lr: 0.015770
2019-12-27 20:36:06.513301: step 132140/136300 (epoch 97/100), loss = 0.136257 (0.232 sec/batch), lr: 0.015770
2019-12-27 20:36:11.289500: step 132160/136300 (epoch 97/100), loss = 0.204930 (0.234 sec/batch), lr: 0.015770
2019-12-27 20:36:16.205506: step 132180/136300 (epoch 97/100), loss = 0.227043 (0.205 sec/batch), lr: 0.015770
2019-12-27 20:36:21.062085: step 132200/136300 (epoch 97/100), loss = 0.089236 (0.233 sec/batch), lr: 0.015770
Evaluating on dev set...
Precision (micro): 71.437%
   Recall (micro): 63.263%
       F1 (micro): 67.102%
epoch 97: train_loss = 0.203496, dev_loss = 0.463612, dev_f1 = 0.6710
model saved to ./saved_models/01/checkpoint_epoch_97.pt

2019-12-27 20:36:59.316043: step 132220/136300 (epoch 98/100), loss = 0.155643 (0.219 sec/batch), lr: 0.014193
2019-12-27 20:37:05.512428: step 132240/136300 (epoch 98/100), loss = 0.099103 (0.240 sec/batch), lr: 0.014193
2019-12-27 20:37:10.328763: step 132260/136300 (epoch 98/100), loss = 0.371866 (0.216 sec/batch), lr: 0.014193
2019-12-27 20:37:15.032135: step 132280/136300 (epoch 98/100), loss = 0.144041 (0.233 sec/batch), lr: 0.014193
2019-12-27 20:37:19.978182: step 132300/136300 (epoch 98/100), loss = 0.211280 (0.242 sec/batch), lr: 0.014193
2019-12-27 20:37:24.803122: step 132320/136300 (epoch 98/100), loss = 0.441518 (0.244 sec/batch), lr: 0.014193
2019-12-27 20:37:29.509262: step 132340/136300 (epoch 98/100), loss = 0.240451 (0.220 sec/batch), lr: 0.014193
2019-12-27 20:37:34.398360: step 132360/136300 (epoch 98/100), loss = 0.202054 (0.233 sec/batch), lr: 0.014193
2019-12-27 20:37:39.185053: step 132380/136300 (epoch 98/100), loss = 0.220224 (0.233 sec/batch), lr: 0.014193
2019-12-27 20:37:44.089091: step 132400/136300 (epoch 98/100), loss = 0.111332 (0.247 sec/batch), lr: 0.014193
2019-12-27 20:37:48.870500: step 132420/136300 (epoch 98/100), loss = 0.105345 (0.176 sec/batch), lr: 0.014193
2019-12-27 20:37:55.229654: step 132440/136300 (epoch 98/100), loss = 0.151949 (0.213 sec/batch), lr: 0.014193
2019-12-27 20:38:00.061273: step 132460/136300 (epoch 98/100), loss = 0.150862 (0.202 sec/batch), lr: 0.014193
2019-12-27 20:38:04.906121: step 132480/136300 (epoch 98/100), loss = 0.167345 (0.237 sec/batch), lr: 0.014193
2019-12-27 20:38:09.694025: step 132500/136300 (epoch 98/100), loss = 0.239415 (0.236 sec/batch), lr: 0.014193
2019-12-27 20:38:14.430115: step 132520/136300 (epoch 98/100), loss = 0.277138 (0.170 sec/batch), lr: 0.014193
2019-12-27 20:38:19.233213: step 132540/136300 (epoch 98/100), loss = 0.241151 (0.235 sec/batch), lr: 0.014193
2019-12-27 20:38:24.039070: step 132560/136300 (epoch 98/100), loss = 0.194947 (0.248 sec/batch), lr: 0.014193
2019-12-27 20:38:28.925439: step 132580/136300 (epoch 98/100), loss = 0.192811 (0.242 sec/batch), lr: 0.014193
2019-12-27 20:38:33.610077: step 132600/136300 (epoch 98/100), loss = 0.364118 (0.218 sec/batch), lr: 0.014193
2019-12-27 20:38:39.753550: step 132620/136300 (epoch 98/100), loss = 0.209800 (1.675 sec/batch), lr: 0.014193
2019-12-27 20:38:44.499873: step 132640/136300 (epoch 98/100), loss = 0.150513 (0.232 sec/batch), lr: 0.014193
2019-12-27 20:38:49.440480: step 132660/136300 (epoch 98/100), loss = 0.186926 (0.228 sec/batch), lr: 0.014193
2019-12-27 20:38:54.294708: step 132680/136300 (epoch 98/100), loss = 0.346912 (0.236 sec/batch), lr: 0.014193
2019-12-27 20:38:59.073065: step 132700/136300 (epoch 98/100), loss = 0.276361 (0.200 sec/batch), lr: 0.014193
2019-12-27 20:39:03.947891: step 132720/136300 (epoch 98/100), loss = 0.471039 (0.211 sec/batch), lr: 0.014193
2019-12-27 20:39:08.810169: step 132740/136300 (epoch 98/100), loss = 0.245336 (0.238 sec/batch), lr: 0.014193
2019-12-27 20:39:13.674164: step 132760/136300 (epoch 98/100), loss = 0.193154 (0.182 sec/batch), lr: 0.014193
2019-12-27 20:39:18.521003: step 132780/136300 (epoch 98/100), loss = 0.131102 (0.242 sec/batch), lr: 0.014193
2019-12-27 20:39:23.327597: step 132800/136300 (epoch 98/100), loss = 0.183292 (0.232 sec/batch), lr: 0.014193
2019-12-27 20:39:29.586228: step 132820/136300 (epoch 98/100), loss = 0.263419 (0.173 sec/batch), lr: 0.014193
2019-12-27 20:39:34.398667: step 132840/136300 (epoch 98/100), loss = 0.373280 (0.215 sec/batch), lr: 0.014193
2019-12-27 20:39:39.150041: step 132860/136300 (epoch 98/100), loss = 0.097176 (0.222 sec/batch), lr: 0.014193
2019-12-27 20:39:43.979973: step 132880/136300 (epoch 98/100), loss = 0.177590 (0.237 sec/batch), lr: 0.014193
2019-12-27 20:39:48.872585: step 132900/136300 (epoch 98/100), loss = 0.152700 (0.213 sec/batch), lr: 0.014193
2019-12-27 20:39:53.765694: step 132920/136300 (epoch 98/100), loss = 0.243248 (0.221 sec/batch), lr: 0.014193
2019-12-27 20:39:58.662790: step 132940/136300 (epoch 98/100), loss = 0.149669 (0.220 sec/batch), lr: 0.014193
2019-12-27 20:40:03.545473: step 132960/136300 (epoch 98/100), loss = 0.077862 (0.235 sec/batch), lr: 0.014193
2019-12-27 20:40:08.235120: step 132980/136300 (epoch 98/100), loss = 0.145670 (0.231 sec/batch), lr: 0.014193
2019-12-27 20:40:13.128075: step 133000/136300 (epoch 98/100), loss = 0.283966 (0.205 sec/batch), lr: 0.014193
2019-12-27 20:40:19.465866: step 133020/136300 (epoch 98/100), loss = 0.189519 (0.180 sec/batch), lr: 0.014193
2019-12-27 20:40:24.275106: step 133040/136300 (epoch 98/100), loss = 0.182073 (0.248 sec/batch), lr: 0.014193
2019-12-27 20:40:29.226055: step 133060/136300 (epoch 98/100), loss = 0.314452 (0.238 sec/batch), lr: 0.014193
2019-12-27 20:40:33.898413: step 133080/136300 (epoch 98/100), loss = 0.191025 (0.180 sec/batch), lr: 0.014193
2019-12-27 20:40:38.745215: step 133100/136300 (epoch 98/100), loss = 0.489742 (0.230 sec/batch), lr: 0.014193
2019-12-27 20:40:43.664588: step 133120/136300 (epoch 98/100), loss = 0.291086 (0.243 sec/batch), lr: 0.014193
2019-12-27 20:40:48.456706: step 133140/136300 (epoch 98/100), loss = 0.235910 (0.224 sec/batch), lr: 0.014193
2019-12-27 20:40:53.356101: step 133160/136300 (epoch 98/100), loss = 0.213673 (0.246 sec/batch), lr: 0.014193
2019-12-27 20:40:58.302017: step 133180/136300 (epoch 98/100), loss = 0.114424 (0.231 sec/batch), lr: 0.014193
2019-12-27 20:41:03.086556: step 133200/136300 (epoch 98/100), loss = 0.218591 (0.225 sec/batch), lr: 0.014193
2019-12-27 20:41:09.369711: step 133220/136300 (epoch 98/100), loss = 0.337589 (0.241 sec/batch), lr: 0.014193
2019-12-27 20:41:14.284684: step 133240/136300 (epoch 98/100), loss = 0.398147 (0.238 sec/batch), lr: 0.014193
2019-12-27 20:41:18.884878: step 133260/136300 (epoch 98/100), loss = 0.214685 (0.233 sec/batch), lr: 0.014193
2019-12-27 20:41:23.602354: step 133280/136300 (epoch 98/100), loss = 0.106992 (0.239 sec/batch), lr: 0.014193
2019-12-27 20:41:28.449441: step 133300/136300 (epoch 98/100), loss = 0.099944 (0.213 sec/batch), lr: 0.014193
2019-12-27 20:41:33.235877: step 133320/136300 (epoch 98/100), loss = 0.277353 (0.228 sec/batch), lr: 0.014193
2019-12-27 20:41:38.138686: step 133340/136300 (epoch 98/100), loss = 0.138299 (0.233 sec/batch), lr: 0.014193
2019-12-27 20:41:42.896781: step 133360/136300 (epoch 98/100), loss = 0.318224 (0.237 sec/batch), lr: 0.014193
2019-12-27 20:41:47.591704: step 133380/136300 (epoch 98/100), loss = 0.061264 (0.202 sec/batch), lr: 0.014193
2019-12-27 20:41:52.357282: step 133400/136300 (epoch 98/100), loss = 0.180444 (0.209 sec/batch), lr: 0.014193
2019-12-27 20:41:58.765004: step 133420/136300 (epoch 98/100), loss = 0.276788 (0.241 sec/batch), lr: 0.014193
2019-12-27 20:42:03.695941: step 133440/136300 (epoch 98/100), loss = 0.235058 (0.220 sec/batch), lr: 0.014193
2019-12-27 20:42:08.593349: step 133460/136300 (epoch 98/100), loss = 0.195843 (0.238 sec/batch), lr: 0.014193
2019-12-27 20:42:13.267869: step 133480/136300 (epoch 98/100), loss = 0.045280 (0.204 sec/batch), lr: 0.014193
2019-12-27 20:42:18.082373: step 133500/136300 (epoch 98/100), loss = 0.129453 (0.230 sec/batch), lr: 0.014193
2019-12-27 20:42:22.814413: step 133520/136300 (epoch 98/100), loss = 0.135524 (0.242 sec/batch), lr: 0.014193
2019-12-27 20:42:27.733681: step 133540/136300 (epoch 98/100), loss = 0.201067 (0.209 sec/batch), lr: 0.014193
2019-12-27 20:42:32.590675: step 133560/136300 (epoch 98/100), loss = 0.261916 (0.229 sec/batch), lr: 0.014193
Evaluating on dev set...
Precision (micro): 70.867%
   Recall (micro): 63.319%
       F1 (micro): 66.880%
epoch 98: train_loss = 0.204978, dev_loss = 0.460118, dev_f1 = 0.6688
model saved to ./saved_models/01/checkpoint_epoch_98.pt

2019-12-27 20:43:10.820067: step 133580/136300 (epoch 99/100), loss = 0.195767 (0.245 sec/batch), lr: 0.012774
2019-12-27 20:43:16.967320: step 133600/136300 (epoch 99/100), loss = 0.137930 (0.211 sec/batch), lr: 0.012774
2019-12-27 20:43:21.802926: step 133620/136300 (epoch 99/100), loss = 0.205976 (0.235 sec/batch), lr: 0.012774
2019-12-27 20:43:26.554958: step 133640/136300 (epoch 99/100), loss = 0.163305 (0.228 sec/batch), lr: 0.012774
2019-12-27 20:43:31.478313: step 133660/136300 (epoch 99/100), loss = 0.167498 (0.228 sec/batch), lr: 0.012774
2019-12-27 20:43:36.298231: step 133680/136300 (epoch 99/100), loss = 0.146318 (0.247 sec/batch), lr: 0.012774
2019-12-27 20:43:41.034066: step 133700/136300 (epoch 99/100), loss = 0.213434 (0.207 sec/batch), lr: 0.012774
2019-12-27 20:43:45.902465: step 133720/136300 (epoch 99/100), loss = 0.217708 (0.205 sec/batch), lr: 0.012774
2019-12-27 20:43:50.740630: step 133740/136300 (epoch 99/100), loss = 0.305854 (0.234 sec/batch), lr: 0.012774
2019-12-27 20:43:55.634490: step 133760/136300 (epoch 99/100), loss = 0.257114 (0.225 sec/batch), lr: 0.012774
2019-12-27 20:44:00.449038: step 133780/136300 (epoch 99/100), loss = 0.089628 (0.212 sec/batch), lr: 0.012774
2019-12-27 20:44:06.797046: step 133800/136300 (epoch 99/100), loss = 0.205909 (0.207 sec/batch), lr: 0.012774
2019-12-27 20:44:11.650441: step 133820/136300 (epoch 99/100), loss = 0.073953 (0.238 sec/batch), lr: 0.012774
2019-12-27 20:44:16.432625: step 133840/136300 (epoch 99/100), loss = 0.249222 (0.244 sec/batch), lr: 0.012774
2019-12-27 20:44:21.231376: step 133860/136300 (epoch 99/100), loss = 0.152523 (0.236 sec/batch), lr: 0.012774
2019-12-27 20:44:26.069441: step 133880/136300 (epoch 99/100), loss = 0.213822 (0.235 sec/batch), lr: 0.012774
2019-12-27 20:44:30.798175: step 133900/136300 (epoch 99/100), loss = 0.279695 (0.233 sec/batch), lr: 0.012774
2019-12-27 20:44:35.606507: step 133920/136300 (epoch 99/100), loss = 0.273591 (0.240 sec/batch), lr: 0.012774
2019-12-27 20:44:40.525337: step 133940/136300 (epoch 99/100), loss = 0.268811 (0.247 sec/batch), lr: 0.012774
2019-12-27 20:44:45.243767: step 133960/136300 (epoch 99/100), loss = 0.090578 (0.214 sec/batch), lr: 0.012774
2019-12-27 20:44:49.878469: step 133980/136300 (epoch 99/100), loss = 0.138208 (0.221 sec/batch), lr: 0.012774
2019-12-27 20:44:56.150298: step 134000/136300 (epoch 99/100), loss = 0.315842 (0.230 sec/batch), lr: 0.012774
2019-12-27 20:45:01.105730: step 134020/136300 (epoch 99/100), loss = 0.188239 (0.226 sec/batch), lr: 0.012774
2019-12-27 20:45:06.009114: step 134040/136300 (epoch 99/100), loss = 0.202650 (0.231 sec/batch), lr: 0.012774
2019-12-27 20:45:10.783305: step 134060/136300 (epoch 99/100), loss = 0.133786 (0.179 sec/batch), lr: 0.012774
2019-12-27 20:45:15.645634: step 134080/136300 (epoch 99/100), loss = 0.290865 (0.202 sec/batch), lr: 0.012774
2019-12-27 20:45:20.513389: step 134100/136300 (epoch 99/100), loss = 0.132978 (0.238 sec/batch), lr: 0.012774
2019-12-27 20:45:25.461232: step 134120/136300 (epoch 99/100), loss = 0.221733 (0.228 sec/batch), lr: 0.012774
2019-12-27 20:45:30.257145: step 134140/136300 (epoch 99/100), loss = 0.044138 (0.252 sec/batch), lr: 0.012774
2019-12-27 20:45:35.017196: step 134160/136300 (epoch 99/100), loss = 0.274485 (0.229 sec/batch), lr: 0.012774
2019-12-27 20:45:39.777836: step 134180/136300 (epoch 99/100), loss = 0.415205 (0.201 sec/batch), lr: 0.012774
2019-12-27 20:45:46.045753: step 134200/136300 (epoch 99/100), loss = 0.156200 (0.206 sec/batch), lr: 0.012774
2019-12-27 20:45:50.815665: step 134220/136300 (epoch 99/100), loss = 0.107873 (0.238 sec/batch), lr: 0.012774
2019-12-27 20:45:55.589713: step 134240/136300 (epoch 99/100), loss = 0.234087 (0.231 sec/batch), lr: 0.012774
2019-12-27 20:46:00.475934: step 134260/136300 (epoch 99/100), loss = 0.181004 (0.235 sec/batch), lr: 0.012774
2019-12-27 20:46:05.387826: step 134280/136300 (epoch 99/100), loss = 0.304406 (0.215 sec/batch), lr: 0.012774
2019-12-27 20:46:10.290333: step 134300/136300 (epoch 99/100), loss = 0.202514 (0.228 sec/batch), lr: 0.012774
2019-12-27 20:46:15.222081: step 134320/136300 (epoch 99/100), loss = 0.143188 (0.236 sec/batch), lr: 0.012774
2019-12-27 20:46:19.964555: step 134340/136300 (epoch 99/100), loss = 0.286923 (0.235 sec/batch), lr: 0.012774
2019-12-27 20:46:24.775155: step 134360/136300 (epoch 99/100), loss = 0.143091 (0.251 sec/batch), lr: 0.012774
2019-12-27 20:46:31.254182: step 134380/136300 (epoch 99/100), loss = 0.227063 (0.249 sec/batch), lr: 0.012774
2019-12-27 20:46:35.985924: step 134400/136300 (epoch 99/100), loss = 0.289195 (0.206 sec/batch), lr: 0.012774
2019-12-27 20:46:40.961616: step 134420/136300 (epoch 99/100), loss = 0.200281 (0.241 sec/batch), lr: 0.012774
2019-12-27 20:46:45.688001: step 134440/136300 (epoch 99/100), loss = 0.230712 (0.211 sec/batch), lr: 0.012774
2019-12-27 20:46:50.477041: step 134460/136300 (epoch 99/100), loss = 0.035305 (0.233 sec/batch), lr: 0.012774
2019-12-27 20:46:55.381077: step 134480/136300 (epoch 99/100), loss = 0.251479 (0.243 sec/batch), lr: 0.012774
2019-12-27 20:47:00.289467: step 134500/136300 (epoch 99/100), loss = 0.229663 (0.222 sec/batch), lr: 0.012774
2019-12-27 20:47:05.083740: step 134520/136300 (epoch 99/100), loss = 0.246236 (0.221 sec/batch), lr: 0.012774
2019-12-27 20:47:10.051722: step 134540/136300 (epoch 99/100), loss = 0.232371 (0.232 sec/batch), lr: 0.012774
2019-12-27 20:47:14.899894: step 134560/136300 (epoch 99/100), loss = 0.209582 (0.208 sec/batch), lr: 0.012774
2019-12-27 20:47:21.254046: step 134580/136300 (epoch 99/100), loss = 0.307608 (0.236 sec/batch), lr: 0.012774
2019-12-27 20:47:26.140711: step 134600/136300 (epoch 99/100), loss = 0.118550 (0.236 sec/batch), lr: 0.012774
2019-12-27 20:47:30.759218: step 134620/136300 (epoch 99/100), loss = 0.233120 (0.228 sec/batch), lr: 0.012774
2019-12-27 20:47:35.526621: step 134640/136300 (epoch 99/100), loss = 0.225217 (0.237 sec/batch), lr: 0.012774
2019-12-27 20:47:40.344209: step 134660/136300 (epoch 99/100), loss = 0.203318 (0.208 sec/batch), lr: 0.012774
2019-12-27 20:47:45.154121: step 134680/136300 (epoch 99/100), loss = 0.130189 (0.228 sec/batch), lr: 0.012774
2019-12-27 20:47:50.069243: step 134700/136300 (epoch 99/100), loss = 0.214902 (0.226 sec/batch), lr: 0.012774
2019-12-27 20:47:54.813062: step 134720/136300 (epoch 99/100), loss = 0.244926 (0.205 sec/batch), lr: 0.012774
2019-12-27 20:47:59.592704: step 134740/136300 (epoch 99/100), loss = 0.167817 (0.226 sec/batch), lr: 0.012774
2019-12-27 20:48:04.392094: step 134760/136300 (epoch 99/100), loss = 0.087906 (0.247 sec/batch), lr: 0.012774
2019-12-27 20:48:10.853476: step 134780/136300 (epoch 99/100), loss = 0.279145 (0.245 sec/batch), lr: 0.012774
2019-12-27 20:48:15.774331: step 134800/136300 (epoch 99/100), loss = 0.203017 (0.248 sec/batch), lr: 0.012774
2019-12-27 20:48:20.644584: step 134820/136300 (epoch 99/100), loss = 0.171814 (0.237 sec/batch), lr: 0.012774
2019-12-27 20:48:25.428116: step 134840/136300 (epoch 99/100), loss = 0.139599 (0.217 sec/batch), lr: 0.012774
2019-12-27 20:48:30.157759: step 134860/136300 (epoch 99/100), loss = 0.204687 (0.233 sec/batch), lr: 0.012774
2019-12-27 20:48:34.913684: step 134880/136300 (epoch 99/100), loss = 0.236791 (0.243 sec/batch), lr: 0.012774
2019-12-27 20:48:39.799094: step 134900/136300 (epoch 99/100), loss = 0.239295 (0.245 sec/batch), lr: 0.012774
2019-12-27 20:48:44.681276: step 134920/136300 (epoch 99/100), loss = 0.278899 (0.237 sec/batch), lr: 0.012774
Evaluating on dev set...
Precision (micro): 72.195%
   Recall (micro): 62.380%
       F1 (micro): 66.930%
epoch 99: train_loss = 0.203886, dev_loss = 0.463565, dev_f1 = 0.6693
model saved to ./saved_models/01/checkpoint_epoch_99.pt

2019-12-27 20:49:22.845758: step 134940/136300 (epoch 100/100), loss = 0.222078 (0.216 sec/batch), lr: 0.012774
2019-12-27 20:49:29.152690: step 134960/136300 (epoch 100/100), loss = 0.206727 (0.215 sec/batch), lr: 0.012774/home/sda/wangbolin/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))

2019-12-27 20:49:33.947690: step 134980/136300 (epoch 100/100), loss = 0.315273 (0.240 sec/batch), lr: 0.012774
2019-12-27 20:49:38.732034: step 135000/136300 (epoch 100/100), loss = 0.187451 (0.205 sec/batch), lr: 0.012774
2019-12-27 20:49:43.609648: step 135020/136300 (epoch 100/100), loss = 0.306680 (0.232 sec/batch), lr: 0.012774
2019-12-27 20:49:48.393971: step 135040/136300 (epoch 100/100), loss = 0.240591 (0.214 sec/batch), lr: 0.012774
2019-12-27 20:49:53.172588: step 135060/136300 (epoch 100/100), loss = 0.113905 (0.232 sec/batch), lr: 0.012774
2019-12-27 20:49:58.024144: step 135080/136300 (epoch 100/100), loss = 0.301152 (0.238 sec/batch), lr: 0.012774
2019-12-27 20:50:02.816203: step 135100/136300 (epoch 100/100), loss = 0.281574 (0.225 sec/batch), lr: 0.012774
2019-12-27 20:50:07.691623: step 135120/136300 (epoch 100/100), loss = 0.115878 (0.219 sec/batch), lr: 0.012774
2019-12-27 20:50:12.558224: step 135140/136300 (epoch 100/100), loss = 0.297449 (0.248 sec/batch), lr: 0.012774
2019-12-27 20:50:18.706110: step 135160/136300 (epoch 100/100), loss = 0.230763 (0.215 sec/batch), lr: 0.012774
2019-12-27 20:50:23.492493: step 135180/136300 (epoch 100/100), loss = 0.239377 (0.239 sec/batch), lr: 0.012774
2019-12-27 20:50:28.257478: step 135200/136300 (epoch 100/100), loss = 0.077804 (0.203 sec/batch), lr: 0.012774
2019-12-27 20:50:33.101411: step 135220/136300 (epoch 100/100), loss = 0.147382 (0.240 sec/batch), lr: 0.012774
2019-12-27 20:50:37.953186: step 135240/136300 (epoch 100/100), loss = 0.294455 (0.233 sec/batch), lr: 0.012774
2019-12-27 20:50:42.687885: step 135260/136300 (epoch 100/100), loss = 0.322456 (0.183 sec/batch), lr: 0.012774
2019-12-27 20:50:47.462017: step 135280/136300 (epoch 100/100), loss = 0.097158 (0.217 sec/batch), lr: 0.012774
2019-12-27 20:50:52.360933: step 135300/136300 (epoch 100/100), loss = 0.201459 (0.220 sec/batch), lr: 0.012774
2019-12-27 20:50:57.116609: step 135320/136300 (epoch 100/100), loss = 0.250227 (0.232 sec/batch), lr: 0.012774
2019-12-27 20:51:01.831582: step 135340/136300 (epoch 100/100), loss = 0.080131 (0.214 sec/batch), lr: 0.012774
2019-12-27 20:51:08.189022: step 135360/136300 (epoch 100/100), loss = 0.263939 (0.216 sec/batch), lr: 0.012774
2019-12-27 20:51:13.100413: step 135380/136300 (epoch 100/100), loss = 0.203433 (0.244 sec/batch), lr: 0.012774
2019-12-27 20:51:17.976129: step 135400/136300 (epoch 100/100), loss = 0.232204 (0.223 sec/batch), lr: 0.012774
2019-12-27 20:51:22.785974: step 135420/136300 (epoch 100/100), loss = 0.119664 (0.240 sec/batch), lr: 0.012774
2019-12-27 20:51:27.598795: step 135440/136300 (epoch 100/100), loss = 0.154933 (0.236 sec/batch), lr: 0.012774
2019-12-27 20:51:32.451839: step 135460/136300 (epoch 100/100), loss = 0.277527 (0.237 sec/batch), lr: 0.012774
2019-12-27 20:51:37.411747: step 135480/136300 (epoch 100/100), loss = 0.280252 (0.224 sec/batch), lr: 0.012774
2019-12-27 20:51:42.167310: step 135500/136300 (epoch 100/100), loss = 0.168349 (0.227 sec/batch), lr: 0.012774
2019-12-27 20:51:46.917422: step 135520/136300 (epoch 100/100), loss = 0.142872 (0.177 sec/batch), lr: 0.012774
2019-12-27 20:51:51.667943: step 135540/136300 (epoch 100/100), loss = 0.202833 (0.236 sec/batch), lr: 0.012774
2019-12-27 20:51:57.746492: step 135560/136300 (epoch 100/100), loss = 0.174979 (0.232 sec/batch), lr: 0.012774
2019-12-27 20:52:02.517369: step 135580/136300 (epoch 100/100), loss = 0.370335 (0.221 sec/batch), lr: 0.012774
2019-12-27 20:52:07.297853: step 135600/136300 (epoch 100/100), loss = 0.094345 (0.220 sec/batch), lr: 0.012774
2019-12-27 20:52:12.160329: step 135620/136300 (epoch 100/100), loss = 0.140066 (0.230 sec/batch), lr: 0.012774
2019-12-27 20:52:17.087745: step 135640/136300 (epoch 100/100), loss = 0.151364 (0.241 sec/batch), lr: 0.012774
2019-12-27 20:52:22.011493: step 135660/136300 (epoch 100/100), loss = 0.132044 (0.240 sec/batch), lr: 0.012774
2019-12-27 20:52:26.878885: step 135680/136300 (epoch 100/100), loss = 0.358866 (0.230 sec/batch), lr: 0.012774
2019-12-27 20:52:31.647303: step 135700/136300 (epoch 100/100), loss = 0.244931 (0.239 sec/batch), lr: 0.012774
2019-12-27 20:52:36.480646: step 135720/136300 (epoch 100/100), loss = 0.182192 (0.242 sec/batch), lr: 0.012774
2019-12-27 20:52:42.874386: step 135740/136300 (epoch 100/100), loss = 0.177964 (0.248 sec/batch), lr: 0.012774
2019-12-27 20:52:47.640993: step 135760/136300 (epoch 100/100), loss = 0.262105 (0.249 sec/batch), lr: 0.012774
2019-12-27 20:52:52.545161: step 135780/136300 (epoch 100/100), loss = 0.440718 (0.239 sec/batch), lr: 0.012774
2019-12-27 20:52:57.349838: step 135800/136300 (epoch 100/100), loss = 0.203684 (0.212 sec/batch), lr: 0.012774
2019-12-27 20:53:02.060926: step 135820/136300 (epoch 100/100), loss = 0.155207 (0.218 sec/batch), lr: 0.012774
2019-12-27 20:53:06.937734: step 135840/136300 (epoch 100/100), loss = 0.098015 (0.237 sec/batch), lr: 0.012774
2019-12-27 20:53:11.872530: step 135860/136300 (epoch 100/100), loss = 0.093033 (0.248 sec/batch), lr: 0.012774
2019-12-27 20:53:16.664337: step 135880/136300 (epoch 100/100), loss = 0.253023 (0.242 sec/batch), lr: 0.012774
2019-12-27 20:53:21.592900: step 135900/136300 (epoch 100/100), loss = 0.204029 (0.179 sec/batch), lr: 0.012774
2019-12-27 20:53:26.513397: step 135920/136300 (epoch 100/100), loss = 0.122322 (0.237 sec/batch), lr: 0.012774
2019-12-27 20:53:32.750029: step 135940/136300 (epoch 100/100), loss = 0.310234 (0.200 sec/batch), lr: 0.012774
2019-12-27 20:53:37.610063: step 135960/136300 (epoch 100/100), loss = 0.172786 (0.235 sec/batch), lr: 0.012774
2019-12-27 20:53:42.269014: step 135980/136300 (epoch 100/100), loss = 0.309279 (0.212 sec/batch), lr: 0.012774
2019-12-27 20:53:47.044036: step 136000/136300 (epoch 100/100), loss = 0.296461 (0.238 sec/batch), lr: 0.012774
2019-12-27 20:53:51.908286: step 136020/136300 (epoch 100/100), loss = 0.120889 (0.245 sec/batch), lr: 0.012774
2019-12-27 20:53:56.645706: step 136040/136300 (epoch 100/100), loss = 0.284388 (0.218 sec/batch), lr: 0.012774
2019-12-27 20:54:01.566221: step 136060/136300 (epoch 100/100), loss = 0.225340 (0.220 sec/batch), lr: 0.012774
2019-12-27 20:54:06.386772: step 136080/136300 (epoch 100/100), loss = 0.209794 (0.183 sec/batch), lr: 0.012774
2019-12-27 20:54:11.171058: step 136100/136300 (epoch 100/100), loss = 0.066143 (0.216 sec/batch), lr: 0.012774
2019-12-27 20:54:15.864532: step 136120/136300 (epoch 100/100), loss = 0.270467 (0.166 sec/batch), lr: 0.012774
2019-12-27 20:54:22.255216: step 136140/136300 (epoch 100/100), loss = 0.202295 (0.236 sec/batch), lr: 0.012774
2019-12-27 20:54:27.138180: step 136160/136300 (epoch 100/100), loss = 0.221517 (0.232 sec/batch), lr: 0.012774
2019-12-27 20:54:32.003039: step 136180/136300 (epoch 100/100), loss = 0.242239 (0.206 sec/batch), lr: 0.012774
2019-12-27 20:54:36.818455: step 136200/136300 (epoch 100/100), loss = 0.265402 (0.222 sec/batch), lr: 0.012774
2019-12-27 20:54:41.511582: step 136220/136300 (epoch 100/100), loss = 0.236385 (0.219 sec/batch), lr: 0.012774
2019-12-27 20:54:46.262236: step 136240/136300 (epoch 100/100), loss = 0.273885 (0.207 sec/batch), lr: 0.012774
2019-12-27 20:54:51.121532: step 136260/136300 (epoch 100/100), loss = 0.134112 (0.233 sec/batch), lr: 0.012774
2019-12-27 20:54:56.031773: step 136280/136300 (epoch 100/100), loss = 0.134825 (0.241 sec/batch), lr: 0.012774
2019-12-27 20:55:00.812066: step 136300/136300 (epoch 100/100), loss = 0.026748 (0.131 sec/batch), lr: 0.012774
Evaluating on dev set...
Precision (micro): 71.432%
   Recall (micro): 63.153%
       F1 (micro): 67.038%
epoch 100: train_loss = 0.203983, dev_loss = 0.461160, dev_f1 = 0.6704
model saved to ./saved_models/01/checkpoint_epoch_100.pt

Training ended with 100 epochs.
