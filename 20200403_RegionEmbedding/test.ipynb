{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil,time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import nd,gluon,autograd,init\n",
    "from dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "###     variable need to modify   ###\n",
    "#####################################\n",
    "data_path = 'data/'\n",
    "print_step = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctx = mx.gpu()\n",
    "# ctx\n",
    "# ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.test_utils.list_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "isContextWord = True # choose ContextWord or WordContext\n",
    "index = 6 # which task, see list list_task in line31 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "###        hyper parameters       ###\n",
    "#####################################\n",
    "emb_size = 128\n",
    "region_size = 21\n",
    "region_radius = region_size//2\n",
    "batch_size = 7\n",
    "max_epoch = 3\n",
    "learning_rate = 0.0001\n",
    "# list_max_sequence_length = [1024,256,256,256,1024,256,4424]\n",
    "list_max_sequence_length = [1024,256,256,256,1024,256,4096]\n",
    "list_n_classes = [5,2,5,4,10,14,2]\n",
    "list_vocab_size = [124273,394385,356312,42783,361926,227863,21785]\n",
    "list_task = ['yelp_full','amazon_polarity','amazon_full','ag','yahoo','dbpedia','DJIA']\n",
    "n_classes = list_n_classes[index]\n",
    "vocab_size = list_vocab_size[index]\n",
    "max_sequence_length = list_max_sequence_length[index]\n",
    "task_path = list_task[index]+'/'\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextWordNet(nn.HybridBlock):\n",
    "    def __init__(self):\n",
    "        super(ContextWordNet, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(vocab_size,emb_size)\n",
    "            self.embedding_region = nn.Embedding(vocab_size*region_size,emb_size)\n",
    "            self.max_pool = nn.GlobalMaxPool1D()\n",
    "            self.dense = nn.Dense(n_classes)\n",
    "    def hybrid_forward(self, F,aligned_seq,trimed_seq,mask):\n",
    "        region_aligned_unit = self.embedding_region(aligned_seq)\n",
    "        word_emb = self.embedding(trimed_seq).expand_dims(axis=2).broadcast_axes(axis=2,size=region_size)\n",
    "        projected_emb = region_aligned_unit * word_emb\n",
    "        feature = self.max_pool(projected_emb.transpose((0,1,3,2)).reshape((batch_size,-1,region_size))).reshape((batch_size,-1,emb_size))\n",
    "        feature = feature*mask\n",
    "        res = F.sum(feature, axis=1).reshape((batch_size,emb_size))\n",
    "        res = self.dense(res)\n",
    "        return res\n",
    "class WordContextNet(nn.HybridBlock):\n",
    "    def __init__(self):\n",
    "        super(WordContextNet, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(vocab_size,region_size*emb_size)\n",
    "            self.embedding_region = nn.Embedding(vocab_size,emb_size)\n",
    "            self.max_pool = nn.GlobalMaxPool1D()\n",
    "            self.dense = nn.Dense(n_classes)\n",
    "    def hybrid_forward(self, F,aligned_seq,trimed_seq,mask):\n",
    "        region_aligned_seq = aligned_seq.transpose((1, 0, 2))\n",
    "        region_aligned_emb = self.embedding_region(region_aligned_seq).reshape((batch_size,-1,region_size,emb_size))\n",
    "        context_unit = self.embedding(trimed_seq).reshape((batch_size,-1,region_size,emb_size))\n",
    "        projected_emb = region_aligned_emb * context_unit\n",
    "        feature = self.max_pool(projected_emb.transpose((0,1,3,2)).reshape((batch_size,-1,region_size))).reshape((batch_size,-1,emb_size))\n",
    "        feature = feature*mask\n",
    "        res = F.sum(feature, axis=1).reshape((batch_size,emb_size))\n",
    "        res = self.dense(res)\n",
    "        return res\n",
    "def accuracy(output,label,batch_size):\n",
    "    out = nd.argmax(output,axis=1)\n",
    "    res = nd.sum(nd.equal(out.reshape((-1,1)),label))/batch_size\n",
    "    return res\n",
    "def batch_process(seq,isContextWord,ctx):\n",
    "    seq = np.array(seq)\n",
    "    aligned_seq = np.zeros((max_sequence_length - 2*region_radius,batch_size,region_size))\n",
    "    for i in range(region_radius, max_sequence_length - region_radius):\n",
    "        aligned_seq[i-region_radius] = seq[:,i-region_radius:i-region_radius+region_size]\n",
    "    if isContextWord:\n",
    "        unit_id_bias = np.array([i * vocab_size for i in range(region_size)])\n",
    "        aligned_seq = aligned_seq.transpose((1,0,2))+unit_id_bias\n",
    "    aligned_seq = nd.array(aligned_seq,ctx)\n",
    "    batch_sequence = nd.array(seq,ctx)\n",
    "    trimed_seq = batch_sequence[:, region_radius: max_sequence_length - region_radius]\n",
    "    mask = nd.broadcast_axes(nd.greater(trimed_seq,0).reshape((batch_size,-1,1)),axis=2,size=128)\n",
    "    return aligned_seq,nd.array(trimed_seq,ctx),mask\n",
    "def evaluate(data,batch_size):\n",
    "    test_loss = 0.0\n",
    "    acc_test = 0.0\n",
    "    cnt = 0\n",
    "    for epoch_percent, batch_slots in batch_iter(data,batch_size,shuffle=False):\n",
    "        batch_sequence, batch_label = zip(*batch_slots)\n",
    "        batch_label = nd.array(batch_label,ctx)\n",
    "        aligned_seq,trimed_seq,mask = batch_process(batch_sequence,isContextWord,ctx)\n",
    "        output = net(aligned_seq,trimed_seq,mask)\n",
    "        loss = SCE(output,batch_label)\n",
    "        acc_test += accuracy(output,batch_label,batch_size)\n",
    "        test_loss += nd.mean(loss)\n",
    "        cnt = cnt+1\n",
    "    return acc_test.asscalar()/cnt,test_loss.asscalar()/cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ContextWordNet() if isContextWord else WordContextNet()\n",
    "SCE = mx.gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading train...\n",
      "read 1 lines\n",
      "Loading test...\n",
      "read 1 lines\n",
      "cpu(0) DJIA\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(),'adam', {'learning_rate': learning_rate})\n",
    "data_test,data_train = load_data(data_path+task_path,max_sequence_length)\n",
    "best_acc,global_step,train_loss,train_acc = 0,0,0,0\n",
    "net.hybridize()\n",
    "ctime = time.time()\n",
    "print(ctx,list_task[index])\n",
    "for epoch in range(max_epoch):\n",
    "    for epoch_percent, batch_slots in batch_iter(data_train,batch_size,shuffle=True):\n",
    "        batch_sequence, batch_label = zip(*batch_slots)\n",
    "        global_step = global_step + 1\n",
    "        batch_label = nd.array(batch_label,ctx)\n",
    "        aligned_seq,trimed_seq,mask = batch_process(batch_sequence,isContextWord,ctx)\n",
    "        with autograd.record():\n",
    "            output = net(aligned_seq,trimed_seq,mask)\n",
    "            loss = SCE(output,batch_label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        train_acc += accuracy(output,batch_label,batch_size)\n",
    "        train_loss += nd.mean(loss)\n",
    "        if global_step%print_step==0:\n",
    "            print('%.4f %%'%epoch_percent,'train_loss:',train_loss.asscalar()/print_step,' train_acc:',train_acc.asscalar()/print_step,'time:',time.time()-ctime)\n",
    "            train_loss,train_acc = 0,0\n",
    "            ctime = time.time()\n",
    "    test_acc,test_loss = evaluate(data_test,batch_size)\n",
    "    if test_acc>best_acc:\n",
    "        best_acc = test_acc\n",
    "        net.save_parameters('params/regionemb_'+list_task[index])\n",
    "    print('epoch %d done'%(epoch+1),'acc = %.4f,loss = %.4f'%(test_acc,test_loss))    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
